{
  "version": 3,
  "sources": ["../src/polyfills/index.ts", "../../../node_modules/mux-embed/dist/mux.mjs", "../../../node_modules/hls.js/dist/node_modules/url-toolkit/src/url-toolkit.js", "../../../node_modules/hls.js/dist/src/polyfills/number.ts", "../../../node_modules/hls.js/dist/src/events.ts", "../../../node_modules/hls.js/dist/src/errors.ts", "../../../node_modules/hls.js/dist/src/utils/logger.ts", "../../../node_modules/hls.js/dist/src/utils/attr-list.ts", "../../../node_modules/hls.js/dist/src/loader/date-range.ts", "../../../node_modules/hls.js/dist/src/loader/load-stats.ts", "../../../node_modules/hls.js/dist/src/loader/fragment.ts", "../../../node_modules/hls.js/dist/src/loader/level-details.ts", "../../../node_modules/hls.js/dist/src/utils/numeric-encoding-utils.ts", "../../../node_modules/hls.js/dist/src/utils/keysystem-util.ts", "../../../node_modules/hls.js/dist/src/utils/global.ts", "../../../node_modules/hls.js/dist/src/utils/mediakeys-helper.ts", "../../../node_modules/hls.js/dist/src/utils/typed-array.ts", "../../../node_modules/hls.js/dist/src/demux/id3.ts", "../../../node_modules/hls.js/dist/src/utils/hex.ts", "../../../node_modules/hls.js/dist/src/utils/mp4-tools.ts", "../../../node_modules/hls.js/dist/src/loader/level-key.ts", "../../../node_modules/hls.js/dist/src/utils/variable-substitution.ts", "../../../node_modules/hls.js/dist/src/utils/mediasource-helper.ts", "../../../node_modules/hls.js/dist/src/utils/codecs.ts", "../../../node_modules/hls.js/dist/src/loader/m3u8-parser.ts", "../../../node_modules/hls.js/dist/src/types/loader.ts", "../../../node_modules/hls.js/dist/src/loader/playlist-loader.ts", "../../../node_modules/hls.js/dist/src/utils/texttrack-utils.ts", "../../../node_modules/hls.js/dist/src/types/demuxer.ts", "../../../node_modules/hls.js/dist/src/controller/id3-track-controller.ts", "../../../node_modules/hls.js/dist/src/controller/latency-controller.ts", "../../../node_modules/hls.js/dist/src/types/level.ts", "../../../node_modules/hls.js/dist/src/utils/level-helper.ts", "../../../node_modules/hls.js/dist/src/utils/error-helper.ts", "../../../node_modules/hls.js/dist/src/utils/binary-search.ts", "../../../node_modules/hls.js/dist/src/controller/fragment-finders.ts", "../../../node_modules/hls.js/dist/src/controller/error-controller.ts", "../../../node_modules/hls.js/dist/src/controller/base-playlist-controller.ts", "../../../node_modules/hls.js/dist/src/utils/ewma.ts", "../../../node_modules/hls.js/dist/src/utils/ewma-bandwidth-estimator.ts", "../../../node_modules/hls.js/dist/src/utils/mediacapabilities-helper.ts", "../../../node_modules/hls.js/dist/src/utils/hdr.ts", "../../../node_modules/hls.js/dist/src/utils/rendition-helper.ts", "../../../node_modules/hls.js/dist/src/controller/abr-controller.ts", "../../../node_modules/hls.js/dist/src/task-loop.ts", "../../../node_modules/hls.js/dist/src/controller/fragment-tracker.ts", "../../../node_modules/hls.js/dist/src/utils/buffer-helper.ts", "../../../node_modules/hls.js/dist/src/types/transmuxer.ts", "../../../node_modules/hls.js/dist/src/utils/discontinuities.ts", "../../../node_modules/hls.js/dist/src/loader/fragment-loader.ts", "../../../node_modules/hls.js/dist/src/crypt/aes-crypto.ts", "../../../node_modules/hls.js/dist/src/crypt/fast-aes-key.ts", "../../../node_modules/hls.js/dist/src/crypt/aes-decryptor.ts", "../../../node_modules/hls.js/dist/src/crypt/decrypter.ts", "../../../node_modules/hls.js/dist/src/utils/time-ranges.ts", "../../../node_modules/hls.js/dist/src/controller/base-stream-controller.ts", "../../../node_modules/hls.js/dist/src/demux/chunk-cache.ts", "../../../node_modules/hls.js/dist/src/demux/inject-worker.ts", "../../../node_modules/hls.js/dist/src/demux/dummy-demuxed-track.ts", "../../../node_modules/hls.js/dist/src/demux/audio/base-audio-demuxer.ts", "../../../node_modules/hls.js/dist/src/demux/audio/adts.ts", "../../../node_modules/hls.js/dist/src/demux/audio/mpegaudio.ts", "../../../node_modules/hls.js/dist/src/demux/audio/aacdemuxer.ts", "../../../node_modules/hls.js/dist/src/demux/mp4demuxer.ts", "../../../node_modules/hls.js/dist/src/demux/audio/dolby.ts", "../../../node_modules/hls.js/dist/src/demux/audio/ac3-demuxer.ts", "../../../node_modules/hls.js/dist/src/demux/video/base-video-parser.ts", "../../../node_modules/hls.js/dist/src/demux/video/exp-golomb.ts", "../../../node_modules/hls.js/dist/src/demux/video/avc-video-parser.ts", "../../../node_modules/hls.js/dist/src/demux/sample-aes.ts", "../../../node_modules/hls.js/dist/src/demux/tsdemuxer.ts", "../../../node_modules/hls.js/dist/src/demux/audio/mp3demuxer.ts", "../../../node_modules/hls.js/dist/src/remux/aac-helper.ts", "../../../node_modules/hls.js/dist/src/remux/mp4-generator.ts", "../../../node_modules/hls.js/dist/src/utils/timescale-conversion.ts", "../../../node_modules/hls.js/dist/src/remux/mp4-remuxer.ts", "../../../node_modules/hls.js/dist/src/remux/passthrough-remuxer.ts", "../../../node_modules/hls.js/dist/src/demux/transmuxer.ts", "../../../node_modules/hls.js/dist/node_modules/eventemitter3/index.js", "../../../node_modules/hls.js/dist/src/demux/transmuxer-interface.ts", "../../../node_modules/hls.js/dist/src/utils/media-option-attributes.ts", "../../../node_modules/hls.js/dist/src/controller/audio-stream-controller.ts", "../../../node_modules/hls.js/dist/src/controller/audio-track-controller.ts", "../../../node_modules/hls.js/dist/src/controller/subtitle-stream-controller.ts", "../../../node_modules/hls.js/dist/src/controller/subtitle-track-controller.ts", "../../../node_modules/hls.js/dist/src/controller/buffer-operation-queue.ts", "../../../node_modules/hls.js/dist/src/controller/buffer-controller.ts", "../../../node_modules/hls.js/dist/src/utils/cea-608-parser.ts", "../../../node_modules/hls.js/dist/src/utils/output-filter.ts", "../../../node_modules/hls.js/dist/src/utils/vttcue.ts", "../../../node_modules/hls.js/dist/src/utils/vttparser.ts", "../../../node_modules/hls.js/dist/src/utils/webvtt-parser.ts", "../../../node_modules/hls.js/dist/src/utils/imsc1-ttml-parser.ts", "../../../node_modules/hls.js/dist/src/controller/timeline-controller.ts", "../../../node_modules/hls.js/dist/src/controller/cap-level-controller.ts", "../../../node_modules/hls.js/dist/src/controller/fps-controller.ts", "../../../node_modules/hls.js/dist/src/controller/eme-controller.ts", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/cta/CmObjectType.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/cta/CmStreamingFormat.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/cmcd/CmcdHeaderField.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/cmcd/CmcdHeaderMap.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/structuredfield/SfItem.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/structuredfield/SfToken.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/structuredfield/utils/DICT.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/structuredfield/utils/throwError.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/structuredfield/utils/BARE_ITEM.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/structuredfield/utils/BOOLEAN.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/structuredfield/utils/BYTES.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/structuredfield/utils/DECIMAL.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/structuredfield/utils/INTEGER.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/structuredfield/utils/isInvalidInt.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/structuredfield/utils/STRING_REGEX.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/structuredfield/utils/TOKEN.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/structuredfield/utils/KEY.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/structuredfield/serialize/serializeError.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/structuredfield/serialize/serializeBoolean.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/utils/base64encode.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/structuredfield/serialize/serializeByteSequence.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/structuredfield/serialize/serializeInteger.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/structuredfield/serialize/serializeDate.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/utils/roundToEven.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/structuredfield/serialize/serializeDecimal.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/structuredfield/utils/STRING.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/structuredfield/serialize/serializeString.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/cta/utils/symbolToStr.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/structuredfield/serialize/serializeToken.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/structuredfield/serialize/serializeBareItem.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/structuredfield/serialize/serializeKey.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/structuredfield/serialize/serializeParams.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/structuredfield/serialize/serializeItem.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/structuredfield/serialize/serializeInnerList.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/structuredfield/serialize/serializeDict.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/structuredfield/encodeSfDict.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/cta/utils/isTokenField.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/cta/utils/isValid.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/utils/urlToRelativePath.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/utils/uuid.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/cmcd/CmcdFormatters.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/cmcd/utils/processCmcd.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/cmcd/encodeCmcd.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/cmcd/toCmcdHeaders.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/cmcd/appendCmcdHeaders.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/cmcd/CMCD_PARAM.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/cmcd/toCmcdQuery.js", "../../../node_modules/hls.js/dist/node_modules/@svta/common-media-library/dist/cmcd/appendCmcdQuery.js", "../../../node_modules/hls.js/dist/src/controller/cmcd-controller.ts", "../../../node_modules/hls.js/dist/src/controller/content-steering-controller.ts", "../../../node_modules/hls.js/dist/src/utils/xhr-loader.ts", "../../../node_modules/hls.js/dist/src/utils/fetch-loader.ts", "../../../node_modules/hls.js/dist/src/utils/cues.ts", "../../../node_modules/hls.js/dist/src/config.ts", "../../../node_modules/hls.js/dist/src/controller/level-controller.ts", "../../../node_modules/hls.js/dist/src/loader/key-loader.ts", "../../../node_modules/hls.js/dist/src/is-supported.ts", "../../../node_modules/hls.js/dist/src/controller/gap-controller.ts", "../../../node_modules/hls.js/dist/src/controller/stream-controller.ts", "../../../node_modules/hls.js/dist/src/hls.ts", "../../playback-core/src/index.ts", "../../playback-core/src/hls.ts", "../../playback-core/src/errors.ts", "../../playback-core/src/types.ts", "../../playback-core/lang/en.json", "../../playback-core/src/util.ts", "../../playback-core/src/autoplay.ts", "../../playback-core/src/preload.ts", "../../playback-core/src/media-tracks.ts", "../../playback-core/src/text-tracks.ts", "../../playback-core/src/pdt.ts", "../../playback-core/src/request-errors.ts", "../src/env.ts", "../../../node_modules/custom-media-element/custom-media-element.js", "../../../node_modules/castable-video/castable-utils.js", "../../../node_modules/castable-video/castable-remote-playback.js", "../../../node_modules/castable-video/castable-mixin.js", "../../../node_modules/media-tracks/dist/track-event.js", "../../../node_modules/media-tracks/dist/utils.js", "../../../node_modules/media-tracks/dist/video-track-list.js", "../../../node_modules/media-tracks/dist/rendition-event.js", "../../../node_modules/media-tracks/dist/video-rendition-list.js", "../../../node_modules/media-tracks/dist/video-rendition.js", "../../../node_modules/media-tracks/dist/video-track.js", "../../../node_modules/media-tracks/dist/audio-rendition-list.js", "../../../node_modules/media-tracks/dist/audio-rendition.js", "../../../node_modules/media-tracks/dist/audio-track-list.js", "../../../node_modules/media-tracks/dist/audio-track.js", "../../../node_modules/media-tracks/dist/mixin.js", "../src/index.ts"],
  "sourcesContent": ["/* eslint @typescript-eslint/no-empty-function: \"off\", @typescript-eslint/no-unused-vars: \"off\" */\n\nclass EventTarget {\n  addEventListener() {}\n  removeEventListener() {}\n  dispatchEvent(_event: Event) {\n    return true;\n  }\n}\n\n// @github/template-parts requires DocumentFragment to be available on globalThis for SSR\nif (typeof DocumentFragment === 'undefined') {\n  class DocumentFragment extends EventTarget {}\n  // @ts-ignore\n  globalThis.DocumentFragment = DocumentFragment;\n}\n\nclass HTMLElement extends EventTarget {}\nclass HTMLVideoElement extends EventTarget {}\n\nconst customElements: CustomElementRegistry = {\n  get(_name: string) {\n    return undefined;\n  },\n  define(_name, _constructor, _options) {},\n  getName(_constructor) {\n    return null;\n  },\n  upgrade(_root) {},\n  whenDefined(_name) {\n    return Promise.resolve(HTMLElement as unknown as CustomElementConstructor);\n  },\n};\n\nclass CustomEvent {\n  #detail;\n  get detail() {\n    return this.#detail;\n  }\n  constructor(typeArg: string, eventInitDict: CustomEventInit = {}) {\n    // super(typeArg, eventInitDict);\n    this.#detail = eventInitDict?.detail;\n  }\n  initCustomEvent() {}\n}\n\nfunction createElement(_tagName: string, _options?: ElementCreationOptions): HTMLElement {\n  return new HTMLElement();\n}\n\nconst globalThisShim = {\n  document: {\n    createElement,\n  },\n  DocumentFragment,\n  customElements,\n  CustomEvent,\n  EventTarget,\n  HTMLElement,\n  HTMLVideoElement,\n};\n\n// const isServer = typeof window === 'undefined' || typeof globalThis.customElements === 'undefined';\n// const GlobalThis = isServer ? globalThisShim : globalThis;\n// const Document = isServer ? globalThisShim.document : globalThis.document;\n//\n// export { GlobalThis as globalThis, Document as document };\nconst isServer = typeof window === 'undefined' || typeof globalThis.customElements === 'undefined';\ntype GlobalThis = typeof globalThis;\nconst internalGlobalThis: GlobalThis = (isServer ? globalThisShim : globalThis) as GlobalThis;\nconst internalDocument: Document = (isServer ? globalThisShim.document : globalThis.document) as Document;\n\nexport { internalGlobalThis as globalThis, internalDocument as document };\n", "var Jr=Object.create;var lt=Object.defineProperty;var Qr=Object.getOwnPropertyDescriptor;var zr=Object.getOwnPropertyNames;var Kr=Object.getPrototypeOf,Yr=Object.prototype.hasOwnProperty;var ct=function(r,e){return function(){return r&&(e=r(r=0)),e}};var H=function(r,e){return function(){return e||r((e={exports:{}}).exports,e),e.exports}};var Xr=function(r,e,t,i){if(e&&typeof e==\"object\"||typeof e==\"function\")for(var a=zr(e),n=0,o=a.length,s;n<o;n++)s=a[n],!Yr.call(r,s)&&s!==t&&lt(r,s,{get:function(l){return e[l]}.bind(null,s),enumerable:!(i=Qr(e,s))||i.enumerable});return r};var W=function(r,e,t){return t=r!=null?Jr(Kr(r)):{},Xr(e||!r||!r.__esModule?lt(t,\"default\",{value:r,enumerable:!0}):t,r)};var G=H(function(Ui,vt){var ke;typeof window!=\"undefined\"?ke=window:typeof global!=\"undefined\"?ke=global:typeof self!=\"undefined\"?ke=self:ke={};vt.exports=ke});function B(r,e){return e!=null&&typeof Symbol!=\"undefined\"&&e[Symbol.hasInstance]?!!e[Symbol.hasInstance](r):B(r,e)}var ee=ct(function(){ee()});function le(r){\"@swc/helpers - typeof\";return r&&typeof Symbol!=\"undefined\"&&r.constructor===Symbol?\"symbol\":typeof r}var Pe=ct(function(){});var ze=H(function(pu,fr){var _r=Array.prototype.slice;fr.exports=Wa;function Wa(r,e){for((\"length\"in r)||(r=[r]),r=_r.call(r);r.length;){var t=r.shift(),i=e(t);if(i)return i;t.childNodes&&t.childNodes.length&&(r=_r.call(t.childNodes).concat(r))}}});var vr=H(function(mu,pr){ee();pr.exports=ve;function ve(r,e){if(!B(this,ve))return new ve(r,e);this.data=r,this.nodeValue=r,this.length=r.length,this.ownerDocument=e||null}ve.prototype.nodeType=8;ve.prototype.nodeName=\"#comment\";ve.prototype.toString=function(){return\"[object Comment]\"}});var hr=H(function(yu,mr){ee();mr.exports=ae;function ae(r,e){if(!B(this,ae))return new ae(r);this.data=r||\"\",this.length=this.data.length,this.ownerDocument=e||null}ae.prototype.type=\"DOMTextNode\";ae.prototype.nodeType=3;ae.prototype.nodeName=\"#text\";ae.prototype.toString=function(){return this.data};ae.prototype.replaceData=function(e,t,i){var a=this.data,n=a.substring(0,e),o=a.substring(e+t,a.length);this.data=n+i+o,this.length=this.data.length}});var Ke=H(function(gu,yr){yr.exports=Ga;function Ga(r){var e=this,t=r.type;r.target||(r.target=e),e.listeners||(e.listeners={});var i=e.listeners[t];if(i)return i.forEach(function(a){r.currentTarget=e,typeof a==\"function\"?a(r):a.handleEvent(r)});e.parentNode&&e.parentNode.dispatchEvent(r)}});var Ye=H(function(bu,gr){gr.exports=Ja;function Ja(r,e){var t=this;t.listeners||(t.listeners={}),t.listeners[r]||(t.listeners[r]=[]),t.listeners[r].indexOf(e)===-1&&t.listeners[r].push(e)}});var Xe=H(function(Tu,br){br.exports=Qa;function Qa(r,e){var t=this;if(t.listeners&&t.listeners[r]){var i=t.listeners[r],a=i.indexOf(e);a!==-1&&i.splice(a,1)}}});var kr=H(function(Eu,Er){Pe();Er.exports=Tr;var za=[\"area\",\"base\",\"br\",\"col\",\"embed\",\"hr\",\"img\",\"input\",\"keygen\",\"link\",\"menuitem\",\"meta\",\"param\",\"source\",\"track\",\"wbr\"];function Tr(r){switch(r.nodeType){case 3:return $e(r.data);case 8:return\"<!--\"+r.data+\"-->\";default:return Ka(r)}}function Ka(r){var e=[],t=r.tagName;return r.namespaceURI===\"http://www.w3.org/1999/xhtml\"&&(t=t.toLowerCase()),e.push(\"<\"+t+Za(r)+$a(r)),za.indexOf(t)>-1?e.push(\" />\"):(e.push(\">\"),r.childNodes.length?e.push.apply(e,r.childNodes.map(Tr)):r.textContent||r.innerText?e.push($e(r.textContent||r.innerText)):r.innerHTML&&e.push(r.innerHTML),e.push(\"</\"+t+\">\")),e.join(\"\")}function Ya(r,e){var t=le(r[e]);return e===\"style\"&&Object.keys(r.style).length>0?!0:r.hasOwnProperty(e)&&(t===\"string\"||t===\"boolean\"||t===\"number\")&&e!==\"nodeName\"&&e!==\"className\"&&e!==\"tagName\"&&e!==\"textContent\"&&e!==\"innerText\"&&e!==\"namespaceURI\"&&e!==\"innerHTML\"}function Xa(r){if(typeof r==\"string\")return r;var e=\"\";return Object.keys(r).forEach(function(t){var i=r[t];t=t.replace(/[A-Z]/g,function(a){return\"-\"+a.toLowerCase()}),e+=t+\":\"+i+\";\"}),e}function $a(r){var e=r.dataset,t=[];for(var i in e)t.push({name:\"data-\"+i,value:e[i]});return t.length?wr(t):\"\"}function wr(r){var e=[];return r.forEach(function(t){var i=t.name,a=t.value;i===\"style\"&&(a=Xa(a)),e.push(i+'=\"'+ei(a)+'\"')}),e.length?\" \"+e.join(\" \"):\"\"}function Za(r){var e=[];for(var t in r)Ya(r,t)&&e.push({name:t,value:r[t]});for(var i in r._attributes)for(var a in r._attributes[i]){var n=r._attributes[i][a],o=(n.prefix?n.prefix+\":\":\"\")+a;e.push({name:o,value:n.value})}return r.className&&e.push({name:\"class\",value:r.className}),e.length?wr(e):\"\"}function $e(r){var e=\"\";return typeof r==\"string\"?e=r:r&&(e=r.toString()),e.replace(/&/g,\"&amp;\").replace(/</g,\"&lt;\").replace(/>/g,\"&gt;\")}function ei(r){return $e(r).replace(/\"/g,\"&quot;\")}});var et=H(function(xu,Dr){ee();var Ze=ze(),ti=Ke(),ri=Ye(),ai=Xe(),ii=kr(),xr=\"http://www.w3.org/1999/xhtml\";Dr.exports=N;function N(r,e,t){if(!B(this,N))return new N(r);var i=t===void 0?xr:t||null;this.tagName=i===xr?String(r).toUpperCase():r,this.nodeName=this.tagName,this.className=\"\",this.dataset={},this.childNodes=[],this.parentNode=null,this.style={},this.ownerDocument=e||null,this.namespaceURI=i,this._attributes={},this.tagName===\"INPUT\"&&(this.type=\"text\")}N.prototype.type=\"DOMElement\";N.prototype.nodeType=1;N.prototype.appendChild=function(e){return e.parentNode&&e.parentNode.removeChild(e),this.childNodes.push(e),e.parentNode=this,e};N.prototype.replaceChild=function(e,t){e.parentNode&&e.parentNode.removeChild(e);var i=this.childNodes.indexOf(t);return t.parentNode=null,this.childNodes[i]=e,e.parentNode=this,t};N.prototype.removeChild=function(e){var t=this.childNodes.indexOf(e);return this.childNodes.splice(t,1),e.parentNode=null,e};N.prototype.insertBefore=function(e,t){e.parentNode&&e.parentNode.removeChild(e);var i=t==null?-1:this.childNodes.indexOf(t);return i>-1?this.childNodes.splice(i,0,e):this.childNodes.push(e),e.parentNode=this,e};N.prototype.setAttributeNS=function(e,t,i){var a=null,n=t,o=t.indexOf(\":\");if(o>-1&&(a=t.substr(0,o),n=t.substr(o+1)),this.tagName===\"INPUT\"&&t===\"type\")this.type=i;else{var s=this._attributes[e]||(this._attributes[e]={});s[n]={value:i,prefix:a}}};N.prototype.getAttributeNS=function(e,t){var i=this._attributes[e],a=i&&i[t]&&i[t].value;return this.tagName===\"INPUT\"&&t===\"type\"?this.type:typeof a!=\"string\"?null:a};N.prototype.removeAttributeNS=function(e,t){var i=this._attributes[e];i&&delete i[t]};N.prototype.hasAttributeNS=function(e,t){var i=this._attributes[e];return!!i&&t in i};N.prototype.setAttribute=function(e,t){return this.setAttributeNS(null,e,t)};N.prototype.getAttribute=function(e){return this.getAttributeNS(null,e)};N.prototype.removeAttribute=function(e){return this.removeAttributeNS(null,e)};N.prototype.hasAttribute=function(e){return this.hasAttributeNS(null,e)};N.prototype.removeEventListener=ai;N.prototype.addEventListener=ri;N.prototype.dispatchEvent=ti;N.prototype.focus=function(){};N.prototype.toString=function(){return ii(this)};N.prototype.getElementsByClassName=function(e){var t=e.split(\" \"),i=[];return Ze(this,function(a){if(a.nodeType===1){var n=a.className||\"\",o=n.split(\" \");t.every(function(s){return o.indexOf(s)!==-1})&&i.push(a)}}),i};N.prototype.getElementsByTagName=function(e){e=e.toLowerCase();var t=[];return Ze(this.childNodes,function(i){i.nodeType===1&&(e===\"*\"||i.tagName.toLowerCase()===e)&&t.push(i)}),t};N.prototype.contains=function(e){return Ze(this,function(t){return e===t})||!1}});var Rr=H(function(Su,Sr){ee();var tt=et();Sr.exports=z;function z(r){if(!B(this,z))return new z;this.childNodes=[],this.parentNode=null,this.ownerDocument=r||null}z.prototype.type=\"DocumentFragment\";z.prototype.nodeType=11;z.prototype.nodeName=\"#document-fragment\";z.prototype.appendChild=tt.prototype.appendChild;z.prototype.replaceChild=tt.prototype.replaceChild;z.prototype.removeChild=tt.prototype.removeChild;z.prototype.toString=function(){return this.childNodes.map(function(e){return String(e)}).join(\"\")}});var Ar=H(function(Ru,qr){qr.exports=rt;function rt(r){}rt.prototype.initEvent=function(e,t,i){this.type=e,this.bubbles=t,this.cancelable=i};rt.prototype.preventDefault=function(){}});var Ir=H(function(Au,Or){ee();var ni=ze(),oi=vr(),si=hr(),Re=et(),ui=Rr(),di=Ar(),li=Ke(),ci=Ye(),_i=Xe();Or.exports=Ue;function Ue(){if(!B(this,Ue))return new Ue;this.head=this.createElement(\"head\"),this.body=this.createElement(\"body\"),this.documentElement=this.createElement(\"html\"),this.documentElement.appendChild(this.head),this.documentElement.appendChild(this.body),this.childNodes=[this.documentElement],this.nodeType=9}var F=Ue.prototype;F.createTextNode=function(e){return new si(e,this)};F.createElementNS=function(e,t){var i=e===null?null:String(e);return new Re(t,this,i)};F.createElement=function(e){return new Re(e,this)};F.createDocumentFragment=function(){return new ui(this)};F.createEvent=function(e){return new di(e)};F.createComment=function(e){return new oi(e,this)};F.getElementById=function(e){e=String(e);var t=ni(this.childNodes,function(i){if(String(i.id)===e)return i});return t||null};F.getElementsByClassName=Re.prototype.getElementsByClassName;F.getElementsByTagName=Re.prototype.getElementsByTagName;F.contains=Re.prototype.contains;F.removeEventListener=_i;F.addEventListener=ci;F.dispatchEvent=li});var Pr=H(function(Ou,Nr){var fi=Ir();Nr.exports=new fi});var Mr=H(function(Iu,Cr){var Lr=typeof global!=\"undefined\"?global:typeof window!=\"undefined\"?window:{},pi=Pr(),qe;typeof document!=\"undefined\"?qe=document:(qe=Lr[\"__GLOBAL_DOCUMENT_CACHE@4\"],qe||(qe=Lr[\"__GLOBAL_DOCUMENT_CACHE@4\"]=pi));Cr.exports=qe});function _t(r){if(Array.isArray(r))return r}function ft(r,e){var t=r==null?null:typeof Symbol!=\"undefined\"&&r[Symbol.iterator]||r[\"@@iterator\"];if(t!=null){var i=[],a=!0,n=!1,o,s;try{for(t=t.call(r);!(a=(o=t.next()).done)&&(i.push(o.value),!(e&&i.length===e));a=!0);}catch(l){n=!0,s=l}finally{try{!a&&t.return!=null&&t.return()}finally{if(n)throw s}}return i}}function pt(){throw new TypeError(\"Invalid attempt to destructure non-iterable instance.\\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.\")}function Ee(r,e){(e==null||e>r.length)&&(e=r.length);for(var t=0,i=new Array(e);t<e;t++)i[t]=r[t];return i}function Ae(r,e){if(r){if(typeof r==\"string\")return Ee(r,e);var t=Object.prototype.toString.call(r).slice(8,-1);if(t===\"Object\"&&r.constructor&&(t=r.constructor.name),t===\"Map\"||t===\"Set\")return Array.from(t);if(t===\"Arguments\"||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(t))return Ee(r,e)}}function M(r,e){return _t(r)||ft(r,e)||Ae(r,e)||pt()}var ge=W(G());var $=function(){return\"xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx\".replace(/[xy]/g,function(e){var t=Math.random()*16|0,i=e===\"x\"?t:t&3|8;return i.toString(16)})},Oe=function(){return(\"000000\"+(Math.random()*Math.pow(36,6)<<0).toString(36)).slice(-6)};var Z=function(e){if(e&&typeof e.nodeName!=\"undefined\")return e.muxId||(e.muxId=Oe()),e.muxId;var t;try{t=document.querySelector(e)}catch(i){}return t&&!t.muxId&&(t.muxId=e),(t==null?void 0:t.muxId)||e},xe=function(e){var t;e&&typeof e.nodeName!=\"undefined\"?(t=e,e=Z(t)):t=document.querySelector(e);var i=t&&t.nodeName?t.nodeName.toLowerCase():\"\";return[t,e,i]};function mt(r){if(Array.isArray(r))return Ee(r)}function ht(r){if(typeof Symbol!=\"undefined\"&&r[Symbol.iterator]!=null||r[\"@@iterator\"]!=null)return Array.from(r)}function yt(){throw new TypeError(\"Invalid attempt to spread non-iterable instance.\\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.\")}function J(r){return mt(r)||ht(r)||Ae(r)||yt()}var K={TRACE:0,DEBUG:1,INFO:2,WARN:3,ERROR:4,SILENT:5},gt=function(r){var e=arguments.length>1&&arguments[1]!==void 0?arguments[1]:3,t,i,a,n,o,s=r?[console,r]:[console],l=(t=console.trace).bind.apply(t,J(s)),m=(i=console.info).bind.apply(i,J(s)),g=(a=console.debug).bind.apply(a,J(s)),T=(n=console.warn).bind.apply(n,J(s)),w=(o=console.error).bind.apply(o,J(s)),f=e;return{trace:function(){for(var E=arguments.length,x=new Array(E),v=0;v<E;v++)x[v]=arguments[v];if(!(f>K.TRACE))return l.apply(void 0,J(x))},debug:function(){for(var E=arguments.length,x=new Array(E),v=0;v<E;v++)x[v]=arguments[v];if(!(f>K.DEBUG))return g.apply(void 0,J(x))},info:function(){for(var E=arguments.length,x=new Array(E),v=0;v<E;v++)x[v]=arguments[v];if(!(f>K.INFO))return m.apply(void 0,J(x))},warn:function(){for(var E=arguments.length,x=new Array(E),v=0;v<E;v++)x[v]=arguments[v];if(!(f>K.WARN))return T.apply(void 0,J(x))},error:function(){for(var E=arguments.length,x=new Array(E),v=0;v<E;v++)x[v]=arguments[v];if(!(f>K.ERROR))return w.apply(void 0,J(x))},get level(){return f},set level(_){_!==this.level&&(f=_!=null?_:e)}}};var O=gt(\"[mux]\");var Ie=W(G());function de(){var r=Ie.default.doNotTrack||Ie.default.navigator&&Ie.default.navigator.doNotTrack;return r===\"1\"}var bt=W(G()),$r={now:function(){var r=bt.default.performance,e=r&&r.timing,t=e&&e.navigationStart,i=typeof t==\"number\"&&typeof r.now==\"function\"?t+r.now():Date.now();return Math.round(i)}},q=$r;function h(r){if(r===void 0)throw new ReferenceError(\"this hasn't been initialised - super() hasn't been called\");return r}ee();function D(r,e){if(!B(r,e))throw new TypeError(\"Cannot call a class as a function\")}function Tt(r,e){for(var t=0;t<e.length;t++){var i=e[t];i.enumerable=i.enumerable||!1,i.configurable=!0,\"value\"in i&&(i.writable=!0),Object.defineProperty(r,i.key,i)}}function L(r,e,t){return e&&Tt(r.prototype,e),t&&Tt(r,t),r}function u(r,e,t){return e in r?Object.defineProperty(r,e,{value:t,enumerable:!0,configurable:!0,writable:!0}):r[e]=t,r}function Ne(r,e){return Ne=Object.setPrototypeOf||function(i,a){return i.__proto__=a,i},Ne(r,e)}function wt(r,e){if(typeof e!=\"function\"&&e!==null)throw new TypeError(\"Super expression must either be null or a function\");r.prototype=Object.create(e&&e.prototype,{constructor:{value:r,writable:!0,configurable:!0}}),e&&Ne(r,e)}function De(r){return De=Object.setPrototypeOf?Object.getPrototypeOf:function(t){return t.__proto__||Object.getPrototypeOf(t)},De(r)}function Et(){if(typeof Reflect==\"undefined\"||!Reflect.construct||Reflect.construct.sham)return!1;if(typeof Proxy==\"function\")return!0;try{return Boolean.prototype.valueOf.call(Reflect.construct(Boolean,[],function(){})),!0}catch(r){return!1}}Pe();function kt(r,e){return e&&(le(e)===\"object\"||typeof e==\"function\")?e:h(r)}function xt(r){var e=Et();return function(){var i=De(r),a;if(e){var n=De(this).constructor;a=Reflect.construct(i,arguments,n)}else a=i.apply(this,arguments);return kt(this,a)}}var U=function(r){return te(r)[0]};var te=function(r){if(typeof r!=\"string\"||r===\"\")return[\"localhost\"];var e=/^(([^:\\/?#]+):)?(\\/\\/([^\\/?#]*))?([^?#]*)(\\?([^#]*))?(#(.*))?/,t=r.match(e)||[],i=t[4],a;return i&&(a=(i.match(/[^\\.]+\\.[^\\.]+$/)||[])[0]),[i,a]};var Le=W(G()),Zr={exists:function(){var r=Le.default.performance,e=r&&r.timing;return e!==void 0},domContentLoadedEventEnd:function(){var r=Le.default.performance,e=r&&r.timing;return e&&e.domContentLoadedEventEnd},navigationStart:function(){var r=Le.default.performance,e=r&&r.timing;return e&&e.navigationStart}},ce=Zr;function A(r,e,t){t=t===void 0?1:t,r[e]=r[e]||0,r[e]+=t}function oe(r){for(var e=1;e<arguments.length;e++){var t=arguments[e]!=null?arguments[e]:{},i=Object.keys(t);typeof Object.getOwnPropertySymbols==\"function\"&&(i=i.concat(Object.getOwnPropertySymbols(t).filter(function(a){return Object.getOwnPropertyDescriptor(t,a).enumerable}))),i.forEach(function(a){u(r,a,t[a])})}return r}function ea(r,e){var t=Object.keys(r);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(r);e&&(i=i.filter(function(a){return Object.getOwnPropertyDescriptor(r,a).enumerable})),t.push.apply(t,i)}return t}function _e(r,e){return e=e!=null?e:{},Object.getOwnPropertyDescriptors?Object.defineProperties(r,Object.getOwnPropertyDescriptors(e)):ea(Object(e)).forEach(function(t){Object.defineProperty(r,t,Object.getOwnPropertyDescriptor(e,t))}),r}var ta=[\"x-cdn\",\"content-type\"],Dt=[\"x-request-id\",\"cf-ray\",\"x-amz-cf-id\",\"x-akamai-request-id\"],ra=ta.concat(Dt);function fe(r){r=r||\"\";var e={},t=r.trim().split(/[\\r\\n]+/);return t.forEach(function(i){if(i){var a=i.split(\": \"),n=a.shift();n&&(ra.indexOf(n.toLowerCase())>=0||n.toLowerCase().indexOf(\"x-litix-\")===0)&&(e[n]=a.join(\": \"))}}),e}function se(r){if(r){var e=Dt.find(function(t){return r[t]!==void 0});return e?r[e]:void 0}}var aa=function(r){var e={};for(var t in r){var i=r[t],a=i[\"DATA-ID\"].search(\"io.litix.data.\");if(a!==-1){var n=i[\"DATA-ID\"].replace(\"io.litix.data.\",\"\");e[n]=i.VALUE}}return e},Ce=aa;var Me=function(r){if(!r)return{};var e=ce.navigationStart(),t=r.loading,i=t?t.start:r.trequest,a=t?t.first:r.tfirst,n=t?t.end:r.tload;return{bytesLoaded:r.total,requestStart:Math.round(e+i),responseStart:Math.round(e+a),responseEnd:Math.round(e+n)}},Se=function(r){if(!(!r||typeof r.getAllResponseHeaders!=\"function\"))return fe(r.getAllResponseHeaders())},St=function(r,e,t){var i=arguments.length>3&&arguments[3]!==void 0?arguments[3]:{},a=arguments.length>4?arguments[4]:void 0,n=r.log,o=r.utils.secondsToMs,s=function(v){var p=parseInt(a.version),c;return p===1&&v.programDateTime!==null&&(c=v.programDateTime),p===0&&v.pdt!==null&&(c=v.pdt),c};if(!ce.exists()){n.warn(\"performance timing not supported. Not tracking HLS.js.\");return}var l=function(v,p){return r.emit(e,v,p)},m=function(v,p){var c=p.levels,d=p.audioTracks,y=p.url,b=p.stats,k=p.networkDetails,S=p.sessionData,P={},C={};c.forEach(function(V,ne){P[ne]={width:V.width,height:V.height,bitrate:V.bitrate,attrs:V.attrs}}),d.forEach(function(V,ne){C[ne]={name:V.name,language:V.lang,bitrate:V.bitrate}});var I=Me(b),R=I.bytesLoaded,X=I.requestStart,be=I.responseStart,Te=I.responseEnd;l(\"requestcompleted\",_e(oe({},Ce(S)),{request_event_type:v,request_bytes_loaded:R,request_start:X,request_response_start:be,request_response_end:Te,request_type:\"manifest\",request_hostname:U(y),request_response_headers:Se(k),request_rendition_lists:{media:P,audio:C,video:{}}}))};t.on(a.Events.MANIFEST_LOADED,m);var g=function(v,p){var c=p.details,d=p.level,y=p.networkDetails,b=p.stats,k=Me(b),S=k.bytesLoaded,P=k.requestStart,C=k.responseStart,I=k.responseEnd,R=c.fragments[c.fragments.length-1],X=s(R)+o(R.duration);l(\"requestcompleted\",{request_event_type:v,request_bytes_loaded:S,request_start:P,request_response_start:C,request_response_end:I,request_current_level:d,request_type:\"manifest\",request_hostname:U(c.url),request_response_headers:Se(y),video_holdback:c.holdBack&&o(c.holdBack),video_part_holdback:c.partHoldBack&&o(c.partHoldBack),video_part_target_duration:c.partTarget&&o(c.partTarget),video_target_duration:c.targetduration&&o(c.targetduration),video_source_is_live:c.live,player_manifest_newest_program_time:isNaN(X)?void 0:X})};t.on(a.Events.LEVEL_LOADED,g);var T=function(v,p){var c=p.details,d=p.networkDetails,y=p.stats,b=Me(y),k=b.bytesLoaded,S=b.requestStart,P=b.responseStart,C=b.responseEnd;l(\"requestcompleted\",{request_event_type:v,request_bytes_loaded:k,request_start:S,request_response_start:P,request_response_end:C,request_type:\"manifest\",request_hostname:U(c.url),request_response_headers:Se(d)})};t.on(a.Events.AUDIO_TRACK_LOADED,T);var w=function(v,p){var c=p.stats,d=p.networkDetails,y=p.frag;c=c||y.stats;var b=Me(c),k=b.bytesLoaded,S=b.requestStart,P=b.responseStart,C=b.responseEnd,I=d?Se(d):void 0,R={request_event_type:v,request_bytes_loaded:k,request_start:S,request_response_start:P,request_response_end:C,request_hostname:d?U(d.responseURL):void 0,request_id:I?se(I):void 0,request_response_headers:I,request_media_duration:y.duration,request_url:d==null?void 0:d.responseURL};y.type===\"main\"?(R.request_type=\"media\",R.request_current_level=y.level,R.request_video_width=(t.levels[y.level]||{}).width,R.request_video_height=(t.levels[y.level]||{}).height,R.request_labeled_bitrate=(t.levels[y.level]||{}).bitrate):R.request_type=y.type,l(\"requestcompleted\",R)};t.on(a.Events.FRAG_LOADED,w);var f=function(v,p){var c=p.frag,d=c.start,y=s(c),b={currentFragmentPDT:y,currentFragmentStart:o(d)};l(\"fragmentchange\",b)};t.on(a.Events.FRAG_CHANGED,f);var _=function(v,p){var c=p.type,d=p.details,y=p.response,b=p.fatal,k=p.frag,S=p.networkDetails,P=(k==null?void 0:k.url)||p.url||\"\",C=S?Se(S):void 0;if((d===a.ErrorDetails.MANIFEST_LOAD_ERROR||d===a.ErrorDetails.MANIFEST_LOAD_TIMEOUT||d===a.ErrorDetails.FRAG_LOAD_ERROR||d===a.ErrorDetails.FRAG_LOAD_TIMEOUT||d===a.ErrorDetails.LEVEL_LOAD_ERROR||d===a.ErrorDetails.LEVEL_LOAD_TIMEOUT||d===a.ErrorDetails.AUDIO_TRACK_LOAD_ERROR||d===a.ErrorDetails.AUDIO_TRACK_LOAD_TIMEOUT||d===a.ErrorDetails.SUBTITLE_LOAD_ERROR||d===a.ErrorDetails.SUBTITLE_LOAD_TIMEOUT||d===a.ErrorDetails.KEY_LOAD_ERROR||d===a.ErrorDetails.KEY_LOAD_TIMEOUT)&&l(\"requestfailed\",{request_error:d,request_url:P,request_hostname:U(P),request_id:C?se(C):void 0,request_type:d===a.ErrorDetails.FRAG_LOAD_ERROR||d===a.ErrorDetails.FRAG_LOAD_TIMEOUT?\"media\":d===a.ErrorDetails.AUDIO_TRACK_LOAD_ERROR||d===a.ErrorDetails.AUDIO_TRACK_LOAD_TIMEOUT?\"audio\":d===a.ErrorDetails.SUBTITLE_LOAD_ERROR||d===a.ErrorDetails.SUBTITLE_LOAD_TIMEOUT?\"subtitle\":d===a.ErrorDetails.KEY_LOAD_ERROR||d===a.ErrorDetails.KEY_LOAD_TIMEOUT?\"encryption\":\"manifest\",request_error_code:y==null?void 0:y.code,request_error_text:y==null?void 0:y.text}),b){var I,R=\"\".concat(P?\"url: \".concat(P,\"\\n\"):\"\")+\"\".concat(y&&(y.code||y.text)?\"response: \".concat(y.code,\", \").concat(y.text,\"\\n\"):\"\")+\"\".concat(p.reason?\"failure reason: \".concat(p.reason,\"\\n\"):\"\")+\"\".concat(p.level?\"level: \".concat(p.level,\"\\n\"):\"\")+\"\".concat(p.parent?\"parent stream controller: \".concat(p.parent,\"\\n\"):\"\")+\"\".concat(p.buffer?\"buffer length: \".concat(p.buffer,\"\\n\"):\"\")+\"\".concat(p.error?\"error: \".concat(p.error,\"\\n\"):\"\")+\"\".concat(p.event?\"event: \".concat(p.event,\"\\n\"):\"\")+\"\".concat(p.err?\"error message: \".concat((I=p.err)===null||I===void 0?void 0:I.message,\"\\n\"):\"\");l(\"error\",{player_error_code:c,player_error_message:d,player_error_context:R})}};t.on(a.Events.ERROR,_);var E=function(v,p){var c=p.frag,d=c&&c._url||\"\";l(\"requestcanceled\",{request_event_type:v,request_url:d,request_type:\"media\",request_hostname:U(d)})};t.on(a.Events.FRAG_LOAD_EMERGENCY_ABORTED,E);var x=function(v,p){var c=p.level,d=t.levels[c];if(d&&d.attrs&&d.attrs.BANDWIDTH){var y=d.attrs.BANDWIDTH,b,k=parseFloat(d.attrs[\"FRAME-RATE\"]);isNaN(k)||(b=k),y?l(\"renditionchange\",{video_source_fps:b,video_source_bitrate:y,video_source_width:d.width,video_source_height:d.height,video_source_rendition_name:d.name,video_source_codec:d==null?void 0:d.videoCodec}):n.warn(\"missing BANDWIDTH from HLS manifest parsed by HLS.js\")}};t.on(a.Events.LEVEL_SWITCHED,x),t._stopMuxMonitor=function(){t.off(a.Events.MANIFEST_LOADED,m),t.off(a.Events.LEVEL_LOADED,g),t.off(a.Events.AUDIO_TRACK_LOADED,T),t.off(a.Events.FRAG_LOADED,w),t.off(a.Events.FRAG_CHANGED,f),t.off(a.Events.ERROR,_),t.off(a.Events.FRAG_LOAD_EMERGENCY_ABORTED,E),t.off(a.Events.LEVEL_SWITCHED,x),t.off(a.Events.DESTROYING,t._stopMuxMonitor),delete t._stopMuxMonitor},t.on(a.Events.DESTROYING,t._stopMuxMonitor)},Rt=function(r){r&&typeof r._stopMuxMonitor==\"function\"&&r._stopMuxMonitor()};var qt=function(r,e){if(!r||!r.requestEndDate)return{};var t=U(r.url),i=r.url,a=r.bytesLoaded,n=new Date(r.requestStartDate).getTime(),o=new Date(r.firstByteDate).getTime(),s=new Date(r.requestEndDate).getTime(),l=isNaN(r.duration)?0:r.duration,m=typeof e.getMetricsFor==\"function\"?e.getMetricsFor(r.mediaType).HttpList:e.getDashMetrics().getHttpRequests(r.mediaType),g;m.length>0&&(g=fe(m[m.length-1]._responseHeaders||\"\"));var T=g?se(g):void 0;return{requestStart:n,requestResponseStart:o,requestResponseEnd:s,requestBytesLoaded:a,requestResponseHeaders:g,requestMediaDuration:l,requestHostname:t,requestUrl:i,requestId:T}},ia=function(r,e){var t=e.getQualityFor(r),i=e.getCurrentTrackFor(r).bitrateList;return i?{currentLevel:t,renditionWidth:i[t].width||null,renditionHeight:i[t].height||null,renditionBitrate:i[t].bandwidth}:{}},na=function(r){var e;return(e=r.match(/.*codecs\\*?=\"(.*)\"/))===null||e===void 0?void 0:e[1]},oa=function(e){try{var t,i,a=(i=e.getVersion)===null||i===void 0||(t=i.call(e))===null||t===void 0?void 0:t.split(\".\").map(function(n){return parseInt(n)})[0];return a}catch(n){return!1}},At=function(r,e,t){var i=arguments.length>3&&arguments[3]!==void 0?arguments[3]:{},a=r.log;if(!t||!t.on){a.warn(\"Invalid dash.js player reference. Monitoring blocked.\");return}var n=oa(t),o=function(c,d){return r.emit(e,c,d)},s=function(c){var d=c.type,y=c.data,b=(y||{}).url;o(\"requestcompleted\",{request_event_type:d,request_start:0,request_response_start:0,request_response_end:0,request_bytes_loaded:-1,request_type:\"manifest\",request_hostname:U(b),request_url:b})};t.on(\"manifestLoaded\",s);var l={},m=function(c){if(typeof c.getRequests!=\"function\")return null;var d=c.getRequests({state:\"executed\"});return d.length===0?null:d[d.length-1]},g=function(c){var d=c.type,y=c.fragmentModel,b=c.chunk,k=m(y);T({type:d,request:k,chunk:b})},T=function(c){var d=c.type,y=c.chunk,b=c.request,k=(y||{}).mediaInfo,S=k||{},P=S.type,C=S.bitrateList;C=C||[];var I={};C.forEach(function(we,Q){I[Q]={},I[Q].width=we.width,I[Q].height=we.height,I[Q].bitrate=we.bandwidth,I[Q].attrs={}}),P===\"video\"?l.video=I:P===\"audio\"?l.audio=I:l.media=I;var R=qt(b,t),X=R.requestStart,be=R.requestResponseStart,Te=R.requestResponseEnd,V=R.requestResponseHeaders,ne=R.requestMediaDuration,Ve=R.requestHostname,je=R.requestUrl,We=R.requestId;o(\"requestcompleted\",{request_event_type:d,request_start:X,request_response_start:be,request_response_end:Te,request_bytes_loaded:-1,request_type:P+\"_init\",request_response_headers:V,request_hostname:Ve,request_id:We,request_url:je,request_media_duration:ne,request_rendition_lists:l})};n>=4?t.on(\"initFragmentLoaded\",T):t.on(\"initFragmentLoaded\",g);var w=function(c){var d=c.type,y=c.fragmentModel,b=c.chunk,k=m(y);f({type:d,request:k,chunk:b})},f=function(c){var d=c.type,y=c.chunk,b=c.request,k=y||{},S=k.mediaInfo,P=k.start,C=S||{},I=C.type,R=qt(b,t),X=R.requestStart,be=R.requestResponseStart,Te=R.requestResponseEnd,V=R.requestBytesLoaded,ne=R.requestResponseHeaders,Ve=R.requestMediaDuration,je=R.requestHostname,We=R.requestUrl,we=R.requestId,Q=ia(I,t),Vr=Q.currentLevel,jr=Q.renditionWidth,Wr=Q.renditionHeight,Gr=Q.renditionBitrate;o(\"requestcompleted\",{request_event_type:d,request_start:X,request_response_start:be,request_response_end:Te,request_bytes_loaded:V,request_type:I,request_response_headers:ne,request_hostname:je,request_id:we,request_url:We,request_media_start_time:P,request_media_duration:Ve,request_current_level:Vr,request_labeled_bitrate:Gr,request_video_width:jr,request_video_height:Wr})};n>=4?t.on(\"mediaFragmentLoaded\",f):t.on(\"mediaFragmentLoaded\",w);var _={video:void 0,audio:void 0,totalBitrate:void 0},E=function(){if(_.video&&typeof _.video.bitrate==\"number\"){if(!(_.video.width&&_.video.height)){a.warn(\"have bitrate info for video but missing width/height\");return}var c=_.video.bitrate;if(_.audio&&typeof _.audio.bitrate==\"number\"&&(c+=_.audio.bitrate),c!==_.totalBitrate)return _.totalBitrate=c,{video_source_bitrate:c,video_source_height:_.video.height,video_source_width:_.video.width,video_source_codec:na(_.video.codec)}}},x=function(c,d,y){if(typeof c.newQuality!=\"number\"){a.warn(\"missing evt.newQuality in qualityChangeRendered event\",c);return}var b=c.mediaType;if(b===\"audio\"||b===\"video\"){var k=t.getBitrateInfoListFor(b).find(function(P){var C=P.qualityIndex;return C===c.newQuality});if(!(k&&typeof k.bitrate==\"number\")){a.warn(\"missing bitrate info for \".concat(b));return}_[b]=_e(oe({},k),{codec:t.getCurrentTrackFor(b).codec});var S=E();S&&o(\"renditionchange\",S)}};t.on(\"qualityChangeRendered\",x);var v=function(c){var d=c.request,y=c.mediaType;d=d||{},o(\"requestcanceled\",{request_event_type:d.type+\"_\"+d.action,request_url:d.url,request_type:y,request_hostname:U(d.url)})};t.on(\"fragmentLoadingAbandoned\",v);var p=function(c){var d=c.error,y,b,k=(d==null||(y=d.data)===null||y===void 0?void 0:y.request)||{},S=(d==null||(b=d.data)===null||b===void 0?void 0:b.response)||{};(d==null?void 0:d.code)===27&&o(\"requestfailed\",{request_error:k.type+\"_\"+k.action,request_url:k.url,request_hostname:U(k.url),request_type:k.mediaType,request_error_code:S.status,request_error_text:S.statusText});var P=\"\".concat(k!=null&&k.url?\"url: \".concat(k.url,\"\\n\"):\"\")+\"\".concat(S!=null&&S.status||S!=null&&S.statusText?\"response: \".concat(S==null?void 0:S.status,\", \").concat(S==null?void 0:S.statusText,\"\\n\"):\"\");o(\"error\",{player_error_code:d==null?void 0:d.code,player_error_message:d==null?void 0:d.message,player_error_context:P})};t.on(\"error\",p),t._stopMuxMonitor=function(){t.off(\"manifestLoaded\",s),t.off(\"initFragmentLoaded\",T),t.off(\"mediaFragmentLoaded\",f),t.off(\"qualityChangeRendered\",x),t.off(\"error\",p),t.off(\"fragmentLoadingAbandoned\",v),delete t._stopMuxMonitor}},Ot=function(r){r&&typeof r._stopMuxMonitor==\"function\"&&r._stopMuxMonitor()};var It=0,sa=function(){\"use strict\";function r(){D(this,r),u(this,\"_listeners\",void 0)}return L(r,[{key:\"on\",value:function(t,i,a){return i._eventEmitterGuid=i._eventEmitterGuid||++It,this._listeners=this._listeners||{},this._listeners[t]=this._listeners[t]||[],a&&(i=i.bind(a)),this._listeners[t].push(i),i}},{key:\"off\",value:function(t,i){var a=this._listeners&&this._listeners[t];a&&a.forEach(function(n,o){n._eventEmitterGuid===i._eventEmitterGuid&&a.splice(o,1)})}},{key:\"one\",value:function(t,i,a){var n=this;i._eventEmitterGuid=i._eventEmitterGuid||++It;var o=function(){n.off(t,o),i.apply(a||this,arguments)};o._eventEmitterGuid=i._eventEmitterGuid,this.on(t,o)}},{key:\"emit\",value:function(t,i){var a=this;if(this._listeners){i=i||{};var n=this._listeners[\"before*\"]||[],o=this._listeners[t]||[],s=this._listeners[\"after\"+t]||[],l=function(m,g){m=m.slice(),m.forEach(function(T){T.call(a,{type:t},g)})};l(n,i),l(o,i),l(s,i)}}}]),r}(),Nt=sa;var He=W(G()),ua=function(){\"use strict\";function r(e){var t=this;D(this,r),u(this,\"_playbackHeartbeatInterval\",void 0),u(this,\"_playheadShouldBeProgressing\",void 0),u(this,\"pm\",void 0),this.pm=e,this._playbackHeartbeatInterval=null,this._playheadShouldBeProgressing=!1,e.on(\"playing\",function(){t._playheadShouldBeProgressing=!0}),e.on(\"play\",this._startPlaybackHeartbeatInterval.bind(this)),e.on(\"playing\",this._startPlaybackHeartbeatInterval.bind(this)),e.on(\"adbreakstart\",this._startPlaybackHeartbeatInterval.bind(this)),e.on(\"adplay\",this._startPlaybackHeartbeatInterval.bind(this)),e.on(\"adplaying\",this._startPlaybackHeartbeatInterval.bind(this)),e.on(\"devicewake\",this._startPlaybackHeartbeatInterval.bind(this)),e.on(\"viewstart\",this._startPlaybackHeartbeatInterval.bind(this)),e.on(\"rebufferstart\",this._startPlaybackHeartbeatInterval.bind(this)),e.on(\"pause\",this._stopPlaybackHeartbeatInterval.bind(this)),e.on(\"ended\",this._stopPlaybackHeartbeatInterval.bind(this)),e.on(\"viewend\",this._stopPlaybackHeartbeatInterval.bind(this)),e.on(\"error\",this._stopPlaybackHeartbeatInterval.bind(this)),e.on(\"aderror\",this._stopPlaybackHeartbeatInterval.bind(this)),e.on(\"adpause\",this._stopPlaybackHeartbeatInterval.bind(this)),e.on(\"adended\",this._stopPlaybackHeartbeatInterval.bind(this)),e.on(\"adbreakend\",this._stopPlaybackHeartbeatInterval.bind(this)),e.on(\"seeked\",function(){e.data.player_is_paused?t._stopPlaybackHeartbeatInterval():t._startPlaybackHeartbeatInterval()}),e.on(\"timeupdate\",function(){t._playbackHeartbeatInterval!==null&&e.emit(\"playbackheartbeat\")}),e.on(\"devicesleep\",function(i,a){t._playbackHeartbeatInterval!==null&&(He.default.clearInterval(t._playbackHeartbeatInterval),e.emit(\"playbackheartbeatend\",{viewer_time:a.viewer_time}),t._playbackHeartbeatInterval=null)})}return L(r,[{key:\"_startPlaybackHeartbeatInterval\",value:function(){var t=this;this._playbackHeartbeatInterval===null&&(this.pm.emit(\"playbackheartbeat\"),this._playbackHeartbeatInterval=He.default.setInterval(function(){t.pm.emit(\"playbackheartbeat\")},this.pm.playbackHeartbeatTime))}},{key:\"_stopPlaybackHeartbeatInterval\",value:function(){this._playheadShouldBeProgressing=!1,this._playbackHeartbeatInterval!==null&&(He.default.clearInterval(this._playbackHeartbeatInterval),this.pm.emit(\"playbackheartbeatend\"),this._playbackHeartbeatInterval=null)}}]),r}(),Pt=ua;var da=function r(e){\"use strict\";var t=this;D(this,r),u(this,\"viewErrored\",void 0),e.on(\"viewinit\",function(){t.viewErrored=!1}),e.on(\"error\",function(i,a){try{var n=e.errorTranslator({player_error_code:a.player_error_code,player_error_message:a.player_error_message,player_error_context:a.player_error_context,player_error_severity:a.player_error_severity,player_error_business_exception:a.player_error_business_exception});n&&(e.data.player_error_code=n.player_error_code||a.player_error_code,e.data.player_error_message=n.player_error_message||a.player_error_message,e.data.player_error_context=n.player_error_context||a.player_error_context,e.data.player_error_severity=n.player_error_severity||a.player_error_severity,e.data.player_error_business_exception=n.player_error_business_exception||a.player_error_business_exception,t.viewErrored=!0)}catch(o){e.mux.log.warn(\"Exception in error translator callback.\",o),t.viewErrored=!0}}),e.on(\"aftererror\",function(){var i,a,n,o,s;(i=e.data)===null||i===void 0||delete i.player_error_code,(a=e.data)===null||a===void 0||delete a.player_error_message,(n=e.data)===null||n===void 0||delete n.player_error_context,(o=e.data)===null||o===void 0||delete o.player_error_severity,(s=e.data)===null||s===void 0||delete s.player_error_business_exception})},Lt=da;var la=function(){\"use strict\";function r(e){D(this,r),u(this,\"_watchTimeTrackerLastCheckedTime\",void 0),u(this,\"pm\",void 0),this.pm=e,this._watchTimeTrackerLastCheckedTime=null,e.on(\"playbackheartbeat\",this._updateWatchTime.bind(this)),e.on(\"playbackheartbeatend\",this._clearWatchTimeState.bind(this))}return L(r,[{key:\"_updateWatchTime\",value:function(t,i){var a=i.viewer_time;this._watchTimeTrackerLastCheckedTime===null&&(this._watchTimeTrackerLastCheckedTime=a),A(this.pm.data,\"view_watch_time\",a-this._watchTimeTrackerLastCheckedTime),this._watchTimeTrackerLastCheckedTime=a}},{key:\"_clearWatchTimeState\",value:function(t,i){this._updateWatchTime(t,i),this._watchTimeTrackerLastCheckedTime=null}}]),r}(),Ct=la;var ca=function(){\"use strict\";function r(e){var t=this;D(this,r),u(this,\"_playbackTimeTrackerLastPlayheadPosition\",void 0),u(this,\"_lastTime\",void 0),u(this,\"_isAdPlaying\",void 0),u(this,\"_callbackUpdatePlaybackTime\",void 0),u(this,\"pm\",void 0),this.pm=e,this._playbackTimeTrackerLastPlayheadPosition=-1,this._lastTime=q.now(),this._isAdPlaying=!1,this._callbackUpdatePlaybackTime=null;var i=this._startPlaybackTimeTracking.bind(this);e.on(\"playing\",i),e.on(\"adplaying\",i),e.on(\"seeked\",i);var a=this._stopPlaybackTimeTracking.bind(this);e.on(\"playbackheartbeatend\",a),e.on(\"seeking\",a),e.on(\"adplaying\",function(){t._isAdPlaying=!0}),e.on(\"adended\",function(){t._isAdPlaying=!1}),e.on(\"adpause\",function(){t._isAdPlaying=!1}),e.on(\"adbreakstart\",function(){t._isAdPlaying=!1}),e.on(\"adbreakend\",function(){t._isAdPlaying=!1}),e.on(\"adplay\",function(){t._isAdPlaying=!1}),e.on(\"viewinit\",function(){t._playbackTimeTrackerLastPlayheadPosition=-1,t._lastTime=q.now(),t._isAdPlaying=!1,t._callbackUpdatePlaybackTime=null})}return L(r,[{key:\"_startPlaybackTimeTracking\",value:function(){this._callbackUpdatePlaybackTime===null&&(this._callbackUpdatePlaybackTime=this._updatePlaybackTime.bind(this),this._playbackTimeTrackerLastPlayheadPosition=this.pm.data.player_playhead_time,this.pm.on(\"playbackheartbeat\",this._callbackUpdatePlaybackTime))}},{key:\"_stopPlaybackTimeTracking\",value:function(){this._callbackUpdatePlaybackTime&&(this._updatePlaybackTime(),this.pm.off(\"playbackheartbeat\",this._callbackUpdatePlaybackTime),this._callbackUpdatePlaybackTime=null,this._playbackTimeTrackerLastPlayheadPosition=-1)}},{key:\"_updatePlaybackTime\",value:function(){var t=this.pm.data.player_playhead_time,i=q.now(),a=-1;this._playbackTimeTrackerLastPlayheadPosition>=0&&t>this._playbackTimeTrackerLastPlayheadPosition?a=t-this._playbackTimeTrackerLastPlayheadPosition:this._isAdPlaying&&(a=i-this._lastTime),a>0&&a<=1e3&&A(this.pm.data,\"view_content_playback_time\",a),this._playbackTimeTrackerLastPlayheadPosition=t,this._lastTime=i}}]),r}(),Mt=ca;var _a=function(){\"use strict\";function r(e){D(this,r),u(this,\"pm\",void 0),this.pm=e;var t=this._updatePlayheadTime.bind(this);e.on(\"playbackheartbeat\",t),e.on(\"playbackheartbeatend\",t),e.on(\"timeupdate\",t),e.on(\"destroy\",function(){e.off(\"timeupdate\",t)})}return L(r,[{key:\"_updateMaxPlayheadPosition\",value:function(){this.pm.data.view_max_playhead_position=typeof this.pm.data.view_max_playhead_position==\"undefined\"?this.pm.data.player_playhead_time:Math.max(this.pm.data.view_max_playhead_position,this.pm.data.player_playhead_time)}},{key:\"_updatePlayheadTime\",value:function(t,i){var a=this,n=function(){a.pm.currentFragmentPDT&&a.pm.currentFragmentStart&&(a.pm.data.player_program_time=a.pm.currentFragmentPDT+a.pm.data.player_playhead_time-a.pm.currentFragmentStart)};if(i&&i.player_playhead_time)this.pm.data.player_playhead_time=i.player_playhead_time,n(),this._updateMaxPlayheadPosition();else if(this.pm.getPlayheadTime){var o=this.pm.getPlayheadTime();typeof o!=\"undefined\"&&(this.pm.data.player_playhead_time=o,n(),this._updateMaxPlayheadPosition())}}}]),r}(),Ht=_a;var Bt=5*60*1e3,fa=function r(e){\"use strict\";if(D(this,r),!e.disableRebufferTracking){var t,i=function(n,o){a(o),t=void 0},a=function(n){if(t){var o=n.viewer_time-t;A(e.data,\"view_rebuffer_duration\",o),t=n.viewer_time,e.data.view_rebuffer_duration>Bt&&(e.emit(\"viewend\"),e.send(\"viewend\"),e.mux.log.warn(\"Ending view after rebuffering for longer than \".concat(Bt,\"ms, future events will be ignored unless a programchange or videochange occurs.\")))}e.data.view_watch_time>=0&&e.data.view_rebuffer_count>0&&(e.data.view_rebuffer_frequency=e.data.view_rebuffer_count/e.data.view_watch_time,e.data.view_rebuffer_percentage=e.data.view_rebuffer_duration/e.data.view_watch_time)};e.on(\"playbackheartbeat\",function(n,o){return a(o)}),e.on(\"rebufferstart\",function(n,o){t||(A(e.data,\"view_rebuffer_count\",1),t=o.viewer_time,e.one(\"rebufferend\",i))}),e.on(\"viewinit\",function(){t=void 0,e.off(\"rebufferend\",i)})}},Ut=fa;var pa=function(){\"use strict\";function r(e){var t=this;D(this,r),u(this,\"_lastCheckedTime\",void 0),u(this,\"_lastPlayheadTime\",void 0),u(this,\"_lastPlayheadTimeUpdatedTime\",void 0),u(this,\"_rebuffering\",void 0),u(this,\"pm\",void 0),this.pm=e,!(e.disableRebufferTracking||e.disablePlayheadRebufferTracking)&&(this._lastCheckedTime=null,this._lastPlayheadTime=null,this._lastPlayheadTimeUpdatedTime=null,e.on(\"playbackheartbeat\",this._checkIfRebuffering.bind(this)),e.on(\"playbackheartbeatend\",this._cleanupRebufferTracker.bind(this)),e.on(\"seeking\",function(){t._cleanupRebufferTracker(null,{viewer_time:q.now()})}))}return L(r,[{key:\"_checkIfRebuffering\",value:function(t,i){if(this.pm.seekingTracker.isSeeking||this.pm.adTracker.isAdBreak||!this.pm.playbackHeartbeat._playheadShouldBeProgressing){this._cleanupRebufferTracker(t,i);return}if(this._lastCheckedTime===null){this._prepareRebufferTrackerState(i.viewer_time);return}if(this._lastPlayheadTime!==this.pm.data.player_playhead_time){this._cleanupRebufferTracker(t,i,!0);return}var a=i.viewer_time-this._lastPlayheadTimeUpdatedTime;typeof this.pm.sustainedRebufferThreshold==\"number\"&&a>=this.pm.sustainedRebufferThreshold&&(this._rebuffering||(this._rebuffering=!0,this.pm.emit(\"rebufferstart\",{viewer_time:this._lastPlayheadTimeUpdatedTime}))),this._lastCheckedTime=i.viewer_time}},{key:\"_clearRebufferTrackerState\",value:function(){this._lastCheckedTime=null,this._lastPlayheadTime=null,this._lastPlayheadTimeUpdatedTime=null}},{key:\"_prepareRebufferTrackerState\",value:function(t){this._lastCheckedTime=t,this._lastPlayheadTime=this.pm.data.player_playhead_time,this._lastPlayheadTimeUpdatedTime=t}},{key:\"_cleanupRebufferTracker\",value:function(t,i){var a=arguments.length>2&&arguments[2]!==void 0?arguments[2]:!1;if(this._rebuffering)this._rebuffering=!1,this.pm.emit(\"rebufferend\",{viewer_time:i.viewer_time});else{if(this._lastCheckedTime===null)return;var n=this.pm.data.player_playhead_time-this._lastPlayheadTime,o=i.viewer_time-this._lastPlayheadTimeUpdatedTime;typeof this.pm.minimumRebufferDuration==\"number\"&&n>0&&o-n>this.pm.minimumRebufferDuration&&(this._lastCheckedTime=null,this.pm.emit(\"rebufferstart\",{viewer_time:this._lastPlayheadTimeUpdatedTime}),this.pm.emit(\"rebufferend\",{viewer_time:this._lastPlayheadTimeUpdatedTime+o-n}))}a?this._prepareRebufferTrackerState(i.viewer_time):this._clearRebufferTrackerState()}}]),r}(),Ft=pa;var va=function(){\"use strict\";function r(e){var t=this;D(this,r),u(this,\"NAVIGATION_START\",void 0),u(this,\"pm\",void 0),this.pm=e,e.on(\"viewinit\",function(){var i=e.data,a=i.view_id;if(!i.view_program_changed){var n=function(o,s){var l=s.viewer_time;o.type===\"playing\"&&typeof e.data.view_time_to_first_frame==\"undefined\"?t.calculateTimeToFirstFrame(l||q.now(),a):o.type===\"adplaying\"&&(typeof e.data.view_time_to_first_frame==\"undefined\"||t._inPrerollPosition())&&t.calculateTimeToFirstFrame(l||q.now(),a)};e.one(\"playing\",n),e.one(\"adplaying\",n),e.one(\"viewend\",function(){e.off(\"playing\",n),e.off(\"adplaying\",n)})}})}return L(r,[{key:\"_inPrerollPosition\",value:function(){return typeof this.pm.data.view_content_playback_time==\"undefined\"||this.pm.data.view_content_playback_time<=1e3}},{key:\"calculateTimeToFirstFrame\",value:function(t,i){i===this.pm.data.view_id&&(this.pm.watchTimeTracker._updateWatchTime(null,{viewer_time:t}),this.pm.data.view_time_to_first_frame=this.pm.data.view_watch_time,(this.pm.data.player_autoplay_on||this.pm.data.video_is_autoplay)&&this.NAVIGATION_START&&(this.pm.data.view_aggregate_startup_time=this.pm.data.view_start+this.pm.data.view_watch_time-this.NAVIGATION_START))}}]),r}(),Vt=va;var ma=function r(e){\"use strict\";var t=this;D(this,r),u(this,\"_lastPlayerHeight\",void 0),u(this,\"_lastPlayerWidth\",void 0),u(this,\"_lastPlayheadPosition\",void 0),u(this,\"_lastSourceHeight\",void 0),u(this,\"_lastSourceWidth\",void 0),e.on(\"viewinit\",function(){t._lastPlayheadPosition=-1});var i=[\"pause\",\"rebufferstart\",\"seeking\",\"error\",\"adbreakstart\",\"hb\"],a=[\"playing\",\"hb\"];i.forEach(function(n){e.on(n,function(){if(t._lastPlayheadPosition>=0&&e.data.player_playhead_time>=0&&t._lastPlayerWidth>=0&&t._lastSourceWidth>0&&t._lastPlayerHeight>=0&&t._lastSourceHeight>0){var o=e.data.player_playhead_time-t._lastPlayheadPosition;if(o<0){t._lastPlayheadPosition=-1;return}var s=Math.min(t._lastPlayerWidth/t._lastSourceWidth,t._lastPlayerHeight/t._lastSourceHeight),l=Math.max(0,s-1),m=Math.max(0,1-s);e.data.view_max_upscale_percentage=Math.max(e.data.view_max_upscale_percentage||0,l),e.data.view_max_downscale_percentage=Math.max(e.data.view_max_downscale_percentage||0,m),A(e.data,\"view_total_content_playback_time\",o),A(e.data,\"view_total_upscaling\",l*o),A(e.data,\"view_total_downscaling\",m*o)}t._lastPlayheadPosition=-1})}),a.forEach(function(n){e.on(n,function(){t._lastPlayheadPosition=e.data.player_playhead_time,t._lastPlayerWidth=e.data.player_width,t._lastPlayerHeight=e.data.player_height,t._lastSourceWidth=e.data.video_source_width,t._lastSourceHeight=e.data.video_source_height})})},jt=ma;var ha=2e3,ya=function r(e){\"use strict\";var t=this;D(this,r),u(this,\"isSeeking\",void 0),this.isSeeking=!1;var i=-1,a=function(){var n=q.now(),o=(e.data.viewer_time||n)-(i||n);A(e.data,\"view_seek_duration\",o),e.data.view_max_seek_time=Math.max(e.data.view_max_seek_time||0,o),t.isSeeking=!1,i=-1};e.on(\"seeking\",function(n,o){if(Object.assign(e.data,o),t.isSeeking&&o.viewer_time-i<=ha){i=o.viewer_time;return}t.isSeeking&&a(),t.isSeeking=!0,i=o.viewer_time,A(e.data,\"view_seek_count\",1),e.send(\"seeking\")}),e.on(\"seeked\",function(){a()}),e.on(\"viewend\",function(){t.isSeeking&&(a(),e.send(\"seeked\")),t.isSeeking=!1,i=-1})},Wt=ya;var Gt=function(e,t){e.push(t),e.sort(function(i,a){return i.viewer_time-a.viewer_time})},ga=[\"adbreakstart\",\"adrequest\",\"adresponse\",\"adplay\",\"adplaying\",\"adpause\",\"adended\",\"adbreakend\",\"aderror\",\"adclicked\",\"adskipped\"],ba=function(){\"use strict\";function r(e){var t=this;D(this,r),u(this,\"_adHasPlayed\",void 0),u(this,\"_adRequests\",void 0),u(this,\"_adResponses\",void 0),u(this,\"_currentAdRequestNumber\",void 0),u(this,\"_currentAdResponseNumber\",void 0),u(this,\"_prerollPlayTime\",void 0),u(this,\"_wouldBeNewAdPlay\",void 0),u(this,\"isAdBreak\",void 0),u(this,\"pm\",void 0),this.pm=e,e.on(\"viewinit\",function(){t.isAdBreak=!1,t._currentAdRequestNumber=0,t._currentAdResponseNumber=0,t._adRequests=[],t._adResponses=[],t._adHasPlayed=!1,t._wouldBeNewAdPlay=!0,t._prerollPlayTime=void 0}),ga.forEach(function(a){return e.on(a,t._updateAdData.bind(t))});var i=function(){t.isAdBreak=!1};e.on(\"adbreakstart\",function(){t.isAdBreak=!0}),e.on(\"play\",i),e.on(\"playing\",i),e.on(\"viewend\",i),e.on(\"adrequest\",function(a,n){n=Object.assign({ad_request_id:\"generatedAdRequestId\"+t._currentAdRequestNumber++},n),Gt(t._adRequests,n),A(e.data,\"view_ad_request_count\"),t.inPrerollPosition()&&(e.data.view_preroll_requested=!0,t._adHasPlayed||A(e.data,\"view_preroll_request_count\"))}),e.on(\"adresponse\",function(a,n){n=Object.assign({ad_request_id:\"generatedAdRequestId\"+t._currentAdResponseNumber++},n),Gt(t._adResponses,n);var o=t.findAdRequest(n.ad_request_id);o&&A(e.data,\"view_ad_request_time\",Math.max(0,n.viewer_time-o.viewer_time))}),e.on(\"adplay\",function(a,n){t._adHasPlayed=!0,t._wouldBeNewAdPlay&&(t._wouldBeNewAdPlay=!1,A(e.data,\"view_ad_played_count\")),t.inPrerollPosition()&&!e.data.view_preroll_played&&(e.data.view_preroll_played=!0,t._adRequests.length>0&&(e.data.view_preroll_request_time=Math.max(0,n.viewer_time-t._adRequests[0].viewer_time)),e.data.view_start&&(e.data.view_startup_preroll_request_time=Math.max(0,n.viewer_time-e.data.view_start)),t._prerollPlayTime=n.viewer_time)}),e.on(\"adplaying\",function(a,n){t.inPrerollPosition()&&typeof e.data.view_preroll_load_time==\"undefined\"&&typeof t._prerollPlayTime!=\"undefined\"&&(e.data.view_preroll_load_time=n.viewer_time-t._prerollPlayTime,e.data.view_startup_preroll_load_time=n.viewer_time-t._prerollPlayTime)}),e.on(\"adclicked\",function(a,n){t._wouldBeNewAdPlay||A(e.data,\"view_ad_clicked_count\")}),e.on(\"adskipped\",function(a,n){t._wouldBeNewAdPlay||A(e.data,\"view_ad_skipped_count\")}),e.on(\"adended\",function(){t._wouldBeNewAdPlay=!0}),e.on(\"aderror\",function(){t._wouldBeNewAdPlay=!0})}return L(r,[{key:\"inPrerollPosition\",value:function(){return typeof this.pm.data.view_content_playback_time==\"undefined\"||this.pm.data.view_content_playback_time<=1e3}},{key:\"findAdRequest\",value:function(t){for(var i=0;i<this._adRequests.length;i++)if(this._adRequests[i].ad_request_id===t)return this._adRequests[i]}},{key:\"_updateAdData\",value:function(t,i){if(this.inPrerollPosition()){if(!this.pm.data.view_preroll_ad_tag_hostname&&i.ad_tag_url){var a=M(te(i.ad_tag_url),2),n=a[0],o=a[1];this.pm.data.view_preroll_ad_tag_domain=o,this.pm.data.view_preroll_ad_tag_hostname=n}if(!this.pm.data.view_preroll_ad_asset_hostname&&i.ad_asset_url){var s=M(te(i.ad_asset_url),2),l=s[0],m=s[1];this.pm.data.view_preroll_ad_asset_domain=m,this.pm.data.view_preroll_ad_asset_hostname=l}}this.pm.data.ad_asset_url=i==null?void 0:i.ad_asset_url,this.pm.data.ad_tag_url=i==null?void 0:i.ad_tag_url,this.pm.data.ad_creative_id=i==null?void 0:i.ad_creative_id,this.pm.data.ad_id=i==null?void 0:i.ad_id,this.pm.data.ad_universal_id=i==null?void 0:i.ad_universal_id}}]),r}(),Jt=ba;var Ge=W(G());var Ta=function r(e){\"use strict\";D(this,r);var t,i,a=function(){e.disableRebufferTracking||(A(e.data,\"view_waiting_rebuffer_count\",1),t=q.now(),i=Ge.default.setInterval(function(){if(t){var m=q.now();A(e.data,\"view_waiting_rebuffer_duration\",m-t),t=m}},250))},n=function(){e.disableRebufferTracking||t&&(A(e.data,\"view_waiting_rebuffer_duration\",q.now()-t),t=!1,Ge.default.clearInterval(i))},o=!1,s=function(){o=!0},l=function(){o=!1,n()};e.on(\"waiting\",function(){o&&a()}),e.on(\"playing\",function(){n(),s()}),e.on(\"pause\",l),e.on(\"seeking\",l)},Qt=Ta;var wa=function r(e){\"use strict\";var t=this;D(this,r),u(this,\"lastWallClockTime\",void 0);var i=function(){t.lastWallClockTime=q.now(),e.on(\"before*\",a)},a=function(n){var o=q.now(),s=t.lastWallClockTime;t.lastWallClockTime=o,o-s>3e4&&(e.emit(\"devicesleep\",{viewer_time:s}),Object.assign(e.data,{viewer_time:s}),e.send(\"devicesleep\"),e.emit(\"devicewake\",{viewer_time:o}),Object.assign(e.data,{viewer_time:o}),e.send(\"devicewake\"))};e.one(\"playbackheartbeat\",i),e.on(\"playbackheartbeatend\",function(){e.off(\"before*\",a),e.one(\"playbackheartbeat\",i)})},zt=wa;var Be=W(G());var Je=function(r){return r()}(function(){var r=function(){for(var i=0,a={};i<arguments.length;i++){var n=arguments[i];for(var o in n)a[o]=n[o]}return a};function e(t){function i(a,n,o){var s;if(typeof document!=\"undefined\"){if(arguments.length>1){if(o=r({path:\"/\"},i.defaults,o),typeof o.expires==\"number\"){var l=new Date;l.setMilliseconds(l.getMilliseconds()+o.expires*864e5),o.expires=l}try{s=JSON.stringify(n),/^[\\{\\[]/.test(s)&&(n=s)}catch(E){}return t.write?n=t.write(n,a):n=encodeURIComponent(String(n)).replace(/%(23|24|26|2B|3A|3C|3E|3D|2F|3F|40|5B|5D|5E|60|7B|7D|7C)/g,decodeURIComponent),a=encodeURIComponent(String(a)),a=a.replace(/%(23|24|26|2B|5E|60|7C)/g,decodeURIComponent),a=a.replace(/[\\(\\)]/g,escape),document.cookie=[a,\"=\",n,o.expires?\"; expires=\"+o.expires.toUTCString():\"\",o.path?\"; path=\"+o.path:\"\",o.domain?\"; domain=\"+o.domain:\"\",o.secure?\"; secure\":\"\"].join(\"\")}a||(s={});for(var m=document.cookie?document.cookie.split(\"; \"):[],g=/(%[0-9A-Z]{2})+/g,T=0;T<m.length;T++){var w=m[T].split(\"=\"),f=w.slice(1).join(\"=\");f.charAt(0)==='\"'&&(f=f.slice(1,-1));try{var _=w[0].replace(g,decodeURIComponent);if(f=t.read?t.read(f,_):t(f,_)||f.replace(g,decodeURIComponent),this.json)try{f=JSON.parse(f)}catch(E){}if(a===_){s=f;break}a||(s[_]=f)}catch(E){}}return s}}return i.set=i,i.get=function(a){return i.call(i,a)},i.getJSON=function(){return i.apply({json:!0},[].slice.call(arguments))},i.defaults={},i.remove=function(a,n){i(a,\"\",r(n,{expires:-1}))},i.withConverter=e,i}return e(function(){})});var Kt=\"muxData\",Ea=function(r){return Object.entries(r).map(function(e){var t=M(e,2),i=t[0],a=t[1];return\"\".concat(i,\"=\").concat(a)}).join(\"&\")},ka=function(r){return r.split(\"&\").reduce(function(e,t){var i=M(t.split(\"=\"),2),a=i[0],n=i[1],o=+n,s=n&&o==n?o:n;return e[a]=s,e},{})},Yt=function(){var e;try{e=ka(Je.get(Kt)||\"\")}catch(t){e={}}return e},Xt=function(e){try{Je.set(Kt,Ea(e),{expires:365})}catch(t){}},$t=function(){var e=Yt();return e.mux_viewer_id=e.mux_viewer_id||$(),e.msn=e.msn||Math.random(),Xt(e),{mux_viewer_id:e.mux_viewer_id,mux_sample_number:e.msn}},Zt=function(){var e=Yt(),t=q.now();return e.session_start&&(e.sst=e.session_start,delete e.session_start),e.session_id&&(e.sid=e.session_id,delete e.session_id),e.session_expires&&(e.sex=e.session_expires,delete e.session_expires),(!e.sex||e.sex<t)&&(e.sid=$(),e.sst=t),e.sex=t+25*60*1e3,Xt(e),{session_id:e.sid,session_start:e.sst,session_expires:e.sex}};function Qe(r,e){var t=e.beaconCollectionDomain,i=e.beaconDomain;if(t)return\"https://\"+t;r=r||\"inferred\";var a=i||\"litix.io\";return r.match(/^[a-z0-9]+$/)?\"https://\"+r+\".\"+a:\"https://img.litix.io/a.gif\"}var er=W(G()),tr=function(){var e;switch(rr()){case\"cellular\":e=\"cellular\";break;case\"ethernet\":e=\"wired\";break;case\"wifi\":e=\"wifi\";break;case void 0:break;default:e=\"other\"}return e},rr=function(){var e=er.default.navigator,t=e&&(e.connection||e.mozConnection||e.webkitConnection);return t&&t.type};tr.getConnectionFromAPI=rr;var ar=tr;var xa={a:\"env\",b:\"beacon\",c:\"custom\",d:\"ad\",e:\"event\",f:\"experiment\",i:\"internal\",m:\"mux\",n:\"response\",p:\"player\",q:\"request\",r:\"retry\",s:\"session\",t:\"timestamp\",u:\"viewer\",v:\"video\",w:\"page\",x:\"view\",y:\"sub\"},Da=nr(xa),Sa={ad:\"ad\",ag:\"aggregate\",ap:\"api\",al:\"application\",ar:\"architecture\",as:\"asset\",au:\"autoplay\",av:\"average\",bi:\"bitrate\",br:\"break\",bw:\"browser\",by:\"bytes\",bz:\"business\",ca:\"cached\",cb:\"cancel\",cc:\"codec\",cd:\"code\",cg:\"category\",ch:\"changed\",ck:\"clicked\",cl:\"canceled\",cn:\"config\",co:\"count\",ce:\"counter\",cp:\"complete\",cr:\"creative\",ct:\"content\",cu:\"current\",cx:\"connection\",cz:\"context\",dg:\"downscaling\",dm:\"domain\",dn:\"cdn\",do:\"downscale\",dr:\"drm\",dp:\"dropped\",du:\"duration\",dv:\"device\",ec:\"encoding\",ed:\"edge\",en:\"end\",eg:\"engine\",em:\"embed\",er:\"error\",ep:\"experiments\",es:\"errorcode\",et:\"errortext\",ee:\"event\",ev:\"events\",ex:\"expires\",ez:\"exception\",fa:\"failed\",fi:\"first\",fm:\"family\",ft:\"format\",fp:\"fps\",fq:\"frequency\",fr:\"frame\",fs:\"fullscreen\",ha:\"has\",hb:\"holdback\",he:\"headers\",ho:\"host\",hn:\"hostname\",ht:\"height\",id:\"id\",ii:\"init\",in:\"instance\",ip:\"ip\",is:\"is\",ke:\"key\",la:\"language\",lb:\"labeled\",le:\"level\",li:\"live\",ld:\"loaded\",lo:\"load\",ls:\"lists\",lt:\"latency\",ma:\"max\",md:\"media\",me:\"message\",mf:\"manifest\",mi:\"mime\",ml:\"midroll\",mm:\"min\",mn:\"manufacturer\",mo:\"model\",mx:\"mux\",ne:\"newest\",nm:\"name\",no:\"number\",on:\"on\",os:\"os\",pa:\"paused\",pb:\"playback\",pd:\"producer\",pe:\"percentage\",pf:\"played\",pg:\"program\",ph:\"playhead\",pi:\"plugin\",pl:\"preroll\",pn:\"playing\",po:\"poster\",pr:\"preload\",ps:\"position\",pt:\"part\",py:\"property\",ra:\"rate\",rd:\"requested\",re:\"rebuffer\",rf:\"rendition\",rm:\"remote\",ro:\"ratio\",rp:\"response\",rq:\"request\",rs:\"requests\",sa:\"sample\",sd:\"skipped\",se:\"session\",sk:\"seek\",sm:\"stream\",so:\"source\",sq:\"sequence\",sr:\"series\",st:\"start\",su:\"startup\",sv:\"server\",sw:\"software\",sy:\"severity\",ta:\"tag\",tc:\"tech\",te:\"text\",tg:\"target\",th:\"throughput\",ti:\"time\",tl:\"total\",to:\"to\",tt:\"title\",ty:\"type\",ug:\"upscaling\",un:\"universal\",up:\"upscale\",ur:\"url\",us:\"user\",va:\"variant\",vd:\"viewed\",vi:\"video\",ve:\"version\",vw:\"view\",vr:\"viewer\",wd:\"width\",wa:\"watch\",wt:\"waiting\"},ir=nr(Sa);function nr(r){var e={};for(var t in r)r.hasOwnProperty(t)&&(e[r[t]]=t);return e}function pe(r){var e={},t={};return Object.keys(r).forEach(function(i){var a=!1;if(r.hasOwnProperty(i)&&r[i]!==void 0){var n=i.split(\"_\"),o=n[0],s=Da[o];s||(O.info(\"Data key word `\"+n[0]+\"` not expected in \"+i),s=o+\"_\"),n.splice(1).forEach(function(l){l===\"url\"&&(a=!0),ir[l]?s+=ir[l]:Number(l)&&Math.floor(Number(l))===Number(l)?s+=l:(O.info(\"Data key word `\"+l+\"` not expected in \"+i),s+=\"_\"+l+\"_\")}),a?t[s]=r[i]:e[s]=r[i]}}),Object.assign(e,t)}var re=W(G());var Ra={maxBeaconSize:300,maxQueueLength:3600,baseTimeBetweenBeacons:1e4,maxPayloadKBSize:500},qa=56*1024,Aa=[\"hb\",\"requestcompleted\",\"requestfailed\",\"requestcanceled\"],Oa=\"https://img.litix.io\",Y=function(e){var t=arguments.length>1&&arguments[1]!==void 0?arguments[1]:{};this._beaconUrl=e||Oa,this._eventQueue=[],this._postInFlight=!1,this._failureCount=0,this._sendTimeout=!1,this._options=Object.assign({},Ra,t)};Y.prototype.queueEvent=function(r,e){var t=Object.assign({},e);return this._eventQueue.length<=this._options.maxQueueLength||r===\"eventrateexceeded\"?(this._eventQueue.push(t),this._sendTimeout||this._startBeaconSending(),this._eventQueue.length<=this._options.maxQueueLength):!1};Y.prototype.flushEvents=function(){var r=arguments.length>0&&arguments[0]!==void 0?arguments[0]:!1;if(r&&this._eventQueue.length===1){this._eventQueue.pop();return}this._eventQueue.length&&this._sendBeaconQueue(),this._startBeaconSending()};Y.prototype.destroy=function(){var r=arguments.length>0&&arguments[0]!==void 0?arguments[0]:!1;this.destroyed=!0,r?this._clearBeaconQueue():this.flushEvents(),re.default.clearTimeout(this._sendTimeout)};Y.prototype._clearBeaconQueue=function(){var r=this._eventQueue.length>this._options.maxBeaconSize?this._eventQueue.length-this._options.maxBeaconSize:0,e=this._eventQueue.slice(r);r>0&&Object.assign(e[e.length-1],pe({mux_view_message:\"event queue truncated\"}));var t=this._createPayload(e);or(this._beaconUrl,t,!0,function(){})};Y.prototype._sendBeaconQueue=function(){var r=this;if(!this._postInFlight){var e=this._eventQueue.slice(0,this._options.maxBeaconSize);this._eventQueue=this._eventQueue.slice(this._options.maxBeaconSize),this._postInFlight=!0;var t=this._createPayload(e),i=q.now();or(this._beaconUrl,t,!1,function(a,n){n?(r._eventQueue=e.concat(r._eventQueue),r._failureCount+=1,O.info(\"Error sending beacon: \"+n)):r._failureCount=0,r._roundTripTime=q.now()-i,r._postInFlight=!1})}};Y.prototype._getNextBeaconTime=function(){if(!this._failureCount)return this._options.baseTimeBetweenBeacons;var r=Math.pow(2,this._failureCount-1);return r=r*Math.random(),(1+r)*this._options.baseTimeBetweenBeacons};Y.prototype._startBeaconSending=function(){var r=this;re.default.clearTimeout(this._sendTimeout),!this.destroyed&&(this._sendTimeout=re.default.setTimeout(function(){r._eventQueue.length&&r._sendBeaconQueue(),r._startBeaconSending()},this._getNextBeaconTime()))};Y.prototype._createPayload=function(r){var e=this,t={transmission_timestamp:Math.round(q.now())};this._roundTripTime&&(t.rtt_ms=Math.round(this._roundTripTime));var i,a,n,o=function(){i=JSON.stringify({metadata:t,events:a||r}),n=i.length/1024},s=function(){return n<=e._options.maxPayloadKBSize};return o(),s()||(O.info(\"Payload size is too big (\"+n+\" kb). Removing unnecessary events.\"),a=r.filter(function(l){return Aa.indexOf(l.e)===-1}),o()),s()||(O.info(\"Payload size still too big (\"+n+\" kb). Cropping fields..\"),a.forEach(function(l){for(var m in l){var g=l[m],T=50*1024;typeof g==\"string\"&&g.length>T&&(l[m]=g.substring(0,T))}}),o()),i};var Ia=function(r){return r.length<=qa},or=function(r,e,t,i){if(t&&navigator&&navigator.sendBeacon&&navigator.sendBeacon(r,e)){i();return}if(re.default.fetch){re.default.fetch(r,{method:\"POST\",body:e,headers:{\"Content-Type\":\"text/plain\"},keepalive:Ia(e)}).then(function(n){return i(null,n.ok?null:\"Error\")}).catch(function(n){return i(null,n)});return}if(re.default.XMLHttpRequest){var a=new re.default.XMLHttpRequest;a.onreadystatechange=function(){if(a.readyState===4)return i(null,a.status!==200?\"error\":void 0)},a.open(\"POST\",r),a.setRequestHeader(\"Content-Type\",\"text/plain\"),a.send(e);return}i()},sr=Y;var Na=[\"env_key\",\"view_id\",\"view_sequence_number\",\"player_sequence_number\",\"beacon_domain\",\"player_playhead_time\",\"viewer_time\",\"mux_api_version\",\"event\",\"video_id\",\"player_instance_id\",\"player_error_code\",\"player_error_message\",\"player_error_context\",\"player_error_severity\",\"player_error_business_exception\"],Pa=[\"adplay\",\"adplaying\",\"adpause\",\"adfirstquartile\",\"admidpoint\",\"adthirdquartile\",\"adended\",\"adresponse\",\"adrequest\"],La=[\"ad_id\",\"ad_creative_id\",\"ad_universal_id\"],Ca=[\"viewstart\",\"error\",\"ended\",\"viewend\"],Ma=10*60*1e3,ur=function(){\"use strict\";function r(e,t){var i=arguments.length>2&&arguments[2]!==void 0?arguments[2]:{};D(this,r);var a,n,o,s,l,m,g,T,w,f,_,E;u(this,\"mux\",void 0),u(this,\"envKey\",void 0),u(this,\"options\",void 0),u(this,\"eventQueue\",void 0),u(this,\"sampleRate\",void 0),u(this,\"disableCookies\",void 0),u(this,\"respectDoNotTrack\",void 0),u(this,\"previousBeaconData\",void 0),u(this,\"lastEventTime\",void 0),u(this,\"rateLimited\",void 0),u(this,\"pageLevelData\",void 0),u(this,\"viewerData\",void 0),this.mux=e,this.envKey=t,this.options=i,this.previousBeaconData=null,this.lastEventTime=0,this.rateLimited=!1,this.eventQueue=new sr(Qe(this.envKey,this.options));var x;this.sampleRate=(x=this.options.sampleRate)!==null&&x!==void 0?x:1;var v;this.disableCookies=(v=this.options.disableCookies)!==null&&v!==void 0?v:!1;var p;this.respectDoNotTrack=(p=this.options.respectDoNotTrack)!==null&&p!==void 0?p:!1,this.previousBeaconData=null,this.lastEventTime=0,this.rateLimited=!1,this.pageLevelData={mux_api_version:this.mux.API_VERSION,mux_embed:this.mux.NAME,mux_embed_version:this.mux.VERSION,viewer_application_name:(a=this.options.platform)===null||a===void 0?void 0:a.name,viewer_application_version:(n=this.options.platform)===null||n===void 0?void 0:n.version,viewer_application_engine:(o=this.options.platform)===null||o===void 0?void 0:o.layout,viewer_device_name:(s=this.options.platform)===null||s===void 0?void 0:s.product,viewer_device_category:\"\",viewer_device_manufacturer:(l=this.options.platform)===null||l===void 0?void 0:l.manufacturer,viewer_os_family:(g=this.options.platform)===null||g===void 0||(m=g.os)===null||m===void 0?void 0:m.family,viewer_os_architecture:(w=this.options.platform)===null||w===void 0||(T=w.os)===null||T===void 0?void 0:T.architecture,viewer_os_version:(_=this.options.platform)===null||_===void 0||(f=_.os)===null||f===void 0?void 0:f.version,viewer_connection_type:ar(),page_url:Be.default===null||Be.default===void 0||(E=Be.default.location)===null||E===void 0?void 0:E.href},this.viewerData=this.disableCookies?{}:$t()}return L(r,[{key:\"send\",value:function(t,i){if(!(!t||!(i!=null&&i.view_id))){if(this.respectDoNotTrack&&de())return O.info(\"Not sending `\"+t+\"` because Do Not Track is enabled\");if(!i||typeof i!=\"object\")return O.error(\"A data object was expected in send() but was not provided\");var a=this.disableCookies?{}:Zt(),n=_e(oe({},this.pageLevelData,i,a,this.viewerData),{event:t,env_key:this.envKey});n.user_id&&(n.viewer_user_id=n.user_id,delete n.user_id);var o,s=((o=n.mux_sample_number)!==null&&o!==void 0?o:0)>=this.sampleRate,l=this._deduplicateBeaconData(t,n),m=pe(l);if(this.lastEventTime=this.mux.utils.now(),s)return O.info(\"Not sending event due to sample rate restriction\",t,n,m);if(this.envKey||O.info(\"Missing environment key (envKey) - beacons will be dropped if the video source is not a valid mux video URL\",t,n,m),!this.rateLimited){if(O.info(\"Sending event\",t,n,m),this.rateLimited=!this.eventQueue.queueEvent(t,m),this.mux.WINDOW_UNLOADING&&t===\"viewend\")this.eventQueue.destroy(!0);else if(this.mux.WINDOW_HIDDEN&&t===\"hb\"?this.eventQueue.flushEvents(!0):Ca.indexOf(t)>=0&&this.eventQueue.flushEvents(),this.rateLimited)return n.event=\"eventrateexceeded\",m=pe(n),this.eventQueue.queueEvent(n.event,m),O.error(\"Beaconing disabled due to rate limit.\")}}}},{key:\"destroy\",value:function(){this.eventQueue.destroy(!1)}},{key:\"_deduplicateBeaconData\",value:function(t,i){var a=this,n={},o=i.view_id;if(o===\"-1\"||t===\"viewstart\"||t===\"viewend\"||!this.previousBeaconData||this.mux.utils.now()-this.lastEventTime>=Ma)n=oe({},i),o&&(this.previousBeaconData=n),o&&t===\"viewend\"&&(this.previousBeaconData=null);else{var s=t.indexOf(\"request\")===0;Object.entries(i).forEach(function(l){var m=M(l,2),g=m[0],T=m[1];a.previousBeaconData&&(T!==a.previousBeaconData[g]||Na.indexOf(g)>-1||a.objectHasChanged(s,g,T,a.previousBeaconData[g])||a.eventRequiresKey(t,g))&&(n[g]=T,a.previousBeaconData[g]=T)})}return n}},{key:\"objectHasChanged\",value:function(t,i,a,n){return!t||i.indexOf(\"request_\")!==0?!1:i===\"request_response_headers\"||typeof a!=\"object\"||typeof n!=\"object\"?!0:Object.keys(a||{}).length!==Object.keys(n||{}).length}},{key:\"eventRequiresKey\",value:function(t,i){return!!(t===\"renditionchange\"&&i.indexOf(\"video_source_\")===0||La.includes(i)&&Pa.includes(t))}}]),r}();var Ha=function r(e){\"use strict\";D(this,r);var t=0,i=0,a=0,n=0,o=0,s=0,l=0,m=function(w,f){var _=f.request_start,E=f.request_response_start,x=f.request_response_end,v=f.request_bytes_loaded;n++;var p,c;if(E?(p=E-(_!=null?_:0),c=(x!=null?x:0)-E):c=(x!=null?x:0)-(_!=null?_:0),c>0&&v&&v>0){var d=v/c*8e3;o++,i+=v,a+=c,e.data.view_min_request_throughput=Math.min(e.data.view_min_request_throughput||1/0,d),e.data.view_average_request_throughput=i/a*8e3,e.data.view_request_count=n,p>0&&(t+=p,e.data.view_max_request_latency=Math.max(e.data.view_max_request_latency||0,p),e.data.view_average_request_latency=t/o)}},g=function(w,f){n++,s++,e.data.view_request_count=n,e.data.view_request_failed_count=s},T=function(w,f){n++,l++,e.data.view_request_count=n,e.data.view_request_canceled_count=l};e.on(\"requestcompleted\",m),e.on(\"requestfailed\",g),e.on(\"requestcanceled\",T)},dr=Ha;var Ba=60*60*1e3,Ua=function r(e){\"use strict\";var t=this;D(this,r),u(this,\"_lastEventTime\",void 0),e.on(\"before*\",function(i,a){var n=a.viewer_time,o=q.now(),s=t._lastEventTime;if(t._lastEventTime=o,s&&o-s>Ba){var l=Object.keys(e.data).reduce(function(m,g){return g.indexOf(\"video_\")===0?Object.assign(m,u({},g,e.data[g])):m},{});e.mux.log.info(\"Received event after at least an hour inactivity, creating a new view\"),e.emit(\"viewinit\",Object.assign({viewer_time:n},l)),e.playbackHeartbeat._playheadShouldBeProgressing&&i.type!==\"play\"&&i.type!==\"adbreakstart\"&&(e.emit(\"play\",{viewer_time:n}),i.type!==\"playing\"&&e.emit(\"playing\",{viewer_time:n}))}})},lr=Ua;var Fa=[\"viewstart\",\"ended\",\"loadstart\",\"pause\",\"play\",\"playing\",\"ratechange\",\"waiting\",\"adplay\",\"adpause\",\"adended\",\"aderror\",\"adplaying\",\"adrequest\",\"adresponse\",\"adbreakstart\",\"adbreakend\",\"adfirstquartile\",\"admidpoint\",\"adthirdquartile\",\"rebufferstart\",\"rebufferend\",\"seeked\",\"error\",\"hb\",\"requestcompleted\",\"requestfailed\",\"requestcanceled\",\"renditionchange\"],Va=new Set([\"requestcompleted\",\"requestfailed\",\"requestcanceled\"]),ja=function(r){\"use strict\";wt(t,r);var e=xt(t);function t(i,a,n){D(this,t);var o;o=e.call(this),u(h(o),\"DOM_CONTENT_LOADED_EVENT_END\",void 0),u(h(o),\"NAVIGATION_START\",void 0),u(h(o),\"_destroyed\",void 0),u(h(o),\"_heartBeatTimeout\",void 0),u(h(o),\"adTracker\",void 0),u(h(o),\"dashjs\",void 0),u(h(o),\"data\",void 0),u(h(o),\"disablePlayheadRebufferTracking\",void 0),u(h(o),\"disableRebufferTracking\",void 0),u(h(o),\"errorTracker\",void 0),u(h(o),\"errorTranslator\",void 0),u(h(o),\"getAdData\",void 0),u(h(o),\"getPlayheadTime\",void 0),u(h(o),\"getStateData\",void 0),u(h(o),\"hlsjs\",void 0),u(h(o),\"id\",void 0),u(h(o),\"longResumeTracker\",void 0),u(h(o),\"minimumRebufferDuration\",void 0),u(h(o),\"mux\",void 0),u(h(o),\"oldEmit\",void 0),u(h(o),\"playbackEventDispatcher\",void 0),u(h(o),\"playbackHeartbeat\",void 0),u(h(o),\"playbackHeartbeatTime\",void 0),u(h(o),\"playheadTime\",void 0),u(h(o),\"seekingTracker\",void 0),u(h(o),\"sustainedRebufferThreshold\",void 0),u(h(o),\"watchTimeTracker\",void 0),u(h(o),\"currentFragmentPDT\",void 0),u(h(o),\"currentFragmentStart\",void 0),o.DOM_CONTENT_LOADED_EVENT_END=ce.domContentLoadedEventEnd(),o.NAVIGATION_START=ce.navigationStart();var s={debug:!1,minimumRebufferDuration:250,sustainedRebufferThreshold:1e3,playbackHeartbeatTime:25,beaconDomain:\"litix.io\",sampleRate:1,disableCookies:!1,respectDoNotTrack:!1,disableRebufferTracking:!1,disablePlayheadRebufferTracking:!1,errorTranslator:function(f){return f}};o.mux=i,o.id=a,n!=null&&n.beaconDomain&&o.mux.log.warn(\"The `beaconDomain` setting has been deprecated in favor of `beaconCollectionDomain`. Please change your integration to use `beaconCollectionDomain` instead of `beaconDomain`.\"),n=Object.assign(s,n),n.data=n.data||{},n.data.property_key&&(n.data.env_key=n.data.property_key,delete n.data.property_key),O.level=n.debug?K.DEBUG:K.WARN,o.getPlayheadTime=n.getPlayheadTime,o.getStateData=n.getStateData||function(){return{}},o.getAdData=n.getAdData||function(){},o.minimumRebufferDuration=n.minimumRebufferDuration,o.sustainedRebufferThreshold=n.sustainedRebufferThreshold,o.playbackHeartbeatTime=n.playbackHeartbeatTime,o.disableRebufferTracking=n.disableRebufferTracking,o.disableRebufferTracking&&o.mux.log.warn(\"Disabling rebuffer tracking. This should only be used in specific circumstances as a last resort when your player is known to unreliably track rebuffering.\"),o.disablePlayheadRebufferTracking=n.disablePlayheadRebufferTracking,o.errorTranslator=n.errorTranslator,o.playbackEventDispatcher=new ur(i,n.data.env_key,n),o.data={player_instance_id:$(),mux_sample_rate:n.sampleRate,beacon_domain:n.beaconCollectionDomain||n.beaconDomain},o.data.view_sequence_number=1,o.data.player_sequence_number=1,o.oldEmit=o.emit,o.emit=function(f,_){_=Object.assign({viewer_time:this.mux.utils.now()},_),this.oldEmit(f,_)};var l=function(){typeof this.data.view_start==\"undefined\"&&(this.data.view_start=this.mux.utils.now(),this.emit(\"viewstart\"))}.bind(h(o));o.on(\"viewinit\",function(f,_){this._resetVideoData(),this._resetViewData(),this._resetErrorData(),this._updateStateData(),Object.assign(this.data,_),this._initializeViewData(),this.one(\"play\",l),this.one(\"adbreakstart\",l)});var m=function(f){this.emit(\"viewend\"),this.send(\"viewend\"),this.emit(\"viewinit\",f)}.bind(h(o));if(o.on(\"videochange\",function(f,_){m(_)}),o.on(\"programchange\",function(f,_){this.data.player_is_paused&&this.mux.log.warn(\"The `programchange` event is intended to be used when the content changes mid playback without the video source changing, however the video is not currently playing. If the video source is changing please use the videochange event otherwise you will lose startup time information.\"),m(Object.assign(_,{view_program_changed:!0})),l(),this.emit(\"play\"),this.emit(\"playing\")}),o.on(\"fragmentchange\",function(f,_){this.currentFragmentPDT=_.currentFragmentPDT,this.currentFragmentStart=_.currentFragmentStart}),o.on(\"destroy\",o.destroy),typeof window!=\"undefined\"&&typeof window.addEventListener==\"function\"&&typeof window.removeEventListener==\"function\"){var g=function(){var f=typeof o.data.view_start!=\"undefined\";o.mux.WINDOW_HIDDEN=document.visibilityState===\"hidden\",f&&o.mux.WINDOW_HIDDEN&&(o.data.player_is_paused||o.emit(\"hb\"))};window.addEventListener(\"visibilitychange\",g,!1);var T=function(f){f.persisted||o.destroy()};window.addEventListener(\"pagehide\",T,!1),o.on(\"destroy\",function(){window.removeEventListener(\"visibilitychange\",g),window.removeEventListener(\"pagehide\",T)})}o.on(\"playerready\",function(f,_){Object.assign(this.data,_)}),Fa.forEach(function(f){o.on(f,function(_,E){f.indexOf(\"ad\")!==0&&this._updateStateData(),Object.assign(this.data,E),this._sanitizeData()}),o.on(\"after\"+f,function(){(f!==\"error\"||this.errorTracker.viewErrored)&&this.send(f)})}),o.on(\"viewend\",function(f,_){Object.assign(o.data,_)});var w=function(_){var E=this.mux.utils.now();this.data.player_init_time&&(this.data.player_startup_time=E-this.data.player_init_time),!this.mux.PLAYER_TRACKED&&this.NAVIGATION_START&&(this.mux.PLAYER_TRACKED=!0,(this.data.player_init_time||this.DOM_CONTENT_LOADED_EVENT_END)&&(this.data.page_load_time=Math.min(this.data.player_init_time||1/0,this.DOM_CONTENT_LOADED_EVENT_END||1/0)-this.NAVIGATION_START)),this.send(\"playerready\"),delete this.data.player_startup_time,delete this.data.page_load_time};return o.one(\"playerready\",w),o.longResumeTracker=new lr(h(o)),o.errorTracker=new Lt(h(o)),new zt(h(o)),o.seekingTracker=new Wt(h(o)),o.playheadTime=new Ht(h(o)),o.playbackHeartbeat=new Pt(h(o)),new jt(h(o)),o.watchTimeTracker=new Ct(h(o)),new Mt(h(o)),o.adTracker=new Jt(h(o)),new Ft(h(o)),new Ut(h(o)),new Vt(h(o)),new Qt(h(o)),new dr(h(o)),n.hlsjs&&o.addHLSJS(n),n.dashjs&&o.addDashJS(n),o.emit(\"viewinit\",n.data),o}return L(t,[{key:\"destroy\",value:function(){this._destroyed||(this._destroyed=!0,typeof this.data.view_start!=\"undefined\"&&(this.emit(\"viewend\"),this.send(\"viewend\")),this.playbackEventDispatcher.destroy(),this.removeHLSJS(),this.removeDashJS(),window.clearTimeout(this._heartBeatTimeout))}},{key:\"send\",value:function(a){if(this.data.view_id){var n=Object.assign({},this.data),o=[\"player_program_time\",\"player_manifest_newest_program_time\",\"player_live_edge_program_time\",\"player_program_time\",\"video_holdback\",\"video_part_holdback\",\"video_target_duration\",\"video_part_target_duration\"];if(n.video_source_is_live===void 0&&(n.player_source_duration===1/0||n.video_source_duration===1/0?n.video_source_is_live=!0:(n.player_source_duration>0||n.video_source_duration>0)&&(n.video_source_is_live=!1)),n.video_source_is_live||o.forEach(function(g){n[g]=void 0}),n.video_source_url=n.video_source_url||n.player_source_url,n.video_source_url){var s=M(te(n.video_source_url),2),l=s[0],m=s[1];n.video_source_domain=m,n.video_source_hostname=l}delete n.ad_request_id,this.playbackEventDispatcher.send(a,n),this.data.view_sequence_number++,this.data.player_sequence_number++,Va.has(a)||this._restartHeartBeat(),a===\"viewend\"&&delete this.data.view_id}}},{key:\"_updateStateData\",value:function(){Object.assign(this.data,this.getStateData()),this.playheadTime._updatePlayheadTime(),this._sanitizeData()}},{key:\"_sanitizeData\",value:function(){var a=this,n=[\"player_width\",\"player_height\",\"video_source_width\",\"video_source_height\",\"player_playhead_time\",\"video_source_bitrate\"];n.forEach(function(s){var l=parseInt(a.data[s],10);a.data[s]=isNaN(l)?void 0:l});var o=[\"player_source_url\",\"video_source_url\"];o.forEach(function(s){if(a.data[s]){var l=a.data[s].toLowerCase();(l.indexOf(\"data:\")===0||l.indexOf(\"blob:\")===0)&&(a.data[s]=\"MSE style URL\")}})}},{key:\"_resetVideoData\",value:function(){var a=this;Object.keys(this.data).forEach(function(n){n.indexOf(\"video_\")===0&&delete a.data[n]})}},{key:\"_resetViewData\",value:function(){var a=this;Object.keys(this.data).forEach(function(n){n.indexOf(\"view_\")===0&&delete a.data[n]}),this.data.view_sequence_number=1}},{key:\"_resetErrorData\",value:function(){delete this.data.player_error_code,delete this.data.player_error_message,delete this.data.player_error_context,delete this.data.player_error_severity,delete this.data.player_error_business_exception}},{key:\"_initializeViewData\",value:function(){var a=this,n=this.data.view_id=$(),o=function(){n===a.data.view_id&&A(a.data,\"player_view_count\",1)};this.data.player_is_paused?this.one(\"play\",o):o()}},{key:\"_restartHeartBeat\",value:function(){var a=this;window.clearTimeout(this._heartBeatTimeout),this._heartBeatTimeout=window.setTimeout(function(){a.data.player_is_paused||a.emit(\"hb\")},1e4)}},{key:\"addHLSJS\",value:function(a){if(!a.hlsjs){this.mux.log.warn(\"You must pass a valid hlsjs instance in order to track it.\");return}if(this.hlsjs){this.mux.log.warn(\"An instance of HLS.js is already being monitored for this player.\");return}this.hlsjs=a.hlsjs,St(this.mux,this.id,a.hlsjs,{},a.Hls||window.Hls)}},{key:\"removeHLSJS\",value:function(){this.hlsjs&&(Rt(this.hlsjs),this.hlsjs=void 0)}},{key:\"addDashJS\",value:function(a){if(!a.dashjs){this.mux.log.warn(\"You must pass a valid dashjs instance in order to track it.\");return}if(this.dashjs){this.mux.log.warn(\"An instance of Dash.js is already being monitored for this player.\");return}this.dashjs=a.dashjs,At(this.mux,this.id,a.dashjs)}},{key:\"removeDashJS\",value:function(){this.dashjs&&(Ot(this.dashjs),this.dashjs=void 0)}}]),t}(Nt),cr=ja;Pe();var me=W(Mr());function at(){return me.default&&!!(me.default.fullscreenElement||me.default.webkitFullscreenElement||me.default.mozFullScreenElement||me.default.msFullscreenElement)}var vi=[\"loadstart\",\"pause\",\"play\",\"playing\",\"seeking\",\"seeked\",\"timeupdate\",\"ratechange\",\"stalled\",\"waiting\",\"error\",\"ended\"],mi={1:\"MEDIA_ERR_ABORTED\",2:\"MEDIA_ERR_NETWORK\",3:\"MEDIA_ERR_DECODE\",4:\"MEDIA_ERR_SRC_NOT_SUPPORTED\"};function it(r,e,t){var i=M(xe(e),3),a=i[0],n=i[1],o=i[2],s=r.log,l=r.utils.getComputedStyle,m=r.utils.secondsToMs,g={automaticErrorTracking:!0};if(a){if(o!==\"video\"&&o!==\"audio\")return s.error(\"The element of `\"+n+\"` was not a media element.\")}else return s.error(\"No element was found with the `\"+n+\"` query selector.\");a.mux&&(a.mux.destroy(),delete a.mux,s.warn(\"Already monitoring this video element, replacing existing event listeners\")),t=Object.assign(g,t),t.data=Object.assign({player_software:\"HTML5 Video Element\",player_mux_plugin_name:\"VideoElementMonitor\",player_mux_plugin_version:r.VERSION},t.data),t.getPlayheadTime=function(){return m(a.currentTime)},t.getStateData=function(){var w,f=this.hlsjs&&this.hlsjs.url,_=this.dashjs&&le(this.dashjs.getSource===\"function\")&&this.dashjs.getSource(),E={player_is_paused:a.paused,player_playhead_time:m(a.currentTime),player_width:parseInt(l(a,\"width\")),player_height:parseInt(l(a,\"height\")),player_autoplay_on:a.autoplay,player_preload_on:a.preload,player_language_code:a.lang,player_is_fullscreen:at(),video_poster_url:a.poster,video_source_url:f||_||a.currentSrc,video_source_duration:m(a.duration),video_source_height:a.videoHeight,video_source_width:a.videoWidth,view_dropped_frame_count:a==null||(w=a.getVideoPlaybackQuality)===null||w===void 0?void 0:w.call(a).droppedVideoFrames},x=t.getPlayheadTime();if(a.getStartDate&&x>0){var v=a.getStartDate();if(v&&typeof v.getTime==\"function\"&&v.getTime()){var p=v.getTime();if(E.player_program_time=p+x,a.seekable.length>0){var c=p+a.seekable.end(a.seekable.length-1);E.player_live_edge_program_time=c}}}return E},a.mux=a.mux||{},a.mux.deleted=!1,a.mux.emit=function(w,f){r.emit(n,w,f)};var T=function(){s.error(\"The monitor for this video element has already been destroyed.\")};a.mux.destroy=function(){Object.keys(a.mux.listeners).forEach(function(w){a.removeEventListener(w,a.mux.listeners[w],!1)}),delete a.mux.listeners,a.mux.destroy=T,a.mux.swapElement=T,a.mux.emit=T,a.mux.addHLSJS=T,a.mux.addDashJS=T,a.mux.removeHLSJS=T,a.mux.removeDashJS=T,a.mux.deleted=!0,r.emit(n,\"destroy\")},a.mux.swapElement=function(w){var f=M(xe(w),3),_=f[0],E=f[1],x=f[2];if(_){if(x!==\"video\"&&x!==\"audio\")return r.log.error(\"The element of `\"+E+\"` was not a media element.\")}else return r.log.error(\"No element was found with the `\"+E+\"` query selector.\");_.muxId=a.muxId,delete a.muxId,_.mux=_.mux||{},_.mux.listeners=Object.assign({},a.mux.listeners),delete a.mux.listeners,Object.keys(_.mux.listeners).forEach(function(v){a.removeEventListener(v,_.mux.listeners[v],!1),_.addEventListener(v,_.mux.listeners[v],!1)}),_.mux.swapElement=a.mux.swapElement,_.mux.destroy=a.mux.destroy,delete a.mux,a=_},a.mux.addHLSJS=function(w){r.addHLSJS(n,w)},a.mux.addDashJS=function(w){r.addDashJS(n,w)},a.mux.removeHLSJS=function(){r.removeHLSJS(n)},a.mux.removeDashJS=function(){r.removeDashJS(n)},r.init(n,t),r.emit(n,\"playerready\"),a.paused||(r.emit(n,\"play\"),a.readyState>2&&r.emit(n,\"playing\")),a.mux.listeners={},vi.forEach(function(w){w===\"error\"&&!t.automaticErrorTracking||(a.mux.listeners[w]=function(){var f={};if(w===\"error\"){if(!a.error||a.error.code===1)return;f.player_error_code=a.error.code,f.player_error_message=mi[a.error.code]||a.error.message}r.emit(n,w,f)},a.addEventListener(w,a.mux.listeners[w],!1))})}function nt(r,e,t,i){var a=i;if(r&&typeof r[e]==\"function\")try{a=r[e].apply(r,t)}catch(n){O.info(\"safeCall error\",n)}return a}var ye=W(G()),he;ye.default&&ye.default.WeakMap&&(he=new WeakMap);function ot(r,e){if(!r||!e||!ye.default||typeof ye.default.getComputedStyle!=\"function\")return\"\";var t;return he&&he.has(r)&&(t=he.get(r)),t||(t=ye.default.getComputedStyle(r,null),he&&he.set(r,t)),t.getPropertyValue(e)}function st(r){return Math.floor(r*1e3)}var ue={TARGET_DURATION:\"#EXT-X-TARGETDURATION\",PART_INF:\"#EXT-X-PART-INF\",SERVER_CONTROL:\"#EXT-X-SERVER-CONTROL\",INF:\"#EXTINF\",PROGRAM_DATE_TIME:\"#EXT-X-PROGRAM-DATE-TIME\",VERSION:\"#EXT-X-VERSION\",SESSION_DATA:\"#EXT-X-SESSION-DATA\"},Fe=function(e){return this.buffer=\"\",this.manifest={segments:[],serverControl:{},sessionData:{}},this.currentUri={},this.process(e),this.manifest};Fe.prototype.process=function(r){var e;for(this.buffer+=r,e=this.buffer.indexOf(\"\\n\");e>-1;e=this.buffer.indexOf(\"\\n\"))this.processLine(this.buffer.substring(0,e)),this.buffer=this.buffer.substring(e+1)};Fe.prototype.processLine=function(r){var e=r.indexOf(\":\"),t=bi(r,e),i=t[0],a=t.length===2?dt(t[1]):void 0;if(i[0]!==\"#\")this.currentUri.uri=i,this.manifest.segments.push(this.currentUri),this.manifest.targetDuration&&!(\"duration\"in this.currentUri)&&(this.currentUri.duration=this.manifest.targetDuration),this.currentUri={};else switch(i){case ue.TARGET_DURATION:{if(!isFinite(a)||a<0)return;this.manifest.targetDuration=a,this.setHoldBack();break}case ue.PART_INF:{ut(this.manifest,t),this.manifest.partInf.partTarget&&(this.manifest.partTargetDuration=this.manifest.partInf.partTarget),this.setHoldBack();break}case ue.SERVER_CONTROL:{ut(this.manifest,t),this.setHoldBack();break}case ue.INF:{a===0?this.currentUri.duration=.01:a>0&&(this.currentUri.duration=a);break}case ue.PROGRAM_DATE_TIME:{var n=a,o=new Date(n);this.manifest.dateTimeString||(this.manifest.dateTimeString=n,this.manifest.dateTimeObject=o),this.currentUri.dateTimeString=n,this.currentUri.dateTimeObject=o;break}case ue.VERSION:{ut(this.manifest,t);break}case ue.SESSION_DATA:{var s=Ti(t[1]),l=Ce(s);Object.assign(this.manifest.sessionData,l)}}};Fe.prototype.setHoldBack=function(){var r=this.manifest,e=r.serverControl,t=r.targetDuration,i=r.partTargetDuration;if(e){var a=\"holdBack\",n=\"partHoldBack\",o=t&&t*3,s=i&&i*2;t&&!e.hasOwnProperty(a)&&(e[a]=o),o&&e[a]<o&&(e[a]=o),i&&!e.hasOwnProperty(n)&&(e[n]=i*3),i&&e[n]<s&&(e[n]=s)}};var ut=function(r,e){var t=Hr(e[0].replace(\"#EXT-X-\",\"\")),i;gi(e[1])?(i={},i=Object.assign(yi(e[1]),i)):i=dt(e[1]),r[t]=i},Hr=function(r){return r.toLowerCase().replace(/-(\\w)/g,function(e){return e[1].toUpperCase()})},dt=function(r){if(r.toLowerCase()===\"yes\"||r.toLowerCase()===\"no\")return r.toLowerCase()===\"yes\";var e=r.indexOf(\":\")!==-1?r:parseFloat(r);return isNaN(e)?r:e},hi=function(r){var e={},t=r.split(\"=\");if(t.length>1){var i=Hr(t[0]);e[i]=dt(t[1])}return e},yi=function(r){for(var e=r.split(\",\"),t={},i=0;e.length>i;i++){var a=e[i],n=hi(a);t=Object.assign(n,t)}return t},gi=function(r){return r.indexOf(\"=\")>-1},bi=function(r,e){return e===-1?[r]:[r.substring(0,e),r.substring(e+1)]},Ti=function(r){var e={};if(r){var t=r.search(\",\"),i=r.slice(0,t),a=r.slice(t+1),n=[i,a];return n.forEach(function(o,s){for(var l=o.replace(/['\"]+/g,\"\").split(\"=\"),m=0;m<l.length;m++)l[m]===\"DATA-ID\"&&(e[\"DATA-ID\"]=l[1-m]),l[m]===\"VALUE\"&&(e.VALUE=l[1-m])}),{data:e}}},Br=Fe;var wi={safeCall:nt,safeIncrement:A,getComputedStyle:ot,secondsToMs:st,assign:Object.assign,headersStringToObject:fe,cdnHeadersToRequestId:se,extractHostnameAndDomain:te,extractHostname:U,manifestParser:Br,generateShortID:Oe,generateUUID:$,now:q.now},Ur=wi;var Ei={PLAYER_READY:\"playerready\",VIEW_INIT:\"viewinit\",VIDEO_CHANGE:\"videochange\",PLAY:\"play\",PAUSE:\"pause\",PLAYING:\"playing\",TIME_UPDATE:\"timeupdate\",SEEKING:\"seeking\",SEEKED:\"seeked\",REBUFFER_START:\"rebufferstart\",REBUFFER_END:\"rebufferend\",ERROR:\"error\",ENDED:\"ended\",RENDITION_CHANGE:\"renditionchange\",ORIENTATION_CHANGE:\"orientationchange\",AD_REQUEST:\"adrequest\",AD_RESPONSE:\"adresponse\",AD_BREAK_START:\"adbreakstart\",AD_PLAY:\"adplay\",AD_PLAYING:\"adplaying\",AD_PAUSE:\"adpause\",AD_FIRST_QUARTILE:\"adfirstquartile\",AD_MID_POINT:\"admidpoint\",AD_THIRD_QUARTILE:\"adthirdquartile\",AD_ENDED:\"adended\",AD_BREAK_END:\"adbreakend\",AD_ERROR:\"aderror\",REQUEST_COMPLETED:\"requestcompleted\",REQUEST_FAILED:\"requestfailed\",REQUEST_CANCELLED:\"requestcanceled\"},Fr=Ei;var ki=\"mux-embed\",xi=\"5.3.1\",Di=\"2.1\",j={},ie=function(e){var t=arguments;typeof e==\"string\"?ie.hasOwnProperty(e)?ge.default.setTimeout(function(){t=Array.prototype.splice.call(t,1),ie[e].apply(null,t)},0):O.warn(\"`\"+e+\"` is an unknown task\"):typeof e==\"function\"?ge.default.setTimeout(function(){e(ie)},0):O.warn(\"`\"+e+\"` is invalid.\")},Si={loaded:q.now(),NAME:ki,VERSION:xi,API_VERSION:Di,PLAYER_TRACKED:!1,monitor:function(e,t){return it(ie,e,t)},destroyMonitor:function(e){var t=M(xe(e),1),i=t[0];i&&i.mux&&typeof i.mux.destroy==\"function\"?i.mux.destroy():O.error(\"A video element monitor for `\"+e+\"` has not been initialized via `mux.monitor`.\")},addHLSJS:function(e,t){var i=Z(e);j[i]?j[i].addHLSJS(t):O.error(\"A monitor for `\"+i+\"` has not been initialized.\")},addDashJS:function(e,t){var i=Z(e);j[i]?j[i].addDashJS(t):O.error(\"A monitor for `\"+i+\"` has not been initialized.\")},removeHLSJS:function(e){var t=Z(e);j[t]?j[t].removeHLSJS():O.error(\"A monitor for `\"+t+\"` has not been initialized.\")},removeDashJS:function(e){var t=Z(e);j[t]?j[t].removeDashJS():O.error(\"A monitor for `\"+t+\"` has not been initialized.\")},init:function(e,t){de()&&t&&t.respectDoNotTrack&&O.info(\"The browser's Do Not Track flag is enabled - Mux beaconing is disabled.\");var i=Z(e);j[i]=new cr(ie,i,t)},emit:function(e,t,i){var a=Z(e);j[a]?(j[a].emit(t,i),t===\"destroy\"&&delete j[a]):O.error(\"A monitor for `\"+a+\"` has not been initialized.\")},checkDoNotTrack:de,log:O,utils:Ur,events:Fr,WINDOW_HIDDEN:!1,WINDOW_UNLOADING:!1};Object.assign(ie,Si);typeof ge.default!=\"undefined\"&&typeof ge.default.addEventListener==\"function\"&&ge.default.addEventListener(\"pagehide\",function(r){r.persisted||(ie.WINDOW_UNLOADING=!0)},!1);var cd=ie;export{cd as default};\n/*!\n* JavaScript Cookie v2.1.3\n* https://github.com/js-cookie/js-cookie\n*\n* Copyright 2006, 2015 Klaus Hartl & Fagner Brack\n* Released under the MIT license\n*/\n", "// see https://tools.ietf.org/html/rfc1808\n\n(function (root) {\n  var URL_REGEX =\n    /^(?=((?:[a-zA-Z0-9+\\-.]+:)?))\\1(?=((?:\\/\\/[^\\/?#]*)?))\\2(?=((?:(?:[^?#\\/]*\\/)*[^;?#\\/]*)?))\\3((?:;[^?#]*)?)(\\?[^#]*)?(#[^]*)?$/;\n  var FIRST_SEGMENT_REGEX = /^(?=([^\\/?#]*))\\1([^]*)$/;\n  var SLASH_DOT_REGEX = /(?:\\/|^)\\.(?=\\/)/g;\n  var SLASH_DOT_DOT_REGEX = /(?:\\/|^)\\.\\.\\/(?!\\.\\.\\/)[^\\/]*(?=\\/)/g;\n\n  var URLToolkit = {\n    // If opts.alwaysNormalize is true then the path will always be normalized even when it starts with / or //\n    // E.g\n    // With opts.alwaysNormalize = false (default, spec compliant)\n    // http://a.com/b/cd + /e/f/../g => http://a.com/e/f/../g\n    // With opts.alwaysNormalize = true (not spec compliant)\n    // http://a.com/b/cd + /e/f/../g => http://a.com/e/g\n    buildAbsoluteURL: function (baseURL, relativeURL, opts) {\n      opts = opts || {};\n      // remove any remaining space and CRLF\n      baseURL = baseURL.trim();\n      relativeURL = relativeURL.trim();\n      if (!relativeURL) {\n        // 2a) If the embedded URL is entirely empty, it inherits the\n        // entire base URL (i.e., is set equal to the base URL)\n        // and we are done.\n        if (!opts.alwaysNormalize) {\n          return baseURL;\n        }\n        var basePartsForNormalise = URLToolkit.parseURL(baseURL);\n        if (!basePartsForNormalise) {\n          throw new Error('Error trying to parse base URL.');\n        }\n        basePartsForNormalise.path = URLToolkit.normalizePath(\n          basePartsForNormalise.path\n        );\n        return URLToolkit.buildURLFromParts(basePartsForNormalise);\n      }\n      var relativeParts = URLToolkit.parseURL(relativeURL);\n      if (!relativeParts) {\n        throw new Error('Error trying to parse relative URL.');\n      }\n      if (relativeParts.scheme) {\n        // 2b) If the embedded URL starts with a scheme name, it is\n        // interpreted as an absolute URL and we are done.\n        if (!opts.alwaysNormalize) {\n          return relativeURL;\n        }\n        relativeParts.path = URLToolkit.normalizePath(relativeParts.path);\n        return URLToolkit.buildURLFromParts(relativeParts);\n      }\n      var baseParts = URLToolkit.parseURL(baseURL);\n      if (!baseParts) {\n        throw new Error('Error trying to parse base URL.');\n      }\n      if (!baseParts.netLoc && baseParts.path && baseParts.path[0] !== '/') {\n        // If netLoc missing and path doesn't start with '/', assume everthing before the first '/' is the netLoc\n        // This causes 'example.com/a' to be handled as '//example.com/a' instead of '/example.com/a'\n        var pathParts = FIRST_SEGMENT_REGEX.exec(baseParts.path);\n        baseParts.netLoc = pathParts[1];\n        baseParts.path = pathParts[2];\n      }\n      if (baseParts.netLoc && !baseParts.path) {\n        baseParts.path = '/';\n      }\n      var builtParts = {\n        // 2c) Otherwise, the embedded URL inherits the scheme of\n        // the base URL.\n        scheme: baseParts.scheme,\n        netLoc: relativeParts.netLoc,\n        path: null,\n        params: relativeParts.params,\n        query: relativeParts.query,\n        fragment: relativeParts.fragment,\n      };\n      if (!relativeParts.netLoc) {\n        // 3) If the embedded URL's <net_loc> is non-empty, we skip to\n        // Step 7.  Otherwise, the embedded URL inherits the <net_loc>\n        // (if any) of the base URL.\n        builtParts.netLoc = baseParts.netLoc;\n        // 4) If the embedded URL path is preceded by a slash \"/\", the\n        // path is not relative and we skip to Step 7.\n        if (relativeParts.path[0] !== '/') {\n          if (!relativeParts.path) {\n            // 5) If the embedded URL path is empty (and not preceded by a\n            // slash), then the embedded URL inherits the base URL path\n            builtParts.path = baseParts.path;\n            // 5a) if the embedded URL's <params> is non-empty, we skip to\n            // step 7; otherwise, it inherits the <params> of the base\n            // URL (if any) and\n            if (!relativeParts.params) {\n              builtParts.params = baseParts.params;\n              // 5b) if the embedded URL's <query> is non-empty, we skip to\n              // step 7; otherwise, it inherits the <query> of the base\n              // URL (if any) and we skip to step 7.\n              if (!relativeParts.query) {\n                builtParts.query = baseParts.query;\n              }\n            }\n          } else {\n            // 6) The last segment of the base URL's path (anything\n            // following the rightmost slash \"/\", or the entire path if no\n            // slash is present) is removed and the embedded URL's path is\n            // appended in its place.\n            var baseURLPath = baseParts.path;\n            var newPath =\n              baseURLPath.substring(0, baseURLPath.lastIndexOf('/') + 1) +\n              relativeParts.path;\n            builtParts.path = URLToolkit.normalizePath(newPath);\n          }\n        }\n      }\n      if (builtParts.path === null) {\n        builtParts.path = opts.alwaysNormalize\n          ? URLToolkit.normalizePath(relativeParts.path)\n          : relativeParts.path;\n      }\n      return URLToolkit.buildURLFromParts(builtParts);\n    },\n    parseURL: function (url) {\n      var parts = URL_REGEX.exec(url);\n      if (!parts) {\n        return null;\n      }\n      return {\n        scheme: parts[1] || '',\n        netLoc: parts[2] || '',\n        path: parts[3] || '',\n        params: parts[4] || '',\n        query: parts[5] || '',\n        fragment: parts[6] || '',\n      };\n    },\n    normalizePath: function (path) {\n      // The following operations are\n      // then applied, in order, to the new path:\n      // 6a) All occurrences of \"./\", where \".\" is a complete path\n      // segment, are removed.\n      // 6b) If the path ends with \".\" as a complete path segment,\n      // that \".\" is removed.\n      path = path.split('').reverse().join('').replace(SLASH_DOT_REGEX, '');\n      // 6c) All occurrences of \"<segment>/../\", where <segment> is a\n      // complete path segment not equal to \"..\", are removed.\n      // Removal of these path segments is performed iteratively,\n      // removing the leftmost matching pattern on each iteration,\n      // until no matching pattern remains.\n      // 6d) If the path ends with \"<segment>/..\", where <segment> is a\n      // complete path segment not equal to \"..\", that\n      // \"<segment>/..\" is removed.\n      while (\n        path.length !== (path = path.replace(SLASH_DOT_DOT_REGEX, '')).length\n      ) {}\n      return path.split('').reverse().join('');\n    },\n    buildURLFromParts: function (parts) {\n      return (\n        parts.scheme +\n        parts.netLoc +\n        parts.path +\n        parts.params +\n        parts.query +\n        parts.fragment\n      );\n    },\n  };\n\n  if (typeof exports === 'object' && typeof module === 'object')\n    module.exports = URLToolkit;\n  else if (typeof define === 'function' && define.amd)\n    define([], function () {\n      return URLToolkit;\n    });\n  else if (typeof exports === 'object') exports['URLToolkit'] = URLToolkit;\n  else root['URLToolkit'] = URLToolkit;\n})(this);\n", "// https://caniuse.com/mdn-javascript_builtins_number_isfinite\nexport const isFiniteNumber =\n  Number.isFinite ||\n  function (value) {\n    return typeof value === 'number' && isFinite(value);\n  };\n\n// https://caniuse.com/mdn-javascript_builtins_number_issafeinteger\nexport const isSafeInteger =\n  Number.isSafeInteger ||\n  function (value) {\n    return typeof value === 'number' && Math.abs(value) <= MAX_SAFE_INTEGER;\n  };\n\nexport const MAX_SAFE_INTEGER = Number.MAX_SAFE_INTEGER || 9007199254740991;\n", "import {\n  ManifestLoadedData,\n  ManifestLoadingData,\n  MediaAttachedData,\n  MediaAttachingData,\n  LevelLoadingData,\n  LevelLoadedData,\n  ManifestParsedData,\n  LevelUpdatedData,\n  LevelsUpdatedData,\n  FragParsingUserdataData,\n  FragDecryptedData,\n  FragLoadedData,\n  InitPTSFoundData,\n  CuesParsedData,\n  SubtitleFragProcessedData,\n  NonNativeTextTracksData,\n  FragLoadingData,\n  AudioTrackLoadedData,\n  SubtitleTrackLoadedData,\n  ErrorData,\n  AudioTrackSwitchingData,\n  AudioTrackSwitchedData,\n  KeyLoadedData,\n  KeyLoadingData,\n  SubtitleTrackSwitchData,\n  SubtitleTracksUpdatedData,\n  LevelSwitchedData,\n  FragChangedData,\n  BufferAppendingData,\n  BufferCodecsData,\n  FragParsingMetadataData,\n  FragParsingInitSegmentData,\n  FragBufferedData,\n  BufferFlushingData,\n  BufferEOSData,\n  LevelSwitchingData,\n  MaxAutoLevelUpdatedData,\n  FPSDropLevelCappingData,\n  FPSDropData,\n  BufferCreatedData,\n  BufferAppendedData,\n  LevelPTSUpdatedData,\n  FragParsedData,\n  AudioTracksUpdatedData,\n  FragLoadEmergencyAbortedData,\n  BackBufferData,\n  LiveBackBufferData,\n  TrackLoadingData,\n  BufferFlushedData,\n  SteeringManifestLoadedData,\n} from './types/events';\n\nexport enum Events {\n  // Fired before MediaSource is attaching to media element\n  MEDIA_ATTACHING = 'hlsMediaAttaching',\n  // Fired when MediaSource has been successfully attached to media element\n  MEDIA_ATTACHED = 'hlsMediaAttached',\n  // Fired before detaching MediaSource from media element\n  MEDIA_DETACHING = 'hlsMediaDetaching',\n  // Fired when MediaSource has been detached from media element\n  MEDIA_DETACHED = 'hlsMediaDetached',\n  // Fired when the buffer is going to be reset\n  BUFFER_RESET = 'hlsBufferReset',\n  // Fired when we know about the codecs that we need buffers for to push into - data: {tracks : { container, codec, levelCodec, initSegment, metadata }}\n  BUFFER_CODECS = 'hlsBufferCodecs',\n  // fired when sourcebuffers have been created - data: { tracks : tracks }\n  BUFFER_CREATED = 'hlsBufferCreated',\n  // fired when we append a segment to the buffer - data: { segment: segment object }\n  BUFFER_APPENDING = 'hlsBufferAppending',\n  // fired when we are done with appending a media segment to the buffer - data : { parent : segment parent that triggered BUFFER_APPENDING, pending : nb of segments waiting for appending for this segment parent}\n  BUFFER_APPENDED = 'hlsBufferAppended',\n  // fired when the stream is finished and we want to notify the media buffer that there will be no more data - data: { }\n  BUFFER_EOS = 'hlsBufferEos',\n  // fired when the media buffer should be flushed - data { startOffset, endOffset }\n  BUFFER_FLUSHING = 'hlsBufferFlushing',\n  // fired when the media buffer has been flushed - data: { }\n  BUFFER_FLUSHED = 'hlsBufferFlushed',\n  // fired to signal that a manifest loading starts - data: { url : manifestURL}\n  MANIFEST_LOADING = 'hlsManifestLoading',\n  // fired after manifest has been loaded - data: { levels : [available quality levels], audioTracks : [ available audio tracks ], url : manifestURL, stats : LoaderStats }\n  MANIFEST_LOADED = 'hlsManifestLoaded',\n  // fired after manifest has been parsed - data: { levels : [available quality levels], firstLevel : index of first quality level appearing in Manifest}\n  MANIFEST_PARSED = 'hlsManifestParsed',\n  // fired when a level switch is requested - data: { level : id of new level }\n  LEVEL_SWITCHING = 'hlsLevelSwitching',\n  // fired when a level switch is effective - data: { level : id of new level }\n  LEVEL_SWITCHED = 'hlsLevelSwitched',\n  // fired when a level playlist loading starts - data: { url : level URL, level : id of level being loaded}\n  LEVEL_LOADING = 'hlsLevelLoading',\n  // fired when a level playlist loading finishes - data: { details : levelDetails object, level : id of loaded level, stats : LoaderStats }\n  LEVEL_LOADED = 'hlsLevelLoaded',\n  // fired when a level's details have been updated based on previous details, after it has been loaded - data: { details : levelDetails object, level : id of updated level }\n  LEVEL_UPDATED = 'hlsLevelUpdated',\n  // fired when a level's PTS information has been updated after parsing a fragment - data: { details : levelDetails object, level : id of updated level, drift: PTS drift observed when parsing last fragment }\n  LEVEL_PTS_UPDATED = 'hlsLevelPtsUpdated',\n  // fired to notify that levels have changed after removing a level - data: { levels : [available quality levels] }\n  LEVELS_UPDATED = 'hlsLevelsUpdated',\n  // fired to notify that audio track lists has been updated - data: { audioTracks : audioTracks }\n  AUDIO_TRACKS_UPDATED = 'hlsAudioTracksUpdated',\n  // fired when an audio track switching is requested - data: { id : audio track id }\n  AUDIO_TRACK_SWITCHING = 'hlsAudioTrackSwitching',\n  // fired when an audio track switch actually occurs - data: { id : audio track id }\n  AUDIO_TRACK_SWITCHED = 'hlsAudioTrackSwitched',\n  // fired when an audio track loading starts - data: { url : audio track URL, id : audio track id }\n  AUDIO_TRACK_LOADING = 'hlsAudioTrackLoading',\n  // fired when an audio track loading finishes - data: { details : levelDetails object, id : audio track id, stats : LoaderStats }\n  AUDIO_TRACK_LOADED = 'hlsAudioTrackLoaded',\n  // fired to notify that subtitle track lists has been updated - data: { subtitleTracks : subtitleTracks }\n  SUBTITLE_TRACKS_UPDATED = 'hlsSubtitleTracksUpdated',\n  // fired to notify that subtitle tracks were cleared as a result of stopping the media\n  SUBTITLE_TRACKS_CLEARED = 'hlsSubtitleTracksCleared',\n  // fired when an subtitle track switch occurs - data: { id : subtitle track id }\n  SUBTITLE_TRACK_SWITCH = 'hlsSubtitleTrackSwitch',\n  // fired when a subtitle track loading starts - data: { url : subtitle track URL, id : subtitle track id }\n  SUBTITLE_TRACK_LOADING = 'hlsSubtitleTrackLoading',\n  // fired when a subtitle track loading finishes - data: { details : levelDetails object, id : subtitle track id, stats : LoaderStats }\n  SUBTITLE_TRACK_LOADED = 'hlsSubtitleTrackLoaded',\n  // fired when a subtitle fragment has been processed - data: { success : boolean, frag : the processed frag }\n  SUBTITLE_FRAG_PROCESSED = 'hlsSubtitleFragProcessed',\n  // fired when a set of VTTCues to be managed externally has been parsed - data: { type: string, track: string, cues: [ VTTCue ] }\n  CUES_PARSED = 'hlsCuesParsed',\n  // fired when a text track to be managed externally is found - data: { tracks: [ { label: string, kind: string, default: boolean } ] }\n  NON_NATIVE_TEXT_TRACKS_FOUND = 'hlsNonNativeTextTracksFound',\n  // fired when the first timestamp is found - data: { id : demuxer id, initPTS: initPTS, timescale: timescale, frag : fragment object }\n  INIT_PTS_FOUND = 'hlsInitPtsFound',\n  // fired when a fragment loading starts - data: { frag : fragment object }\n  FRAG_LOADING = 'hlsFragLoading',\n  // fired when a fragment loading is progressing - data: { frag : fragment object, { trequest, tfirst, loaded } }\n  // FRAG_LOAD_PROGRESS = 'hlsFragLoadProgress',\n  // Identifier for fragment load aborting for emergency switch down - data: { frag : fragment object }\n  FRAG_LOAD_EMERGENCY_ABORTED = 'hlsFragLoadEmergencyAborted',\n  // fired when a fragment loading is completed - data: { frag : fragment object, payload : fragment payload, stats : LoaderStats }\n  FRAG_LOADED = 'hlsFragLoaded',\n  // fired when a fragment has finished decrypting - data: { id : demuxer id, frag: fragment object, payload : fragment payload, stats : { tstart, tdecrypt } }\n  FRAG_DECRYPTED = 'hlsFragDecrypted',\n  // fired when Init Segment has been extracted from fragment - data: { id : demuxer id, frag: fragment object, moov : moov MP4 box, codecs : codecs found while parsing fragment }\n  FRAG_PARSING_INIT_SEGMENT = 'hlsFragParsingInitSegment',\n  // fired when parsing sei text is completed - data: { id : demuxer id, frag: fragment object, samples : [ sei samples pes ] }\n  FRAG_PARSING_USERDATA = 'hlsFragParsingUserdata',\n  // fired when parsing id3 is completed - data: { id : demuxer id, frag: fragment object, samples : [ id3 samples pes ] }\n  FRAG_PARSING_METADATA = 'hlsFragParsingMetadata',\n  // fired when data have been extracted from fragment - data: { id : demuxer id, frag: fragment object, data1 : moof MP4 box or TS fragments, data2 : mdat MP4 box or null}\n  // FRAG_PARSING_DATA = 'hlsFragParsingData',\n  // fired when fragment parsing is completed - data: { id : demuxer id, frag: fragment object }\n  FRAG_PARSED = 'hlsFragParsed',\n  // fired when fragment remuxed MP4 boxes have all been appended into SourceBuffer - data: { id : demuxer id, frag : fragment object, stats : LoaderStats }\n  FRAG_BUFFERED = 'hlsFragBuffered',\n  // fired when fragment matching with current media position is changing - data : { id : demuxer id, frag : fragment object }\n  FRAG_CHANGED = 'hlsFragChanged',\n  // Identifier for a FPS drop event - data: { currentDropped, currentDecoded, totalDroppedFrames }\n  FPS_DROP = 'hlsFpsDrop',\n  // triggered when FPS drop triggers auto level capping - data: { level, droppedLevel }\n  FPS_DROP_LEVEL_CAPPING = 'hlsFpsDropLevelCapping',\n  // triggered when maxAutoLevel changes - data { autoLevelCapping, levels, maxAutoLevel, minAutoLevel, maxHdcpLevel }\n  MAX_AUTO_LEVEL_UPDATED = 'hlsMaxAutoLevelUpdated',\n  // Identifier for an error event - data: { type : error type, details : error details, fatal : if true, hls.js cannot/will not try to recover, if false, hls.js will try to recover,other error specific data }\n  ERROR = 'hlsError',\n  // fired when hls.js instance starts destroying. Different from MEDIA_DETACHED as one could want to detach and reattach a media to the instance of hls.js to handle mid-rolls for example - data: { }\n  DESTROYING = 'hlsDestroying',\n  // fired when a decrypt key loading starts - data: { frag : fragment object }\n  KEY_LOADING = 'hlsKeyLoading',\n  // fired when a decrypt key loading is completed - data: { frag : fragment object, keyInfo : KeyLoaderInfo }\n  KEY_LOADED = 'hlsKeyLoaded',\n  // deprecated; please use BACK_BUFFER_REACHED - data : { bufferEnd: number }\n  LIVE_BACK_BUFFER_REACHED = 'hlsLiveBackBufferReached',\n  // fired when the back buffer is reached as defined by the backBufferLength config option - data : { bufferEnd: number }\n  BACK_BUFFER_REACHED = 'hlsBackBufferReached',\n  // fired after steering manifest has been loaded - data: { steeringManifest: SteeringManifest object, url: steering manifest URL }\n  STEERING_MANIFEST_LOADED = 'hlsSteeringManifestLoaded',\n}\n\n/**\n * Defines each Event type and payload by Event name. Used in {@link hls.js#HlsEventEmitter} to strongly type the event listener API.\n */\nexport interface HlsListeners {\n  [Events.MEDIA_ATTACHING]: (\n    event: Events.MEDIA_ATTACHING,\n    data: MediaAttachingData,\n  ) => void;\n  [Events.MEDIA_ATTACHED]: (\n    event: Events.MEDIA_ATTACHED,\n    data: MediaAttachedData,\n  ) => void;\n  [Events.MEDIA_DETACHING]: (event: Events.MEDIA_DETACHING) => void;\n  [Events.MEDIA_DETACHED]: (event: Events.MEDIA_DETACHED) => void;\n  [Events.BUFFER_RESET]: (event: Events.BUFFER_RESET) => void;\n  [Events.BUFFER_CODECS]: (\n    event: Events.BUFFER_CODECS,\n    data: BufferCodecsData,\n  ) => void;\n  [Events.BUFFER_CREATED]: (\n    event: Events.BUFFER_CREATED,\n    data: BufferCreatedData,\n  ) => void;\n  [Events.BUFFER_APPENDING]: (\n    event: Events.BUFFER_APPENDING,\n    data: BufferAppendingData,\n  ) => void;\n  [Events.BUFFER_APPENDED]: (\n    event: Events.BUFFER_APPENDED,\n    data: BufferAppendedData,\n  ) => void;\n  [Events.BUFFER_EOS]: (event: Events.BUFFER_EOS, data: BufferEOSData) => void;\n  [Events.BUFFER_FLUSHING]: (\n    event: Events.BUFFER_FLUSHING,\n    data: BufferFlushingData,\n  ) => void;\n  [Events.BUFFER_FLUSHED]: (\n    event: Events.BUFFER_FLUSHED,\n    data: BufferFlushedData,\n  ) => void;\n  [Events.MANIFEST_LOADING]: (\n    event: Events.MANIFEST_LOADING,\n    data: ManifestLoadingData,\n  ) => void;\n  [Events.MANIFEST_LOADED]: (\n    event: Events.MANIFEST_LOADED,\n    data: ManifestLoadedData,\n  ) => void;\n  [Events.MANIFEST_PARSED]: (\n    event: Events.MANIFEST_PARSED,\n    data: ManifestParsedData,\n  ) => void;\n  [Events.LEVEL_SWITCHING]: (\n    event: Events.LEVEL_SWITCHING,\n    data: LevelSwitchingData,\n  ) => void;\n  [Events.LEVEL_SWITCHED]: (\n    event: Events.LEVEL_SWITCHED,\n    data: LevelSwitchedData,\n  ) => void;\n  [Events.LEVEL_LOADING]: (\n    event: Events.LEVEL_LOADING,\n    data: LevelLoadingData,\n  ) => void;\n  [Events.LEVEL_LOADED]: (\n    event: Events.LEVEL_LOADED,\n    data: LevelLoadedData,\n  ) => void;\n  [Events.LEVEL_UPDATED]: (\n    event: Events.LEVEL_UPDATED,\n    data: LevelUpdatedData,\n  ) => void;\n  [Events.LEVEL_PTS_UPDATED]: (\n    event: Events.LEVEL_PTS_UPDATED,\n    data: LevelPTSUpdatedData,\n  ) => void;\n  [Events.LEVELS_UPDATED]: (\n    event: Events.LEVELS_UPDATED,\n    data: LevelsUpdatedData,\n  ) => void;\n  [Events.AUDIO_TRACKS_UPDATED]: (\n    event: Events.AUDIO_TRACKS_UPDATED,\n    data: AudioTracksUpdatedData,\n  ) => void;\n  [Events.AUDIO_TRACK_SWITCHING]: (\n    event: Events.AUDIO_TRACK_SWITCHING,\n    data: AudioTrackSwitchingData,\n  ) => void;\n  [Events.AUDIO_TRACK_SWITCHED]: (\n    event: Events.AUDIO_TRACK_SWITCHED,\n    data: AudioTrackSwitchedData,\n  ) => void;\n  [Events.AUDIO_TRACK_LOADING]: (\n    event: Events.AUDIO_TRACK_LOADING,\n    data: TrackLoadingData,\n  ) => void;\n  [Events.AUDIO_TRACK_LOADED]: (\n    event: Events.AUDIO_TRACK_LOADED,\n    data: AudioTrackLoadedData,\n  ) => void;\n  [Events.SUBTITLE_TRACKS_UPDATED]: (\n    event: Events.SUBTITLE_TRACKS_UPDATED,\n    data: SubtitleTracksUpdatedData,\n  ) => void;\n  [Events.SUBTITLE_TRACKS_CLEARED]: (\n    event: Events.SUBTITLE_TRACKS_CLEARED,\n  ) => void;\n  [Events.SUBTITLE_TRACK_SWITCH]: (\n    event: Events.SUBTITLE_TRACK_SWITCH,\n    data: SubtitleTrackSwitchData,\n  ) => void;\n  [Events.SUBTITLE_TRACK_LOADING]: (\n    event: Events.SUBTITLE_TRACK_LOADING,\n    data: TrackLoadingData,\n  ) => void;\n  [Events.SUBTITLE_TRACK_LOADED]: (\n    event: Events.SUBTITLE_TRACK_LOADED,\n    data: SubtitleTrackLoadedData,\n  ) => void;\n  [Events.SUBTITLE_FRAG_PROCESSED]: (\n    event: Events.SUBTITLE_FRAG_PROCESSED,\n    data: SubtitleFragProcessedData,\n  ) => void;\n  [Events.CUES_PARSED]: (\n    event: Events.CUES_PARSED,\n    data: CuesParsedData,\n  ) => void;\n  [Events.NON_NATIVE_TEXT_TRACKS_FOUND]: (\n    event: Events.NON_NATIVE_TEXT_TRACKS_FOUND,\n    data: NonNativeTextTracksData,\n  ) => void;\n  [Events.INIT_PTS_FOUND]: (\n    event: Events.INIT_PTS_FOUND,\n    data: InitPTSFoundData,\n  ) => void;\n  [Events.FRAG_LOADING]: (\n    event: Events.FRAG_LOADING,\n    data: FragLoadingData,\n  ) => void;\n  // [Events.FRAG_LOAD_PROGRESS]: TodoEventType\n  [Events.FRAG_LOAD_EMERGENCY_ABORTED]: (\n    event: Events.FRAG_LOAD_EMERGENCY_ABORTED,\n    data: FragLoadEmergencyAbortedData,\n  ) => void;\n  [Events.FRAG_LOADED]: (\n    event: Events.FRAG_LOADED,\n    data: FragLoadedData,\n  ) => void;\n  [Events.FRAG_DECRYPTED]: (\n    event: Events.FRAG_DECRYPTED,\n    data: FragDecryptedData,\n  ) => void;\n  [Events.FRAG_PARSING_INIT_SEGMENT]: (\n    event: Events.FRAG_PARSING_INIT_SEGMENT,\n    data: FragParsingInitSegmentData,\n  ) => void;\n  [Events.FRAG_PARSING_USERDATA]: (\n    event: Events.FRAG_PARSING_USERDATA,\n    data: FragParsingUserdataData,\n  ) => void;\n  [Events.FRAG_PARSING_METADATA]: (\n    event: Events.FRAG_PARSING_METADATA,\n    data: FragParsingMetadataData,\n  ) => void;\n  // [Events.FRAG_PARSING_DATA]: TodoEventType\n  [Events.FRAG_PARSED]: (\n    event: Events.FRAG_PARSED,\n    data: FragParsedData,\n  ) => void;\n  [Events.FRAG_BUFFERED]: (\n    event: Events.FRAG_BUFFERED,\n    data: FragBufferedData,\n  ) => void;\n  [Events.FRAG_CHANGED]: (\n    event: Events.FRAG_CHANGED,\n    data: FragChangedData,\n  ) => void;\n  [Events.FPS_DROP]: (event: Events.FPS_DROP, data: FPSDropData) => void;\n  [Events.FPS_DROP_LEVEL_CAPPING]: (\n    event: Events.FPS_DROP_LEVEL_CAPPING,\n    data: FPSDropLevelCappingData,\n  ) => void;\n  [Events.MAX_AUTO_LEVEL_UPDATED]: (\n    event: Events.MAX_AUTO_LEVEL_UPDATED,\n    data: MaxAutoLevelUpdatedData,\n  ) => void;\n  [Events.ERROR]: (event: Events.ERROR, data: ErrorData) => void;\n  [Events.DESTROYING]: (event: Events.DESTROYING) => void;\n  [Events.KEY_LOADING]: (\n    event: Events.KEY_LOADING,\n    data: KeyLoadingData,\n  ) => void;\n  [Events.KEY_LOADED]: (event: Events.KEY_LOADED, data: KeyLoadedData) => void;\n  [Events.LIVE_BACK_BUFFER_REACHED]: (\n    event: Events.LIVE_BACK_BUFFER_REACHED,\n    data: LiveBackBufferData,\n  ) => void;\n  [Events.BACK_BUFFER_REACHED]: (\n    event: Events.BACK_BUFFER_REACHED,\n    data: BackBufferData,\n  ) => void;\n  [Events.STEERING_MANIFEST_LOADED]: (\n    event: Events.STEERING_MANIFEST_LOADED,\n    data: SteeringManifestLoadedData,\n  ) => void;\n}\nexport interface HlsEventEmitter {\n  on<E extends keyof HlsListeners, Context = undefined>(\n    event: E,\n    listener: HlsListeners[E],\n    context?: Context,\n  ): void;\n  once<E extends keyof HlsListeners, Context = undefined>(\n    event: E,\n    listener: HlsListeners[E],\n    context?: Context,\n  ): void;\n\n  removeAllListeners<E extends keyof HlsListeners>(event?: E): void;\n  off<E extends keyof HlsListeners, Context = undefined>(\n    event: E,\n    listener?: HlsListeners[E],\n    context?: Context,\n    once?: boolean,\n  ): void;\n\n  listeners<E extends keyof HlsListeners>(event: E): HlsListeners[E][];\n  emit<E extends keyof HlsListeners>(\n    event: E,\n    name: E,\n    eventObject: Parameters<HlsListeners[E]>[1],\n  ): boolean;\n  listenerCount<E extends keyof HlsListeners>(event: E): number;\n}\n", "export enum ErrorTypes {\n  // Identifier for a network error (loading error / timeout ...)\n  NETWORK_ERROR = 'networkError',\n  // Identifier for a media Error (video/parsing/mediasource error)\n  MEDIA_ERROR = 'mediaError',\n  // EME (encrypted media extensions) errors\n  KEY_SYSTEM_ERROR = 'keySystemError',\n  // Identifier for a mux Error (demuxing/remuxing)\n  MUX_ERROR = 'muxError',\n  // Identifier for all other errors\n  OTHER_ERROR = 'otherError',\n}\n\nexport enum ErrorDetails {\n  KEY_SYSTEM_NO_KEYS = 'keySystemNoKeys',\n  KEY_SYSTEM_NO_ACCESS = 'keySystemNoAccess',\n  KEY_SYSTEM_NO_SESSION = 'keySystemNoSession',\n  KEY_SYSTEM_NO_CONFIGURED_LICENSE = 'keySystemNoConfiguredLicense',\n  KEY_SYSTEM_LICENSE_REQUEST_FAILED = 'keySystemLicenseRequestFailed',\n  KEY_SYSTEM_SERVER_CERTIFICATE_REQUEST_FAILED = 'keySystemServerCertificateRequestFailed',\n  KEY_SYSTEM_SERVER_CERTIFICATE_UPDATE_FAILED = 'keySystemServerCertificateUpdateFailed',\n  KEY_SYSTEM_SESSION_UPDATE_FAILED = 'keySystemSessionUpdateFailed',\n  KEY_SYSTEM_STATUS_OUTPUT_RESTRICTED = 'keySystemStatusOutputRestricted',\n  KEY_SYSTEM_STATUS_INTERNAL_ERROR = 'keySystemStatusInternalError',\n  // Identifier for a manifest load error - data: { url : faulty URL, response : { code: error code, text: error text }}\n  MANIFEST_LOAD_ERROR = 'manifestLoadError',\n  // Identifier for a manifest load timeout - data: { url : faulty URL, response : { code: error code, text: error text }}\n  MANIFEST_LOAD_TIMEOUT = 'manifestLoadTimeOut',\n  // Identifier for a manifest parsing error - data: { url : faulty URL, reason : error reason}\n  MANIFEST_PARSING_ERROR = 'manifestParsingError',\n  // Identifier for a manifest with only incompatible codecs error - data: { url : faulty URL, reason : error reason}\n  MANIFEST_INCOMPATIBLE_CODECS_ERROR = 'manifestIncompatibleCodecsError',\n  // Identifier for a level which contains no fragments - data: { url: faulty URL, reason: \"no fragments found in level\", level: index of the bad level }\n  LEVEL_EMPTY_ERROR = 'levelEmptyError',\n  // Identifier for a level load error - data: { url : faulty URL, response : { code: error code, text: error text }}\n  LEVEL_LOAD_ERROR = 'levelLoadError',\n  // Identifier for a level load timeout - data: { url : faulty URL, response : { code: error code, text: error text }}\n  LEVEL_LOAD_TIMEOUT = 'levelLoadTimeOut',\n  // Identifier for a level parse error - data: { url : faulty URL, error: Error, reason: error message }\n  LEVEL_PARSING_ERROR = 'levelParsingError',\n  // Identifier for a level switch error - data: { level : faulty level Id, event : error description}\n  LEVEL_SWITCH_ERROR = 'levelSwitchError',\n  // Identifier for an audio track load error - data: { url : faulty URL, response : { code: error code, text: error text }}\n  AUDIO_TRACK_LOAD_ERROR = 'audioTrackLoadError',\n  // Identifier for an audio track load timeout - data: { url : faulty URL, response : { code: error code, text: error text }}\n  AUDIO_TRACK_LOAD_TIMEOUT = 'audioTrackLoadTimeOut',\n  // Identifier for a subtitle track load error - data: { url : faulty URL, response : { code: error code, text: error text }}\n  SUBTITLE_LOAD_ERROR = 'subtitleTrackLoadError',\n  // Identifier for a subtitle track load timeout - data: { url : faulty URL, response : { code: error code, text: error text }}\n  SUBTITLE_TRACK_LOAD_TIMEOUT = 'subtitleTrackLoadTimeOut',\n  // Identifier for fragment load error - data: { frag : fragment object, response : { code: error code, text: error text }}\n  FRAG_LOAD_ERROR = 'fragLoadError',\n  // Identifier for fragment load timeout error - data: { frag : fragment object}\n  FRAG_LOAD_TIMEOUT = 'fragLoadTimeOut',\n  // Identifier for a fragment decryption error event - data: {id : demuxer Id,frag: fragment object, reason : parsing error description }\n  FRAG_DECRYPT_ERROR = 'fragDecryptError',\n  // Identifier for a fragment parsing error event - data: { id : demuxer Id, reason : parsing error description }\n  // will be renamed DEMUX_PARSING_ERROR and switched to MUX_ERROR in the next major release\n  FRAG_PARSING_ERROR = 'fragParsingError',\n  // Identifier for a fragment or part load skipped because of a GAP tag or attribute\n  FRAG_GAP = 'fragGap',\n  // Identifier for a remux alloc error event - data: { id : demuxer Id, frag : fragment object, bytes : nb of bytes on which allocation failed , reason : error text }\n  REMUX_ALLOC_ERROR = 'remuxAllocError',\n  // Identifier for decrypt key load error - data: { frag : fragment object, response : { code: error code, text: error text }}\n  KEY_LOAD_ERROR = 'keyLoadError',\n  // Identifier for decrypt key load timeout error - data: { frag : fragment object}\n  KEY_LOAD_TIMEOUT = 'keyLoadTimeOut',\n  // Triggered when an exception occurs while adding a sourceBuffer to MediaSource - data : { error : exception , mimeType : mimeType }\n  BUFFER_ADD_CODEC_ERROR = 'bufferAddCodecError',\n  // Triggered when source buffer(s) could not be created using level (manifest CODECS attribute), parsed media, or best guess codec(s) - data: { reason : error reason }\n  BUFFER_INCOMPATIBLE_CODECS_ERROR = 'bufferIncompatibleCodecsError',\n  // Identifier for a buffer append error - data: append error description\n  BUFFER_APPEND_ERROR = 'bufferAppendError',\n  // Identifier for a buffer appending error event - data: appending error description\n  BUFFER_APPENDING_ERROR = 'bufferAppendingError',\n  // Identifier for a buffer stalled error event\n  BUFFER_STALLED_ERROR = 'bufferStalledError',\n  // Identifier for a buffer full event\n  BUFFER_FULL_ERROR = 'bufferFullError',\n  // Identifier for a buffer seek over hole event\n  BUFFER_SEEK_OVER_HOLE = 'bufferSeekOverHole',\n  // Identifier for a buffer nudge on stall (playback is stuck although currentTime is in a buffered area)\n  BUFFER_NUDGE_ON_STALL = 'bufferNudgeOnStall',\n  // Identifier for an internal exception happening inside hls.js while handling an event\n  INTERNAL_EXCEPTION = 'internalException',\n  // Identifier for an internal call to abort a loader\n  INTERNAL_ABORTED = 'aborted',\n  // Uncategorized error\n  UNKNOWN = 'unknown',\n}\n", "export interface ILogFunction {\n  (message?: any, ...optionalParams: any[]): void;\n}\n\nexport interface ILogger {\n  trace: ILogFunction;\n  debug: ILogFunction;\n  log: ILogFunction;\n  warn: ILogFunction;\n  info: ILogFunction;\n  error: ILogFunction;\n}\n\nconst noop: ILogFunction = function () {};\n\nconst fakeLogger: ILogger = {\n  trace: noop,\n  debug: noop,\n  log: noop,\n  warn: noop,\n  info: noop,\n  error: noop,\n};\n\nlet exportedLogger: ILogger = fakeLogger;\n\n// let lastCallTime;\n// function formatMsgWithTimeInfo(type, msg) {\n//   const now = Date.now();\n//   const diff = lastCallTime ? '+' + (now - lastCallTime) : '0';\n//   lastCallTime = now;\n//   msg = (new Date(now)).toISOString() + ' | [' +  type + '] > ' + msg + ' ( ' + diff + ' ms )';\n//   return msg;\n// }\n\nfunction consolePrintFn(type: string): ILogFunction {\n  const func: ILogFunction = self.console[type];\n  if (func) {\n    return func.bind(self.console, `[${type}] >`);\n  }\n  return noop;\n}\n\nfunction exportLoggerFunctions(\n  debugConfig: boolean | ILogger,\n  ...functions: string[]\n): void {\n  functions.forEach(function (type) {\n    exportedLogger[type] = debugConfig[type]\n      ? debugConfig[type].bind(debugConfig)\n      : consolePrintFn(type);\n  });\n}\n\nexport function enableLogs(debugConfig: boolean | ILogger, id: string): void {\n  // check that console is available\n  if (\n    (typeof console === 'object' && debugConfig === true) ||\n    typeof debugConfig === 'object'\n  ) {\n    exportLoggerFunctions(\n      debugConfig,\n      // Remove out from list here to hard-disable a log-level\n      // 'trace',\n      'debug',\n      'log',\n      'info',\n      'warn',\n      'error',\n    );\n    // Some browsers don't allow to use bind on console object anyway\n    // fallback to default if needed\n    try {\n      exportedLogger.log(\n        `Debug logs enabled for \"${id}\" in hls.js version ${__VERSION__}`,\n      );\n    } catch (e) {\n      exportedLogger = fakeLogger;\n    }\n  } else {\n    exportedLogger = fakeLogger;\n  }\n}\n\nexport const logger: ILogger = exportedLogger;\n", "const DECIMAL_RESOLUTION_REGEX = /^(\\d+)x(\\d+)$/;\nconst ATTR_LIST_REGEX = /(.+?)=(\".*?\"|.*?)(?:,|$)/g;\n\n// adapted from https://github.com/kanongil/node-m3u8parse/blob/master/attrlist.js\nexport class AttrList {\n  [key: string]: any;\n\n  constructor(attrs: string | Record<string, any>) {\n    if (typeof attrs === 'string') {\n      attrs = AttrList.parseAttrList(attrs);\n    }\n    Object.assign(this, attrs);\n  }\n\n  get clientAttrs(): string[] {\n    return Object.keys(this).filter((attr) => attr.substring(0, 2) === 'X-');\n  }\n\n  decimalInteger(attrName: string): number {\n    const intValue = parseInt(this[attrName], 10);\n    if (intValue > Number.MAX_SAFE_INTEGER) {\n      return Infinity;\n    }\n\n    return intValue;\n  }\n\n  hexadecimalInteger(attrName: string) {\n    if (this[attrName]) {\n      let stringValue = (this[attrName] || '0x').slice(2);\n      stringValue = (stringValue.length & 1 ? '0' : '') + stringValue;\n\n      const value = new Uint8Array(stringValue.length / 2);\n      for (let i = 0; i < stringValue.length / 2; i++) {\n        value[i] = parseInt(stringValue.slice(i * 2, i * 2 + 2), 16);\n      }\n\n      return value;\n    } else {\n      return null;\n    }\n  }\n\n  hexadecimalIntegerAsNumber(attrName: string): number {\n    const intValue = parseInt(this[attrName], 16);\n    if (intValue > Number.MAX_SAFE_INTEGER) {\n      return Infinity;\n    }\n\n    return intValue;\n  }\n\n  decimalFloatingPoint(attrName: string): number {\n    return parseFloat(this[attrName]);\n  }\n\n  optionalFloat(attrName: string, defaultValue: number): number {\n    const value = this[attrName];\n    return value ? parseFloat(value) : defaultValue;\n  }\n\n  enumeratedString(attrName: string): string | undefined {\n    return this[attrName];\n  }\n\n  bool(attrName: string): boolean {\n    return this[attrName] === 'YES';\n  }\n\n  decimalResolution(attrName: string):\n    | {\n        width: number;\n        height: number;\n      }\n    | undefined {\n    const res = DECIMAL_RESOLUTION_REGEX.exec(this[attrName]);\n    if (res === null) {\n      return undefined;\n    }\n\n    return {\n      width: parseInt(res[1], 10),\n      height: parseInt(res[2], 10),\n    };\n  }\n\n  static parseAttrList(input: string): Record<string, any> {\n    let match;\n    const attrs = {};\n    const quote = '\"';\n    ATTR_LIST_REGEX.lastIndex = 0;\n    while ((match = ATTR_LIST_REGEX.exec(input)) !== null) {\n      let value = match[2];\n\n      if (\n        value.indexOf(quote) === 0 &&\n        value.lastIndexOf(quote) === value.length - 1\n      ) {\n        value = value.slice(1, -1);\n      }\n      const name = match[1].trim();\n      attrs[name] = value;\n    }\n    return attrs;\n  }\n}\n", "import { AttrList } from '../utils/attr-list';\nimport { logger } from '../utils/logger';\n\n// Avoid exporting const enum so that these values can be inlined\nconst enum DateRangeAttribute {\n  ID = 'ID',\n  CLASS = 'CLASS',\n  START_DATE = 'START-DATE',\n  DURATION = 'DURATION',\n  END_DATE = 'END-DATE',\n  END_ON_NEXT = 'END-ON-NEXT',\n  PLANNED_DURATION = 'PLANNED-DURATION',\n  SCTE35_OUT = 'SCTE35-OUT',\n  SCTE35_IN = 'SCTE35-IN',\n}\n\nexport function isDateRangeCueAttribute(attrName: string): boolean {\n  return (\n    attrName !== DateRangeAttribute.ID &&\n    attrName !== DateRangeAttribute.CLASS &&\n    attrName !== DateRangeAttribute.START_DATE &&\n    attrName !== DateRangeAttribute.DURATION &&\n    attrName !== DateRangeAttribute.END_DATE &&\n    attrName !== DateRangeAttribute.END_ON_NEXT\n  );\n}\n\nexport function isSCTE35Attribute(attrName: string): boolean {\n  return (\n    attrName === DateRangeAttribute.SCTE35_OUT ||\n    attrName === DateRangeAttribute.SCTE35_IN\n  );\n}\n\nexport class DateRange {\n  public attr: AttrList;\n  private _startDate: Date;\n  private _endDate?: Date;\n  private _badValueForSameId?: string;\n\n  constructor(dateRangeAttr: AttrList, dateRangeWithSameId?: DateRange) {\n    if (dateRangeWithSameId) {\n      const previousAttr = dateRangeWithSameId.attr;\n      for (const key in previousAttr) {\n        if (\n          Object.prototype.hasOwnProperty.call(dateRangeAttr, key) &&\n          dateRangeAttr[key] !== previousAttr[key]\n        ) {\n          logger.warn(\n            `DATERANGE tag attribute: \"${key}\" does not match for tags with ID: \"${dateRangeAttr.ID}\"`,\n          );\n          this._badValueForSameId = key;\n          break;\n        }\n      }\n      // Merge DateRange tags with the same ID\n      dateRangeAttr = Object.assign(\n        new AttrList({}),\n        previousAttr,\n        dateRangeAttr,\n      );\n    }\n    this.attr = dateRangeAttr;\n    this._startDate = new Date(dateRangeAttr[DateRangeAttribute.START_DATE]);\n    if (DateRangeAttribute.END_DATE in this.attr) {\n      const endDate = new Date(this.attr[DateRangeAttribute.END_DATE]);\n      if (Number.isFinite(endDate.getTime())) {\n        this._endDate = endDate;\n      }\n    }\n  }\n\n  get id(): string {\n    return this.attr.ID;\n  }\n\n  get class(): string {\n    return this.attr.CLASS;\n  }\n\n  get startDate(): Date {\n    return this._startDate;\n  }\n\n  get endDate(): Date | null {\n    if (this._endDate) {\n      return this._endDate;\n    }\n    const duration = this.duration;\n    if (duration !== null) {\n      return new Date(this._startDate.getTime() + duration * 1000);\n    }\n    return null;\n  }\n\n  get duration(): number | null {\n    if (DateRangeAttribute.DURATION in this.attr) {\n      const duration = this.attr.decimalFloatingPoint(\n        DateRangeAttribute.DURATION,\n      );\n      if (Number.isFinite(duration)) {\n        return duration;\n      }\n    } else if (this._endDate) {\n      return (this._endDate.getTime() - this._startDate.getTime()) / 1000;\n    }\n    return null;\n  }\n\n  get plannedDuration(): number | null {\n    if (DateRangeAttribute.PLANNED_DURATION in this.attr) {\n      return this.attr.decimalFloatingPoint(\n        DateRangeAttribute.PLANNED_DURATION,\n      );\n    }\n    return null;\n  }\n\n  get endOnNext(): boolean {\n    return this.attr.bool(DateRangeAttribute.END_ON_NEXT);\n  }\n\n  get isValid(): boolean {\n    return (\n      !!this.id &&\n      !this._badValueForSameId &&\n      Number.isFinite(this.startDate.getTime()) &&\n      (this.duration === null || this.duration >= 0) &&\n      (!this.endOnNext || !!this.class)\n    );\n  }\n}\n", "import type {\n  HlsPerformanceTiming,\n  HlsProgressivePerformanceTiming,\n  LoaderStats,\n} from '../types/loader';\n\nexport class LoadStats implements LoaderStats {\n  aborted: boolean = false;\n  loaded: number = 0;\n  retry: number = 0;\n  total: number = 0;\n  chunkCount: number = 0;\n  bwEstimate: number = 0;\n  loading: HlsProgressivePerformanceTiming = { start: 0, first: 0, end: 0 };\n  parsing: HlsPerformanceTiming = { start: 0, end: 0 };\n  buffering: HlsProgressivePerformanceTiming = { start: 0, first: 0, end: 0 };\n}\n", "import { buildAbsoluteURL } from 'url-toolkit';\nimport { LevelKey } from './level-key';\nimport { LoadStats } from './load-stats';\nimport { AttrList } from '../utils/attr-list';\nimport type {\n  FragmentLoaderContext,\n  KeyLoaderContext,\n  Loader,\n  PlaylistLevelType,\n} from '../types/loader';\nimport type { KeySystemFormats } from '../utils/mediakeys-helper';\n\nexport const enum ElementaryStreamTypes {\n  AUDIO = 'audio',\n  VIDEO = 'video',\n  AUDIOVIDEO = 'audiovideo',\n}\n\nexport interface ElementaryStreamInfo {\n  startPTS: number;\n  endPTS: number;\n  startDTS: number;\n  endDTS: number;\n  partial?: boolean;\n}\n\nexport type ElementaryStreams = Record<\n  ElementaryStreamTypes,\n  ElementaryStreamInfo | null\n>;\n\nexport class BaseSegment {\n  private _byteRange: [number, number] | null = null;\n  private _url: string | null = null;\n\n  // baseurl is the URL to the playlist\n  public readonly baseurl: string;\n  // relurl is the portion of the URL that comes from inside the playlist.\n  public relurl?: string;\n  // Holds the types of data this fragment supports\n  public elementaryStreams: ElementaryStreams = {\n    [ElementaryStreamTypes.AUDIO]: null,\n    [ElementaryStreamTypes.VIDEO]: null,\n    [ElementaryStreamTypes.AUDIOVIDEO]: null,\n  };\n\n  constructor(baseurl: string) {\n    this.baseurl = baseurl;\n  }\n\n  // setByteRange converts a EXT-X-BYTERANGE attribute into a two element array\n  setByteRange(value: string, previous?: BaseSegment) {\n    const params = value.split('@', 2);\n    let start: number;\n    if (params.length === 1) {\n      start = previous?.byteRangeEndOffset || 0;\n    } else {\n      start = parseInt(params[1]);\n    }\n    this._byteRange = [start, parseInt(params[0]) + start];\n  }\n\n  get byteRange(): [number, number] | [] {\n    if (!this._byteRange) {\n      return [];\n    }\n\n    return this._byteRange;\n  }\n\n  get byteRangeStartOffset(): number | undefined {\n    return this.byteRange[0];\n  }\n\n  get byteRangeEndOffset(): number | undefined {\n    return this.byteRange[1];\n  }\n\n  get url(): string {\n    if (!this._url && this.baseurl && this.relurl) {\n      this._url = buildAbsoluteURL(this.baseurl, this.relurl, {\n        alwaysNormalize: true,\n      });\n    }\n    return this._url || '';\n  }\n\n  set url(value: string) {\n    this._url = value;\n  }\n}\n\n/**\n * Object representing parsed data from an HLS Segment. Found in {@link hls.js#LevelDetails.fragments}.\n */\nexport class Fragment extends BaseSegment {\n  private _decryptdata: LevelKey | null = null;\n\n  public rawProgramDateTime: string | null = null;\n  public programDateTime: number | null = null;\n  public tagList: Array<string[]> = [];\n\n  // EXTINF has to be present for a m3u8 to be considered valid\n  public duration: number = 0;\n  // sn notates the sequence number for a segment, and if set to a string can be 'initSegment'\n  public sn: number | 'initSegment' = 0;\n  // levelkeys are the EXT-X-KEY tags that apply to this segment for decryption\n  // core difference from the private field _decryptdata is the lack of the initialized IV\n  // _decryptdata will set the IV for this segment based on the segment number in the fragment\n  public levelkeys?: { [key: string]: LevelKey };\n  // A string representing the fragment type\n  public readonly type: PlaylistLevelType;\n  // A reference to the loader. Set while the fragment is loading, and removed afterwards. Used to abort fragment loading\n  public loader: Loader<FragmentLoaderContext> | null = null;\n  // A reference to the key loader. Set while the key is loading, and removed afterwards. Used to abort key loading\n  public keyLoader: Loader<KeyLoaderContext> | null = null;\n  // The level/track index to which the fragment belongs\n  public level: number = -1;\n  // The continuity counter of the fragment\n  public cc: number = 0;\n  // The starting Presentation Time Stamp (PTS) of the fragment. Set after transmux complete.\n  public startPTS?: number;\n  // The ending Presentation Time Stamp (PTS) of the fragment. Set after transmux complete.\n  public endPTS?: number;\n  // The starting Decode Time Stamp (DTS) of the fragment. Set after transmux complete.\n  public startDTS!: number;\n  // The ending Decode Time Stamp (DTS) of the fragment. Set after transmux complete.\n  public endDTS!: number;\n  // The start time of the fragment, as listed in the manifest. Updated after transmux complete.\n  public start: number = 0;\n  // Set by `updateFragPTSDTS` in level-helper\n  public deltaPTS?: number;\n  // The maximum starting Presentation Time Stamp (audio/video PTS) of the fragment. Set after transmux complete.\n  public maxStartPTS?: number;\n  // The minimum ending Presentation Time Stamp (audio/video PTS) of the fragment. Set after transmux complete.\n  public minEndPTS?: number;\n  // Load/parse timing information\n  public stats: LoadStats = new LoadStats();\n  // Init Segment bytes (unset for media segments)\n  public data?: Uint8Array;\n  // A flag indicating whether the segment was downloaded in order to test bitrate, and was not buffered\n  public bitrateTest: boolean = false;\n  // #EXTINF  segment title\n  public title: string | null = null;\n  // The Media Initialization Section for this segment\n  public initSegment: Fragment | null = null;\n  // Fragment is the last fragment in the media playlist\n  public endList?: boolean;\n  // Fragment is marked by an EXT-X-GAP tag indicating that it does not contain media data and should not be loaded\n  public gap?: boolean;\n  // Deprecated\n  public urlId: number = 0;\n\n  constructor(type: PlaylistLevelType, baseurl: string) {\n    super(baseurl);\n    this.type = type;\n  }\n\n  get decryptdata(): LevelKey | null {\n    const { levelkeys } = this;\n    if (!levelkeys && !this._decryptdata) {\n      return null;\n    }\n\n    if (!this._decryptdata && this.levelkeys && !this.levelkeys.NONE) {\n      const key = this.levelkeys.identity;\n      if (key) {\n        this._decryptdata = key.getDecryptData(this.sn);\n      } else {\n        const keyFormats = Object.keys(this.levelkeys);\n        if (keyFormats.length === 1) {\n          return (this._decryptdata = this.levelkeys[\n            keyFormats[0]\n          ].getDecryptData(this.sn));\n        } else {\n          // Multiple keys. key-loader to call Fragment.setKeyFormat based on selected key-system.\n        }\n      }\n    }\n\n    return this._decryptdata;\n  }\n\n  get end(): number {\n    return this.start + this.duration;\n  }\n\n  get endProgramDateTime() {\n    if (this.programDateTime === null) {\n      return null;\n    }\n\n    if (!Number.isFinite(this.programDateTime)) {\n      return null;\n    }\n\n    const duration = !Number.isFinite(this.duration) ? 0 : this.duration;\n\n    return this.programDateTime + duration * 1000;\n  }\n\n  get encrypted() {\n    // At the m3u8-parser level we need to add support for manifest signalled keyformats\n    // when we want the fragment to start reporting that it is encrypted.\n    // Currently, keyFormat will only be set for identity keys\n    if (this._decryptdata?.encrypted) {\n      return true;\n    } else if (this.levelkeys) {\n      const keyFormats = Object.keys(this.levelkeys);\n      const len = keyFormats.length;\n      if (len > 1 || (len === 1 && this.levelkeys[keyFormats[0]].encrypted)) {\n        return true;\n      }\n    }\n\n    return false;\n  }\n\n  setKeyFormat(keyFormat: KeySystemFormats) {\n    if (this.levelkeys) {\n      const key = this.levelkeys[keyFormat];\n      if (key && !this._decryptdata) {\n        this._decryptdata = key.getDecryptData(this.sn);\n      }\n    }\n  }\n\n  abortRequests(): void {\n    this.loader?.abort();\n    this.keyLoader?.abort();\n  }\n\n  setElementaryStreamInfo(\n    type: ElementaryStreamTypes,\n    startPTS: number,\n    endPTS: number,\n    startDTS: number,\n    endDTS: number,\n    partial: boolean = false,\n  ) {\n    const { elementaryStreams } = this;\n    const info = elementaryStreams[type];\n    if (!info) {\n      elementaryStreams[type] = {\n        startPTS,\n        endPTS,\n        startDTS,\n        endDTS,\n        partial,\n      };\n      return;\n    }\n\n    info.startPTS = Math.min(info.startPTS, startPTS);\n    info.endPTS = Math.max(info.endPTS, endPTS);\n    info.startDTS = Math.min(info.startDTS, startDTS);\n    info.endDTS = Math.max(info.endDTS, endDTS);\n  }\n\n  clearElementaryStreamInfo() {\n    const { elementaryStreams } = this;\n    elementaryStreams[ElementaryStreamTypes.AUDIO] = null;\n    elementaryStreams[ElementaryStreamTypes.VIDEO] = null;\n    elementaryStreams[ElementaryStreamTypes.AUDIOVIDEO] = null;\n  }\n}\n\n/**\n * Object representing parsed data from an HLS Partial Segment. Found in {@link hls.js#LevelDetails.partList}.\n */\nexport class Part extends BaseSegment {\n  public readonly fragOffset: number = 0;\n  public readonly duration: number = 0;\n  public readonly gap: boolean = false;\n  public readonly independent: boolean = false;\n  public readonly relurl: string;\n  public readonly fragment: Fragment;\n  public readonly index: number;\n  public stats: LoadStats = new LoadStats();\n\n  constructor(\n    partAttrs: AttrList,\n    frag: Fragment,\n    baseurl: string,\n    index: number,\n    previous?: Part,\n  ) {\n    super(baseurl);\n    this.duration = partAttrs.decimalFloatingPoint('DURATION');\n    this.gap = partAttrs.bool('GAP');\n    this.independent = partAttrs.bool('INDEPENDENT');\n    this.relurl = partAttrs.enumeratedString('URI') as string;\n    this.fragment = frag;\n    this.index = index;\n    const byteRange = partAttrs.enumeratedString('BYTERANGE');\n    if (byteRange) {\n      this.setByteRange(byteRange, previous);\n    }\n    if (previous) {\n      this.fragOffset = previous.fragOffset + previous.duration;\n    }\n  }\n\n  get start(): number {\n    return this.fragment.start + this.fragOffset;\n  }\n\n  get end(): number {\n    return this.start + this.duration;\n  }\n\n  get loaded(): boolean {\n    const { elementaryStreams } = this;\n    return !!(\n      elementaryStreams.audio ||\n      elementaryStreams.video ||\n      elementaryStreams.audiovideo\n    );\n  }\n}\n", "import { Part } from './fragment';\nimport type { Fragment } from './fragment';\nimport type { AttrList } from '../utils/attr-list';\nimport type { DateRange } from './date-range';\nimport type { VariableMap } from '../types/level';\n\nconst DEFAULT_TARGET_DURATION = 10;\n\n/**\n * Object representing parsed data from an HLS Media Playlist. Found in {@link hls.js#Level.details}.\n */\nexport class LevelDetails {\n  public PTSKnown: boolean = false;\n  public alignedSliding: boolean = false;\n  public averagetargetduration?: number;\n  public endCC: number = 0;\n  public endSN: number = 0;\n  public fragments: Fragment[];\n  public fragmentHint?: Fragment;\n  public partList: Part[] | null = null;\n  public dateRanges: Record<string, DateRange>;\n  public live: boolean = true;\n  public ageHeader: number = 0;\n  public advancedDateTime?: number;\n  public updated: boolean = true;\n  public advanced: boolean = true;\n  public availabilityDelay?: number; // Manifest reload synchronization\n  public misses: number = 0;\n  public startCC: number = 0;\n  public startSN: number = 0;\n  public startTimeOffset: number | null = null;\n  public targetduration: number = 0;\n  public totalduration: number = 0;\n  public type: string | null = null;\n  public url: string;\n  public m3u8: string = '';\n  public version: number | null = null;\n  public canBlockReload: boolean = false;\n  public canSkipUntil: number = 0;\n  public canSkipDateRanges: boolean = false;\n  public skippedSegments: number = 0;\n  public recentlyRemovedDateranges?: string[];\n  public partHoldBack: number = 0;\n  public holdBack: number = 0;\n  public partTarget: number = 0;\n  public preloadHint?: AttrList;\n  public renditionReports?: AttrList[];\n  public tuneInGoal: number = 0;\n  public deltaUpdateFailed?: boolean;\n  public driftStartTime: number = 0;\n  public driftEndTime: number = 0;\n  public driftStart: number = 0;\n  public driftEnd: number = 0;\n  public encryptedFragments: Fragment[];\n  public playlistParsingError: Error | null = null;\n  public variableList: VariableMap | null = null;\n  public hasVariableRefs = false;\n\n  constructor(baseUrl: string) {\n    this.fragments = [];\n    this.encryptedFragments = [];\n    this.dateRanges = {};\n    this.url = baseUrl;\n  }\n\n  reloaded(previous: LevelDetails | undefined) {\n    if (!previous) {\n      this.advanced = true;\n      this.updated = true;\n      return;\n    }\n    const partSnDiff = this.lastPartSn - previous.lastPartSn;\n    const partIndexDiff = this.lastPartIndex - previous.lastPartIndex;\n    this.updated =\n      this.endSN !== previous.endSN ||\n      !!partIndexDiff ||\n      !!partSnDiff ||\n      !this.live;\n    this.advanced =\n      this.endSN > previous.endSN ||\n      partSnDiff > 0 ||\n      (partSnDiff === 0 && partIndexDiff > 0);\n    if (this.updated || this.advanced) {\n      this.misses = Math.floor(previous.misses * 0.6);\n    } else {\n      this.misses = previous.misses + 1;\n    }\n    this.availabilityDelay = previous.availabilityDelay;\n  }\n\n  get hasProgramDateTime(): boolean {\n    if (this.fragments.length) {\n      return Number.isFinite(\n        this.fragments[this.fragments.length - 1].programDateTime as number,\n      );\n    }\n    return false;\n  }\n\n  get levelTargetDuration(): number {\n    return (\n      this.averagetargetduration ||\n      this.targetduration ||\n      DEFAULT_TARGET_DURATION\n    );\n  }\n\n  get drift(): number {\n    const runTime = this.driftEndTime - this.driftStartTime;\n    if (runTime > 0) {\n      const runDuration = this.driftEnd - this.driftStart;\n      return (runDuration * 1000) / runTime;\n    }\n    return 1;\n  }\n\n  get edge(): number {\n    return this.partEnd || this.fragmentEnd;\n  }\n\n  get partEnd(): number {\n    if (this.partList?.length) {\n      return this.partList[this.partList.length - 1].end;\n    }\n    return this.fragmentEnd;\n  }\n\n  get fragmentEnd(): number {\n    if (this.fragments?.length) {\n      return this.fragments[this.fragments.length - 1].end;\n    }\n    return 0;\n  }\n\n  get age(): number {\n    if (this.advancedDateTime) {\n      return Math.max(Date.now() - this.advancedDateTime, 0) / 1000;\n    }\n    return 0;\n  }\n\n  get lastPartIndex(): number {\n    if (this.partList?.length) {\n      return this.partList[this.partList.length - 1].index;\n    }\n    return -1;\n  }\n\n  get lastPartSn(): number {\n    if (this.partList?.length) {\n      return this.partList[this.partList.length - 1].fragment.sn as number;\n    }\n    return this.endSN;\n  }\n}\n", "export function base64ToBase64Url(base64encodedStr: string): string {\n  return base64encodedStr\n    .replace(/\\+/g, '-')\n    .replace(/\\//g, '_')\n    .replace(/=+$/, '');\n}\n\nexport function strToBase64Encode(str: string): string {\n  return btoa(str);\n}\n\nexport function base64DecodeToStr(str: string): string {\n  return atob(str);\n}\n\nexport function base64Encode(input: Uint8Array): string {\n  return btoa(String.fromCharCode(...input));\n}\n\nexport function base64UrlEncode(input: Uint8Array): string {\n  return base64ToBase64Url(base64Encode(input));\n}\n\nexport function base64Decode(base64encodedStr: string): Uint8Array {\n  return Uint8Array.from(atob(base64encodedStr), (c) => c.charCodeAt(0));\n}\n", "import { base64Decode } from './numeric-encoding-utils';\n\nfunction getKeyIdBytes(str: string): Uint8Array {\n  const keyIdbytes = strToUtf8array(str).subarray(0, 16);\n  const paddedkeyIdbytes = new Uint8Array(16);\n  paddedkeyIdbytes.set(keyIdbytes, 16 - keyIdbytes.length);\n  return paddedkeyIdbytes;\n}\n\nexport function changeEndianness(keyId: Uint8Array) {\n  const swap = function (array: Uint8Array, from: number, to: number) {\n    const cur = array[from];\n    array[from] = array[to];\n    array[to] = cur;\n  };\n\n  swap(keyId, 0, 3);\n  swap(keyId, 1, 2);\n  swap(keyId, 4, 5);\n  swap(keyId, 6, 7);\n}\n\nexport function convertDataUriToArrayBytes(uri: string): Uint8Array | null {\n  // data:[<media type][;attribute=value][;base64],<data>\n  const colonsplit = uri.split(':');\n  let keydata: Uint8Array | null = null;\n  if (colonsplit[0] === 'data' && colonsplit.length === 2) {\n    const semicolonsplit = colonsplit[1].split(';');\n    const commasplit = semicolonsplit[semicolonsplit.length - 1].split(',');\n    if (commasplit.length === 2) {\n      const isbase64 = commasplit[0] === 'base64';\n      const data = commasplit[1];\n      if (isbase64) {\n        semicolonsplit.splice(-1, 1); // remove from processing\n        keydata = base64Decode(data);\n      } else {\n        keydata = getKeyIdBytes(data);\n      }\n    }\n  }\n  return keydata;\n}\n\nexport function strToUtf8array(str: string): Uint8Array {\n  return Uint8Array.from(unescape(encodeURIComponent(str)), (c) =>\n    c.charCodeAt(0),\n  );\n}\n", "/** returns `undefined` is `self` is missing, e.g. in node */\nexport const optionalSelf = typeof self !== 'undefined' ? self : undefined;\n", "import type { DRMSystemOptions, EMEControllerConfig } from '../config';\nimport { optionalSelf } from './global';\n\n/**\n * @see https://developer.mozilla.org/en-US/docs/Web/API/Navigator/requestMediaKeySystemAccess\n */\nexport const enum KeySystems {\n  CLEARKEY = 'org.w3.clearkey',\n  FAIRPLAY = 'com.apple.fps',\n  PLAYREADY = 'com.microsoft.playready',\n  WIDEVINE = 'com.widevine.alpha',\n}\n\n// Playlist #EXT-X-KEY KEYFORMAT values\nexport const enum KeySystemFormats {\n  CLEARKEY = 'org.w3.clearkey',\n  FAIRPLAY = 'com.apple.streamingkeydelivery',\n  PLAYREADY = 'com.microsoft.playready',\n  WIDEVINE = 'urn:uuid:edef8ba9-79d6-4ace-a3c8-27dcd51d21ed',\n}\n\nexport function keySystemFormatToKeySystemDomain(\n  format: KeySystemFormats,\n): KeySystems | undefined {\n  switch (format) {\n    case KeySystemFormats.FAIRPLAY:\n      return KeySystems.FAIRPLAY;\n    case KeySystemFormats.PLAYREADY:\n      return KeySystems.PLAYREADY;\n    case KeySystemFormats.WIDEVINE:\n      return KeySystems.WIDEVINE;\n    case KeySystemFormats.CLEARKEY:\n      return KeySystems.CLEARKEY;\n  }\n}\n\n// System IDs for which we can extract a key ID from \"encrypted\" event PSSH\nexport const enum KeySystemIds {\n  CENC = '1077efecc0b24d02ace33c1e52e2fb4b',\n  CLEARKEY = 'e2719d58a985b3c9781ab030af78d30e',\n  FAIRPLAY = '94ce86fb07ff4f43adb893d2fa968ca2',\n  PLAYREADY = '9a04f07998404286ab92e65be0885f95',\n  WIDEVINE = 'edef8ba979d64acea3c827dcd51d21ed',\n}\n\nexport function keySystemIdToKeySystemDomain(\n  systemId: KeySystemIds,\n): KeySystems | undefined {\n  if (systemId === KeySystemIds.WIDEVINE) {\n    return KeySystems.WIDEVINE;\n  } else if (systemId === KeySystemIds.PLAYREADY) {\n    return KeySystems.PLAYREADY;\n  } else if (\n    systemId === KeySystemIds.CENC ||\n    systemId === KeySystemIds.CLEARKEY\n  ) {\n    return KeySystems.CLEARKEY;\n  }\n}\n\nexport function keySystemDomainToKeySystemFormat(\n  keySystem: KeySystems,\n): KeySystemFormats | undefined {\n  switch (keySystem) {\n    case KeySystems.FAIRPLAY:\n      return KeySystemFormats.FAIRPLAY;\n    case KeySystems.PLAYREADY:\n      return KeySystemFormats.PLAYREADY;\n    case KeySystems.WIDEVINE:\n      return KeySystemFormats.WIDEVINE;\n    case KeySystems.CLEARKEY:\n      return KeySystemFormats.CLEARKEY;\n  }\n}\n\nexport function getKeySystemsForConfig(\n  config: EMEControllerConfig,\n): KeySystems[] {\n  const { drmSystems, widevineLicenseUrl } = config;\n  const keySystemsToAttempt: KeySystems[] = drmSystems\n    ? [\n        KeySystems.FAIRPLAY,\n        KeySystems.WIDEVINE,\n        KeySystems.PLAYREADY,\n        KeySystems.CLEARKEY,\n      ].filter((keySystem) => !!drmSystems[keySystem])\n    : [];\n  if (!keySystemsToAttempt[KeySystems.WIDEVINE] && widevineLicenseUrl) {\n    keySystemsToAttempt.push(KeySystems.WIDEVINE);\n  }\n  return keySystemsToAttempt;\n}\n\nexport type MediaKeyFunc = (\n  keySystem: KeySystems,\n  supportedConfigurations: MediaKeySystemConfiguration[],\n) => Promise<MediaKeySystemAccess>;\n\nexport const requestMediaKeySystemAccess = (function (): MediaKeyFunc | null {\n  if (optionalSelf?.navigator?.requestMediaKeySystemAccess) {\n    return self.navigator.requestMediaKeySystemAccess.bind(self.navigator);\n  } else {\n    return null;\n  }\n})();\n\n/**\n * @see https://developer.mozilla.org/en-US/docs/Web/API/MediaKeySystemConfiguration\n */\nexport function getSupportedMediaKeySystemConfigurations(\n  keySystem: KeySystems,\n  audioCodecs: string[],\n  videoCodecs: string[],\n  drmSystemOptions: DRMSystemOptions,\n): MediaKeySystemConfiguration[] {\n  let initDataTypes: string[];\n  switch (keySystem) {\n    case KeySystems.FAIRPLAY:\n      initDataTypes = ['cenc', 'sinf'];\n      break;\n    case KeySystems.WIDEVINE:\n    case KeySystems.PLAYREADY:\n      initDataTypes = ['cenc'];\n      break;\n    case KeySystems.CLEARKEY:\n      initDataTypes = ['cenc', 'keyids'];\n      break;\n    default:\n      throw new Error(`Unknown key-system: ${keySystem}`);\n  }\n  return createMediaKeySystemConfigurations(\n    initDataTypes,\n    audioCodecs,\n    videoCodecs,\n    drmSystemOptions,\n  );\n}\n\nfunction createMediaKeySystemConfigurations(\n  initDataTypes: string[],\n  audioCodecs: string[],\n  videoCodecs: string[],\n  drmSystemOptions: DRMSystemOptions,\n): MediaKeySystemConfiguration[] {\n  const baseConfig: MediaKeySystemConfiguration = {\n    initDataTypes: initDataTypes,\n    persistentState: drmSystemOptions.persistentState || 'optional',\n    distinctiveIdentifier: drmSystemOptions.distinctiveIdentifier || 'optional',\n    sessionTypes: drmSystemOptions.sessionTypes || [\n      drmSystemOptions.sessionType || 'temporary',\n    ],\n    audioCapabilities: audioCodecs.map((codec) => ({\n      contentType: `audio/mp4; codecs=\"${codec}\"`,\n      robustness: drmSystemOptions.audioRobustness || '',\n      encryptionScheme: drmSystemOptions.audioEncryptionScheme || null,\n    })),\n    videoCapabilities: videoCodecs.map((codec) => ({\n      contentType: `video/mp4; codecs=\"${codec}\"`,\n      robustness: drmSystemOptions.videoRobustness || '',\n      encryptionScheme: drmSystemOptions.videoEncryptionScheme || null,\n    })),\n  };\n\n  return [baseConfig];\n}\n", "export function sliceUint8(\n  array: Uint8Array,\n  start?: number,\n  end?: number,\n): Uint8Array {\n  // @ts-expect-error This polyfills IE11 usage of Uint8Array slice.\n  // It always exists in the TypeScript definition so fails, but it fails at runtime on IE11.\n  return Uint8Array.prototype.slice\n    ? array.slice(start, end)\n    : new Uint8Array(Array.prototype.slice.call(array, start, end));\n}\n", "type RawFrame = { type: string; size: number; data: Uint8Array };\n\n// breaking up those two types in order to clarify what is happening in the decoding path.\ntype DecodedFrame<T> = { key: string; data: T; info?: any };\nexport type Frame = DecodedFrame<ArrayBuffer | string>;\n\n/**\n * Returns true if an ID3 header can be found at offset in data\n * @param data - The data to search\n * @param offset - The offset at which to start searching\n */\nexport const isHeader = (data: Uint8Array, offset: number): boolean => {\n  /*\n   * http://id3.org/id3v2.3.0\n   * [0]     = 'I'\n   * [1]     = 'D'\n   * [2]     = '3'\n   * [3,4]   = {Version}\n   * [5]     = {Flags}\n   * [6-9]   = {ID3 Size}\n   *\n   * An ID3v2 tag can be detected with the following pattern:\n   *  $49 44 33 yy yy xx zz zz zz zz\n   * Where yy is less than $FF, xx is the 'flags' byte and zz is less than $80\n   */\n  if (offset + 10 <= data.length) {\n    // look for 'ID3' identifier\n    if (\n      data[offset] === 0x49 &&\n      data[offset + 1] === 0x44 &&\n      data[offset + 2] === 0x33\n    ) {\n      // check version is within range\n      if (data[offset + 3] < 0xff && data[offset + 4] < 0xff) {\n        // check size is within range\n        if (\n          data[offset + 6] < 0x80 &&\n          data[offset + 7] < 0x80 &&\n          data[offset + 8] < 0x80 &&\n          data[offset + 9] < 0x80\n        ) {\n          return true;\n        }\n      }\n    }\n  }\n\n  return false;\n};\n\n/**\n * Returns true if an ID3 footer can be found at offset in data\n * @param data - The data to search\n * @param offset - The offset at which to start searching\n */\nexport const isFooter = (data: Uint8Array, offset: number): boolean => {\n  /*\n   * The footer is a copy of the header, but with a different identifier\n   */\n  if (offset + 10 <= data.length) {\n    // look for '3DI' identifier\n    if (\n      data[offset] === 0x33 &&\n      data[offset + 1] === 0x44 &&\n      data[offset + 2] === 0x49\n    ) {\n      // check version is within range\n      if (data[offset + 3] < 0xff && data[offset + 4] < 0xff) {\n        // check size is within range\n        if (\n          data[offset + 6] < 0x80 &&\n          data[offset + 7] < 0x80 &&\n          data[offset + 8] < 0x80 &&\n          data[offset + 9] < 0x80\n        ) {\n          return true;\n        }\n      }\n    }\n  }\n\n  return false;\n};\n\n/**\n * Returns any adjacent ID3 tags found in data starting at offset, as one block of data\n * @param data - The data to search in\n * @param offset - The offset at which to start searching\n * @returns the block of data containing any ID3 tags found\n * or *undefined* if no header is found at the starting offset\n */\nexport const getID3Data = (\n  data: Uint8Array,\n  offset: number,\n): Uint8Array | undefined => {\n  const front = offset;\n  let length = 0;\n\n  while (isHeader(data, offset)) {\n    // ID3 header is 10 bytes\n    length += 10;\n\n    const size = readSize(data, offset + 6);\n    length += size;\n\n    if (isFooter(data, offset + 10)) {\n      // ID3 footer is 10 bytes\n      length += 10;\n    }\n\n    offset += length;\n  }\n\n  if (length > 0) {\n    return data.subarray(front, front + length);\n  }\n\n  return undefined;\n};\n\nconst readSize = (data: Uint8Array, offset: number): number => {\n  let size = 0;\n  size = (data[offset] & 0x7f) << 21;\n  size |= (data[offset + 1] & 0x7f) << 14;\n  size |= (data[offset + 2] & 0x7f) << 7;\n  size |= data[offset + 3] & 0x7f;\n  return size;\n};\n\nexport const canParse = (data: Uint8Array, offset: number): boolean => {\n  return (\n    isHeader(data, offset) &&\n    readSize(data, offset + 6) + 10 <= data.length - offset\n  );\n};\n\n/**\n * Searches for the Elementary Stream timestamp found in the ID3 data chunk\n * @param data - Block of data containing one or more ID3 tags\n */\nexport const getTimeStamp = (data: Uint8Array): number | undefined => {\n  const frames: Frame[] = getID3Frames(data);\n\n  for (let i = 0; i < frames.length; i++) {\n    const frame = frames[i];\n\n    if (isTimeStampFrame(frame)) {\n      return readTimeStamp(frame as DecodedFrame<ArrayBuffer>);\n    }\n  }\n\n  return undefined;\n};\n\n/**\n * Returns true if the ID3 frame is an Elementary Stream timestamp frame\n */\nexport const isTimeStampFrame = (frame: Frame): boolean => {\n  return (\n    frame &&\n    frame.key === 'PRIV' &&\n    frame.info === 'com.apple.streaming.transportStreamTimestamp'\n  );\n};\n\nconst getFrameData = (data: Uint8Array): RawFrame => {\n  /*\n  Frame ID       $xx xx xx xx (four characters)\n  Size           $xx xx xx xx\n  Flags          $xx xx\n  */\n  const type: string = String.fromCharCode(data[0], data[1], data[2], data[3]);\n  const size: number = readSize(data, 4);\n\n  // skip frame id, size, and flags\n  const offset = 10;\n\n  return { type, size, data: data.subarray(offset, offset + size) };\n};\n\n/**\n * Returns an array of ID3 frames found in all the ID3 tags in the id3Data\n * @param id3Data - The ID3 data containing one or more ID3 tags\n */\nexport const getID3Frames = (id3Data: Uint8Array): Frame[] => {\n  let offset = 0;\n  const frames: Frame[] = [];\n\n  while (isHeader(id3Data, offset)) {\n    const size = readSize(id3Data, offset + 6);\n    // skip past ID3 header\n    offset += 10;\n    const end = offset + size;\n    // loop through frames in the ID3 tag\n    while (offset + 8 < end) {\n      const frameData: RawFrame = getFrameData(id3Data.subarray(offset));\n      const frame: Frame | undefined = decodeFrame(frameData);\n      if (frame) {\n        frames.push(frame);\n      }\n\n      // skip frame header and frame data\n      offset += frameData.size + 10;\n    }\n\n    if (isFooter(id3Data, offset)) {\n      offset += 10;\n    }\n  }\n\n  return frames;\n};\n\nexport const decodeFrame = (frame: RawFrame): Frame | undefined => {\n  if (frame.type === 'PRIV') {\n    return decodePrivFrame(frame);\n  } else if (frame.type[0] === 'W') {\n    return decodeURLFrame(frame);\n  }\n\n  return decodeTextFrame(frame);\n};\n\nconst decodePrivFrame = (\n  frame: RawFrame,\n): DecodedFrame<ArrayBuffer> | undefined => {\n  /*\n  Format: <text string>\\0<binary data>\n  */\n  if (frame.size < 2) {\n    return undefined;\n  }\n\n  const owner = utf8ArrayToStr(frame.data, true);\n  const privateData = new Uint8Array(frame.data.subarray(owner.length + 1));\n\n  return { key: frame.type, info: owner, data: privateData.buffer };\n};\n\nconst decodeTextFrame = (frame: RawFrame): DecodedFrame<string> | undefined => {\n  if (frame.size < 2) {\n    return undefined;\n  }\n\n  if (frame.type === 'TXXX') {\n    /*\n    Format:\n    [0]   = {Text Encoding}\n    [1-?] = {Description}\\0{Value}\n    */\n    let index = 1;\n    const description = utf8ArrayToStr(frame.data.subarray(index), true);\n\n    index += description.length + 1;\n    const value = utf8ArrayToStr(frame.data.subarray(index));\n\n    return { key: frame.type, info: description, data: value };\n  }\n  /*\n  Format:\n  [0]   = {Text Encoding}\n  [1-?] = {Value}\n  */\n  const text = utf8ArrayToStr(frame.data.subarray(1));\n  return { key: frame.type, data: text };\n};\n\nconst decodeURLFrame = (frame: RawFrame): DecodedFrame<string> | undefined => {\n  if (frame.type === 'WXXX') {\n    /*\n    Format:\n    [0]   = {Text Encoding}\n    [1-?] = {Description}\\0{URL}\n    */\n    if (frame.size < 2) {\n      return undefined;\n    }\n\n    let index = 1;\n    const description: string = utf8ArrayToStr(\n      frame.data.subarray(index),\n      true,\n    );\n\n    index += description.length + 1;\n    const value: string = utf8ArrayToStr(frame.data.subarray(index));\n\n    return { key: frame.type, info: description, data: value };\n  }\n  /*\n  Format:\n  [0-?] = {URL}\n  */\n  const url: string = utf8ArrayToStr(frame.data);\n  return { key: frame.type, data: url };\n};\n\nconst readTimeStamp = (\n  timeStampFrame: DecodedFrame<ArrayBuffer>,\n): number | undefined => {\n  if (timeStampFrame.data.byteLength === 8) {\n    const data = new Uint8Array(timeStampFrame.data);\n    // timestamp is 33 bit expressed as a big-endian eight-octet number,\n    // with the upper 31 bits set to zero.\n    const pts33Bit = data[3] & 0x1;\n    let timestamp =\n      (data[4] << 23) + (data[5] << 15) + (data[6] << 7) + data[7];\n    timestamp /= 45;\n\n    if (pts33Bit) {\n      timestamp += 47721858.84;\n    } // 2^32 / 90\n\n    return Math.round(timestamp);\n  }\n\n  return undefined;\n};\n\n// http://stackoverflow.com/questions/8936984/uint8array-to-string-in-javascript/22373197\n// http://www.onicos.com/staff/iz/amuse/javascript/expert/utf.txt\n/* utf.js - UTF-8 <=> UTF-16 convertion\n *\n * Copyright (C) 1999 Masanao Izumo <iz@onicos.co.jp>\n * Version: 1.0\n * LastModified: Dec 25 1999\n * This library is free.  You can redistribute it and/or modify it.\n */\nexport const utf8ArrayToStr = (\n  array: Uint8Array,\n  exitOnNull: boolean = false,\n): string => {\n  const decoder = getTextDecoder();\n  if (decoder) {\n    const decoded = decoder.decode(array);\n\n    if (exitOnNull) {\n      // grab up to the first null\n      const idx = decoded.indexOf('\\0');\n      return idx !== -1 ? decoded.substring(0, idx) : decoded;\n    }\n\n    // remove any null characters\n    return decoded.replace(/\\0/g, '');\n  }\n\n  const len = array.length;\n  let c;\n  let char2;\n  let char3;\n  let out = '';\n  let i = 0;\n  while (i < len) {\n    c = array[i++];\n    if (c === 0x00 && exitOnNull) {\n      return out;\n    } else if (c === 0x00 || c === 0x03) {\n      // If the character is 3 (END_OF_TEXT) or 0 (NULL) then skip it\n      continue;\n    }\n    switch (c >> 4) {\n      case 0:\n      case 1:\n      case 2:\n      case 3:\n      case 4:\n      case 5:\n      case 6:\n      case 7:\n        // 0xxxxxxx\n        out += String.fromCharCode(c);\n        break;\n      case 12:\n      case 13:\n        // 110x xxxx   10xx xxxx\n        char2 = array[i++];\n        out += String.fromCharCode(((c & 0x1f) << 6) | (char2 & 0x3f));\n        break;\n      case 14:\n        // 1110 xxxx  10xx xxxx  10xx xxxx\n        char2 = array[i++];\n        char3 = array[i++];\n        out += String.fromCharCode(\n          ((c & 0x0f) << 12) | ((char2 & 0x3f) << 6) | ((char3 & 0x3f) << 0),\n        );\n        break;\n      default:\n    }\n  }\n  return out;\n};\n\nexport const testables = {\n  decodeTextFrame: decodeTextFrame,\n};\n\nlet decoder: TextDecoder;\n\nfunction getTextDecoder() {\n  // On Play Station 4, TextDecoder is defined but partially implemented.\n  // Manual decoding option is preferable\n  if (navigator.userAgent.includes('PlayStation 4')) {\n    return;\n  }\n\n  if (!decoder && typeof self.TextDecoder !== 'undefined') {\n    decoder = new self.TextDecoder('utf-8');\n  }\n\n  return decoder;\n}\n", "/**\n *  hex dump helper class\n */\n\nconst Hex = {\n  hexDump: function (array: Uint8Array) {\n    let str = '';\n    for (let i = 0; i < array.length; i++) {\n      let h = array[i].toString(16);\n      if (h.length < 2) {\n        h = '0' + h;\n      }\n\n      str += h;\n    }\n    return str;\n  },\n};\n\nexport default Hex;\n", "import { ElementaryStreamTypes } from '../loader/fragment';\nimport { sliceUint8 } from './typed-array';\nimport { utf8ArrayToStr } from '../demux/id3';\nimport { logger } from '../utils/logger';\nimport Hex from './hex';\nimport type { KeySystemIds } from './mediakeys-helper';\nimport type { PassthroughTrack, UserdataSample } from '../types/demuxer';\nimport type { DecryptData } from '../loader/level-key';\n\nconst UINT32_MAX = Math.pow(2, 32) - 1;\nconst push = [].push;\n\n// We are using fixed track IDs for driving the MP4 remuxer\n// instead of following the TS PIDs.\n// There is no reason not to do this and some browsers/SourceBuffer-demuxers\n// may not like if there are TrackID \"switches\"\n// See https://github.com/video-dev/hls.js/issues/1331\n// Here we are mapping our internal track types to constant MP4 track IDs\n// With MSE currently one can only have one track of each, and we are muxing\n// whatever video/audio rendition in them.\nexport const RemuxerTrackIdConfig = {\n  video: 1,\n  audio: 2,\n  id3: 3,\n  text: 4,\n};\n\nexport function bin2str(data: Uint8Array): string {\n  return String.fromCharCode.apply(null, data);\n}\n\nexport function readUint16(buffer: Uint8Array, offset: number): number {\n  const val = (buffer[offset] << 8) | buffer[offset + 1];\n  return val < 0 ? 65536 + val : val;\n}\n\nexport function readUint32(buffer: Uint8Array, offset: number): number {\n  const val = readSint32(buffer, offset);\n  return val < 0 ? 4294967296 + val : val;\n}\n\nexport function readUint64(buffer: Uint8Array, offset: number) {\n  let result = readUint32(buffer, offset);\n  result *= Math.pow(2, 32);\n  result += readUint32(buffer, offset + 4);\n  return result;\n}\n\nexport function readSint32(buffer: Uint8Array, offset: number): number {\n  return (\n    (buffer[offset] << 24) |\n    (buffer[offset + 1] << 16) |\n    (buffer[offset + 2] << 8) |\n    buffer[offset + 3]\n  );\n}\n\nexport function writeUint32(buffer: Uint8Array, offset: number, value: number) {\n  buffer[offset] = value >> 24;\n  buffer[offset + 1] = (value >> 16) & 0xff;\n  buffer[offset + 2] = (value >> 8) & 0xff;\n  buffer[offset + 3] = value & 0xff;\n}\n\n// Find \"moof\" box\nexport function hasMoofData(data: Uint8Array): boolean {\n  const end = data.byteLength;\n  for (let i = 0; i < end; ) {\n    const size = readUint32(data, i);\n    if (\n      size > 8 &&\n      data[i + 4] === 0x6d &&\n      data[i + 5] === 0x6f &&\n      data[i + 6] === 0x6f &&\n      data[i + 7] === 0x66\n    ) {\n      return true;\n    }\n    i = size > 1 ? i + size : end;\n  }\n  return false;\n}\n\n// Find the data for a box specified by its path\nexport function findBox(data: Uint8Array, path: string[]): Uint8Array[] {\n  const results = [] as Uint8Array[];\n  if (!path.length) {\n    // short-circuit the search for empty paths\n    return results;\n  }\n  const end = data.byteLength;\n\n  for (let i = 0; i < end; ) {\n    const size = readUint32(data, i);\n    const type = bin2str(data.subarray(i + 4, i + 8));\n    const endbox = size > 1 ? i + size : end;\n    if (type === path[0]) {\n      if (path.length === 1) {\n        // this is the end of the path and we've found the box we were\n        // looking for\n        results.push(data.subarray(i + 8, endbox));\n      } else {\n        // recursively search for the next box along the path\n        const subresults = findBox(data.subarray(i + 8, endbox), path.slice(1));\n        if (subresults.length) {\n          push.apply(results, subresults);\n        }\n      }\n    }\n    i = endbox;\n  }\n\n  // we've finished searching all of data\n  return results;\n}\n\ntype SidxInfo = {\n  earliestPresentationTime: number;\n  timescale: number;\n  version: number;\n  referencesCount: number;\n  references: any[];\n};\n\nexport function parseSegmentIndex(sidx: Uint8Array): SidxInfo | null {\n  const references: any[] = [];\n\n  const version = sidx[0];\n\n  // set initial offset, we skip the reference ID (not needed)\n  let index = 8;\n\n  const timescale = readUint32(sidx, index);\n  index += 4;\n\n  let earliestPresentationTime = 0;\n  let firstOffset = 0;\n\n  if (version === 0) {\n    earliestPresentationTime = readUint32(sidx, index);\n    firstOffset = readUint32(sidx, index + 4);\n    index += 8;\n  } else {\n    earliestPresentationTime = readUint64(sidx, index);\n    firstOffset = readUint64(sidx, index + 8);\n    index += 16;\n  }\n\n  // skip reserved\n  index += 2;\n\n  let startByte = sidx.length + firstOffset;\n\n  const referencesCount = readUint16(sidx, index);\n  index += 2;\n\n  for (let i = 0; i < referencesCount; i++) {\n    let referenceIndex = index;\n\n    const referenceInfo = readUint32(sidx, referenceIndex);\n    referenceIndex += 4;\n\n    const referenceSize = referenceInfo & 0x7fffffff;\n    const referenceType = (referenceInfo & 0x80000000) >>> 31;\n\n    if (referenceType === 1) {\n      logger.warn('SIDX has hierarchical references (not supported)');\n      return null;\n    }\n\n    const subsegmentDuration = readUint32(sidx, referenceIndex);\n    referenceIndex += 4;\n\n    references.push({\n      referenceSize,\n      subsegmentDuration, // unscaled\n      info: {\n        duration: subsegmentDuration / timescale,\n        start: startByte,\n        end: startByte + referenceSize - 1,\n      },\n    });\n\n    startByte += referenceSize;\n\n    // Skipping 1 bit for |startsWithSap|, 3 bits for |sapType|, and 28 bits\n    // for |sapDelta|.\n    referenceIndex += 4;\n\n    // skip to next ref\n    index = referenceIndex;\n  }\n\n  return {\n    earliestPresentationTime,\n    timescale,\n    version,\n    referencesCount,\n    references,\n  };\n}\n\n/**\n * Parses an MP4 initialization segment and extracts stream type and\n * timescale values for any declared tracks. Timescale values indicate the\n * number of clock ticks per second to assume for time-based values\n * elsewhere in the MP4.\n *\n * To determine the start time of an MP4, you need two pieces of\n * information: the timescale unit and the earliest base media decode\n * time. Multiple timescales can be specified within an MP4 but the\n * base media decode time is always expressed in the timescale from\n * the media header box for the track:\n * ```\n * moov > trak > mdia > mdhd.timescale\n * moov > trak > mdia > hdlr\n * ```\n * @param initSegment the bytes of the init segment\n * @returns a hash of track type to timescale values or null if\n * the init segment is malformed.\n */\n\nexport interface InitDataTrack {\n  timescale: number;\n  id: number;\n  codec: string;\n}\n\ntype HdlrType = ElementaryStreamTypes.AUDIO | ElementaryStreamTypes.VIDEO;\n\nexport interface InitData extends Array<any> {\n  [index: number]:\n    | {\n        timescale: number;\n        type: HdlrType;\n        default?: {\n          duration: number;\n          flags: number;\n        };\n      }\n    | undefined;\n  audio?: InitDataTrack;\n  video?: InitDataTrack;\n  caption?: InitDataTrack;\n}\n\nexport function parseInitSegment(initSegment: Uint8Array): InitData {\n  const result: InitData = [];\n  const traks = findBox(initSegment, ['moov', 'trak']);\n  for (let i = 0; i < traks.length; i++) {\n    const trak = traks[i];\n    const tkhd = findBox(trak, ['tkhd'])[0];\n    if (tkhd) {\n      let version = tkhd[0];\n      const trackId = readUint32(tkhd, version === 0 ? 12 : 20);\n      const mdhd = findBox(trak, ['mdia', 'mdhd'])[0];\n      if (mdhd) {\n        version = mdhd[0];\n        const timescale = readUint32(mdhd, version === 0 ? 12 : 20);\n        const hdlr = findBox(trak, ['mdia', 'hdlr'])[0];\n        if (hdlr) {\n          const hdlrType = bin2str(hdlr.subarray(8, 12));\n          const type: HdlrType | undefined = {\n            soun: ElementaryStreamTypes.AUDIO as const,\n            vide: ElementaryStreamTypes.VIDEO as const,\n          }[hdlrType];\n          if (type) {\n            // Parse codec details\n            const stsd = findBox(trak, ['mdia', 'minf', 'stbl', 'stsd'])[0];\n            const stsdData = parseStsd(stsd);\n            result[trackId] = { timescale, type };\n            result[type] = { timescale, id: trackId, ...stsdData };\n          }\n        }\n      }\n    }\n  }\n\n  const trex = findBox(initSegment, ['moov', 'mvex', 'trex']);\n  trex.forEach((trex) => {\n    const trackId = readUint32(trex, 4);\n    const track = result[trackId];\n    if (track) {\n      track.default = {\n        duration: readUint32(trex, 12),\n        flags: readUint32(trex, 20),\n      };\n    }\n  });\n\n  return result;\n}\n\nfunction parseStsd(stsd: Uint8Array): { codec: string; encrypted: boolean } {\n  const sampleEntries = stsd.subarray(8);\n  const sampleEntriesEnd = sampleEntries.subarray(8 + 78);\n  const fourCC = bin2str(sampleEntries.subarray(4, 8));\n  let codec = fourCC;\n  const encrypted = fourCC === 'enca' || fourCC === 'encv';\n  if (encrypted) {\n    const encBox = findBox(sampleEntries, [fourCC])[0];\n    const encBoxChildren = encBox.subarray(fourCC === 'enca' ? 28 : 78);\n    const sinfs = findBox(encBoxChildren, ['sinf']);\n    sinfs.forEach((sinf) => {\n      const schm = findBox(sinf, ['schm'])[0];\n      if (schm) {\n        const scheme = bin2str(schm.subarray(4, 8));\n        if (scheme === 'cbcs' || scheme === 'cenc') {\n          const frma = findBox(sinf, ['frma'])[0];\n          if (frma) {\n            // for encrypted content codec fourCC will be in frma\n            codec = bin2str(frma);\n          }\n        }\n      }\n    });\n  }\n  switch (codec) {\n    case 'avc1':\n    case 'avc2':\n    case 'avc3':\n    case 'avc4': {\n      // extract profile + compatibility + level out of avcC box\n      const avcCBox = findBox(sampleEntriesEnd, ['avcC'])[0];\n      codec += '.' + toHex(avcCBox[1]) + toHex(avcCBox[2]) + toHex(avcCBox[3]);\n      break;\n    }\n    case 'mp4a': {\n      const codecBox = findBox(sampleEntries, [fourCC])[0];\n      const esdsBox = findBox(codecBox.subarray(28), ['esds'])[0];\n      if (esdsBox && esdsBox.length > 12) {\n        let i = 4;\n        // ES Descriptor tag\n        if (esdsBox[i++] !== 0x03) {\n          break;\n        }\n        i = skipBERInteger(esdsBox, i);\n        i += 2; // skip es_id;\n        const flags = esdsBox[i++];\n        if (flags & 0x80) {\n          i += 2; // skip dependency es_id\n        }\n        if (flags & 0x40) {\n          i += esdsBox[i++]; // skip URL\n        }\n        // Decoder config descriptor\n        if (esdsBox[i++] !== 0x04) {\n          break;\n        }\n        i = skipBERInteger(esdsBox, i);\n        const objectType = esdsBox[i++];\n        if (objectType === 0x40) {\n          codec += '.' + toHex(objectType);\n        } else {\n          break;\n        }\n        i += 12;\n        // Decoder specific info\n        if (esdsBox[i++] !== 0x05) {\n          break;\n        }\n        i = skipBERInteger(esdsBox, i);\n        const firstByte = esdsBox[i++];\n        let audioObjectType = (firstByte & 0xf8) >> 3;\n        if (audioObjectType === 31) {\n          audioObjectType +=\n            1 + ((firstByte & 0x7) << 3) + ((esdsBox[i] & 0xe0) >> 5);\n        }\n        codec += '.' + audioObjectType;\n      }\n      break;\n    }\n    case 'hvc1':\n    case 'hev1': {\n      const hvcCBox = findBox(sampleEntriesEnd, ['hvcC'])[0];\n      const profileByte = hvcCBox[1];\n      const profileSpace = ['', 'A', 'B', 'C'][profileByte >> 6];\n      const generalProfileIdc = profileByte & 0x1f;\n      const profileCompat = readUint32(hvcCBox, 2);\n      const tierFlag = (profileByte & 0x20) >> 5 ? 'H' : 'L';\n      const levelIDC = hvcCBox[12];\n      const constraintIndicator = hvcCBox.subarray(6, 12);\n      codec += '.' + profileSpace + generalProfileIdc;\n      codec += '.' + profileCompat.toString(16).toUpperCase();\n      codec += '.' + tierFlag + levelIDC;\n      let constraintString = '';\n      for (let i = constraintIndicator.length; i--; ) {\n        const byte = constraintIndicator[i];\n        if (byte || constraintString) {\n          const encodedByte = byte.toString(16).toUpperCase();\n          constraintString = '.' + encodedByte + constraintString;\n        }\n      }\n      codec += constraintString;\n      break;\n    }\n    case 'dvh1':\n    case 'dvhe': {\n      const dvcCBox = findBox(sampleEntriesEnd, ['dvcC'])[0];\n      const profile = (dvcCBox[2] >> 1) & 0x7f;\n      const level = ((dvcCBox[2] << 5) & 0x20) | ((dvcCBox[3] >> 3) & 0x1f);\n      codec += '.' + addLeadingZero(profile) + '.' + addLeadingZero(level);\n      break;\n    }\n    case 'vp09': {\n      const vpcCBox = findBox(sampleEntriesEnd, ['vpcC'])[0];\n      const profile = vpcCBox[4];\n      const level = vpcCBox[5];\n      const bitDepth = (vpcCBox[6] >> 4) & 0x0f;\n      codec +=\n        '.' +\n        addLeadingZero(profile) +\n        '.' +\n        addLeadingZero(level) +\n        '.' +\n        addLeadingZero(bitDepth);\n      break;\n    }\n    case 'av01': {\n      const av1CBox = findBox(sampleEntriesEnd, ['av1C'])[0];\n      const profile = av1CBox[1] >>> 5;\n      const level = av1CBox[1] & 0x1f;\n      const tierFlag = av1CBox[2] >>> 7 ? 'H' : 'M';\n      const highBitDepth = (av1CBox[2] & 0x40) >> 6;\n      const twelveBit = (av1CBox[2] & 0x20) >> 5;\n      const bitDepth =\n        profile === 2 && highBitDepth\n          ? twelveBit\n            ? 12\n            : 10\n          : highBitDepth\n            ? 10\n            : 8;\n      const monochrome = (av1CBox[2] & 0x10) >> 4;\n      const chromaSubsamplingX = (av1CBox[2] & 0x08) >> 3;\n      const chromaSubsamplingY = (av1CBox[2] & 0x04) >> 2;\n      const chromaSamplePosition = av1CBox[2] & 0x03;\n      // TODO: parse color_description_present_flag\n      // default it to BT.709/limited range for now\n      // more info https://aomediacodec.github.io/av1-isobmff/#av1codecconfigurationbox-syntax\n      const colorPrimaries = 1;\n      const transferCharacteristics = 1;\n      const matrixCoefficients = 1;\n      const videoFullRangeFlag = 0;\n      codec +=\n        '.' +\n        profile +\n        '.' +\n        addLeadingZero(level) +\n        tierFlag +\n        '.' +\n        addLeadingZero(bitDepth) +\n        '.' +\n        monochrome +\n        '.' +\n        chromaSubsamplingX +\n        chromaSubsamplingY +\n        chromaSamplePosition +\n        '.' +\n        addLeadingZero(colorPrimaries) +\n        '.' +\n        addLeadingZero(transferCharacteristics) +\n        '.' +\n        addLeadingZero(matrixCoefficients) +\n        '.' +\n        videoFullRangeFlag;\n      break;\n    }\n    case 'ac-3':\n    case 'ec-3':\n    case 'alac':\n    case 'fLaC':\n    case 'Opus':\n    default:\n      break;\n  }\n  return { codec, encrypted };\n}\n\nfunction skipBERInteger(bytes: Uint8Array, i: number): number {\n  const limit = i + 5;\n  while (bytes[i++] & 0x80 && i < limit) {}\n  return i;\n}\n\nfunction toHex(x: number): string {\n  return ('0' + x.toString(16).toUpperCase()).slice(-2);\n}\n\nfunction addLeadingZero(num: number): string {\n  return (num < 10 ? '0' : '') + num;\n}\n\nexport function patchEncyptionData(\n  initSegment: Uint8Array | undefined,\n  decryptdata: DecryptData | null,\n): Uint8Array | undefined {\n  if (!initSegment || !decryptdata) {\n    return initSegment;\n  }\n  const keyId = decryptdata.keyId;\n  if (keyId && decryptdata.isCommonEncryption) {\n    const traks = findBox(initSegment, ['moov', 'trak']);\n    traks.forEach((trak) => {\n      const stsd = findBox(trak, ['mdia', 'minf', 'stbl', 'stsd'])[0];\n\n      // skip the sample entry count\n      const sampleEntries = stsd.subarray(8);\n      let encBoxes = findBox(sampleEntries, ['enca']);\n      const isAudio = encBoxes.length > 0;\n      if (!isAudio) {\n        encBoxes = findBox(sampleEntries, ['encv']);\n      }\n      encBoxes.forEach((enc) => {\n        const encBoxChildren = isAudio ? enc.subarray(28) : enc.subarray(78);\n        const sinfBoxes = findBox(encBoxChildren, ['sinf']);\n        sinfBoxes.forEach((sinf) => {\n          const tenc = parseSinf(sinf);\n          if (tenc) {\n            // Look for default key id (keyID offset is always 8 within the tenc box):\n            const tencKeyId = tenc.subarray(8, 24);\n            if (!tencKeyId.some((b) => b !== 0)) {\n              logger.log(\n                `[eme] Patching keyId in 'enc${\n                  isAudio ? 'a' : 'v'\n                }>sinf>>tenc' box: ${Hex.hexDump(tencKeyId)} -> ${Hex.hexDump(\n                  keyId,\n                )}`,\n              );\n              tenc.set(keyId, 8);\n            }\n          }\n        });\n      });\n    });\n  }\n\n  return initSegment;\n}\n\nexport function parseSinf(sinf: Uint8Array): Uint8Array | null {\n  const schm = findBox(sinf, ['schm'])[0];\n  if (schm) {\n    const scheme = bin2str(schm.subarray(4, 8));\n    if (scheme === 'cbcs' || scheme === 'cenc') {\n      return findBox(sinf, ['schi', 'tenc'])[0];\n    }\n  }\n  return null;\n}\n\n/**\n * Determine the base media decode start time, in seconds, for an MP4\n * fragment. If multiple fragments are specified, the earliest time is\n * returned.\n *\n * The base media decode time can be parsed from track fragment\n * metadata:\n * ```\n * moof > traf > tfdt.baseMediaDecodeTime\n * ```\n * It requires the timescale value from the mdhd to interpret.\n *\n * @param initData - a hash of track type to timescale values\n * @param fmp4 - the bytes of the mp4 fragment\n * @returns the earliest base media decode start time for the\n * fragment, in seconds\n */\nexport function getStartDTS(\n  initData: InitData,\n  fmp4: Uint8Array,\n): number | null {\n  // we need info from two children of each track fragment box\n  return findBox(fmp4, ['moof', 'traf']).reduce(\n    (result: number | null, traf) => {\n      const tfdt = findBox(traf, ['tfdt'])[0];\n      const version = tfdt[0];\n      const start = findBox(traf, ['tfhd']).reduce(\n        (result: number | null, tfhd) => {\n          // get the track id from the tfhd\n          const id = readUint32(tfhd, 4);\n          const track = initData[id];\n          if (track) {\n            let baseTime = readUint32(tfdt, 4);\n            if (version === 1) {\n              // If value is too large, assume signed 64-bit. Negative track fragment decode times are invalid, but they exist in the wild.\n              // This prevents large values from being used for initPTS, which can cause playlist sync issues.\n              // https://github.com/video-dev/hls.js/issues/5303\n              if (baseTime === UINT32_MAX) {\n                logger.warn(\n                  `[mp4-demuxer]: Ignoring assumed invalid signed 64-bit track fragment decode time`,\n                );\n                return result;\n              }\n              baseTime *= UINT32_MAX + 1;\n              baseTime += readUint32(tfdt, 8);\n            }\n            // assume a 90kHz clock if no timescale was specified\n            const scale = track.timescale || 90e3;\n            // convert base time to seconds\n            const startTime = baseTime / scale;\n            if (\n              Number.isFinite(startTime) &&\n              (result === null || startTime < result)\n            ) {\n              return startTime;\n            }\n          }\n          return result;\n        },\n        null,\n      );\n      if (\n        start !== null &&\n        Number.isFinite(start) &&\n        (result === null || start < result)\n      ) {\n        return start;\n      }\n      return result;\n    },\n    null,\n  );\n}\n\n/*\n  For Reference:\n  aligned(8) class TrackFragmentHeaderBox\n           extends FullBox(tfhd, 0, tf_flags){\n     unsigned int(32)  track_ID;\n     // all the following are optional fields\n     unsigned int(64)  base_data_offset;\n     unsigned int(32)  sample_description_index;\n     unsigned int(32)  default_sample_duration;\n     unsigned int(32)  default_sample_size;\n     unsigned int(32)  default_sample_flags\n  }\n */\nexport function getDuration(data: Uint8Array, initData: InitData) {\n  let rawDuration = 0;\n  let videoDuration = 0;\n  let audioDuration = 0;\n  const trafs = findBox(data, ['moof', 'traf']);\n  for (let i = 0; i < trafs.length; i++) {\n    const traf = trafs[i];\n    // There is only one tfhd & trun per traf\n    // This is true for CMAF style content, and we should perhaps check the ftyp\n    // and only look for a single trun then, but for ISOBMFF we should check\n    // for multiple track runs.\n    const tfhd = findBox(traf, ['tfhd'])[0];\n    // get the track id from the tfhd\n    const id = readUint32(tfhd, 4);\n    const track = initData[id];\n    if (!track) {\n      continue;\n    }\n    const trackDefault = track.default;\n    const tfhdFlags = readUint32(tfhd, 0) | trackDefault?.flags!;\n    let sampleDuration: number | undefined = trackDefault?.duration;\n    if (tfhdFlags & 0x000008) {\n      // 0x000008 indicates the presence of the default_sample_duration field\n      if (tfhdFlags & 0x000002) {\n        // 0x000002 indicates the presence of the sample_description_index field, which precedes default_sample_duration\n        // If present, the default_sample_duration exists at byte offset 12\n        sampleDuration = readUint32(tfhd, 12);\n      } else {\n        // Otherwise, the duration is at byte offset 8\n        sampleDuration = readUint32(tfhd, 8);\n      }\n    }\n    // assume a 90kHz clock if no timescale was specified\n    const timescale = track.timescale || 90e3;\n    const truns = findBox(traf, ['trun']);\n    for (let j = 0; j < truns.length; j++) {\n      rawDuration = computeRawDurationFromSamples(truns[j]);\n      if (!rawDuration && sampleDuration) {\n        const sampleCount = readUint32(truns[j], 4);\n        rawDuration = sampleDuration * sampleCount;\n      }\n      if (track.type === ElementaryStreamTypes.VIDEO) {\n        videoDuration += rawDuration / timescale;\n      } else if (track.type === ElementaryStreamTypes.AUDIO) {\n        audioDuration += rawDuration / timescale;\n      }\n    }\n  }\n  if (videoDuration === 0 && audioDuration === 0) {\n    // If duration samples are not available in the traf use sidx subsegment_duration\n    let sidxMinStart = Infinity;\n    let sidxMaxEnd = 0;\n    let sidxDuration = 0;\n    const sidxs = findBox(data, ['sidx']);\n    for (let i = 0; i < sidxs.length; i++) {\n      const sidx = parseSegmentIndex(sidxs[i]);\n      if (sidx?.references) {\n        sidxMinStart = Math.min(\n          sidxMinStart,\n          sidx.earliestPresentationTime / sidx.timescale,\n        );\n        const subSegmentDuration = sidx.references.reduce(\n          (dur, ref) => dur + ref.info.duration || 0,\n          0,\n        );\n        sidxMaxEnd = Math.max(\n          sidxMaxEnd,\n          subSegmentDuration + sidx.earliestPresentationTime / sidx.timescale,\n        );\n        sidxDuration = sidxMaxEnd - sidxMinStart;\n      }\n    }\n    if (sidxDuration && Number.isFinite(sidxDuration)) {\n      return sidxDuration;\n    }\n  }\n  if (videoDuration) {\n    return videoDuration;\n  }\n  return audioDuration;\n}\n\n/*\n  For Reference:\n  aligned(8) class TrackRunBox\n           extends FullBox(trun, version, tr_flags) {\n     unsigned int(32)  sample_count;\n     // the following are optional fields\n     signed int(32) data_offset;\n     unsigned int(32)  first_sample_flags;\n     // all fields in the following array are optional\n     {\n        unsigned int(32)  sample_duration;\n        unsigned int(32)  sample_size;\n        unsigned int(32)  sample_flags\n        if (version == 0)\n           { unsigned int(32)\n        else\n           { signed int(32)\n     }[ sample_count ]\n  }\n */\nexport function computeRawDurationFromSamples(trun): number {\n  const flags = readUint32(trun, 0);\n  // Flags are at offset 0, non-optional sample_count is at offset 4. Therefore we start 8 bytes in.\n  // Each field is an int32, which is 4 bytes\n  let offset = 8;\n  // data-offset-present flag\n  if (flags & 0x000001) {\n    offset += 4;\n  }\n  // first-sample-flags-present flag\n  if (flags & 0x000004) {\n    offset += 4;\n  }\n\n  let duration = 0;\n  const sampleCount = readUint32(trun, 4);\n  for (let i = 0; i < sampleCount; i++) {\n    // sample-duration-present flag\n    if (flags & 0x000100) {\n      const sampleDuration = readUint32(trun, offset);\n      duration += sampleDuration;\n      offset += 4;\n    }\n    // sample-size-present flag\n    if (flags & 0x000200) {\n      offset += 4;\n    }\n    // sample-flags-present flag\n    if (flags & 0x000400) {\n      offset += 4;\n    }\n    // sample-composition-time-offsets-present flag\n    if (flags & 0x000800) {\n      offset += 4;\n    }\n  }\n  return duration;\n}\n\nexport function offsetStartDTS(\n  initData: InitData,\n  fmp4: Uint8Array,\n  timeOffset: number,\n) {\n  findBox(fmp4, ['moof', 'traf']).forEach((traf) => {\n    findBox(traf, ['tfhd']).forEach((tfhd) => {\n      // get the track id from the tfhd\n      const id = readUint32(tfhd, 4);\n      const track = initData[id];\n      if (!track) {\n        return;\n      }\n      // assume a 90kHz clock if no timescale was specified\n      const timescale = track.timescale || 90e3;\n      // get the base media decode time from the tfdt\n      findBox(traf, ['tfdt']).forEach((tfdt) => {\n        const version = tfdt[0];\n        const offset = timeOffset * timescale;\n        if (offset) {\n          let baseMediaDecodeTime = readUint32(tfdt, 4);\n          if (version === 0) {\n            baseMediaDecodeTime -= offset;\n            baseMediaDecodeTime = Math.max(baseMediaDecodeTime, 0);\n            writeUint32(tfdt, 4, baseMediaDecodeTime);\n          } else {\n            baseMediaDecodeTime *= Math.pow(2, 32);\n            baseMediaDecodeTime += readUint32(tfdt, 8);\n            baseMediaDecodeTime -= offset;\n            baseMediaDecodeTime = Math.max(baseMediaDecodeTime, 0);\n            const upper = Math.floor(baseMediaDecodeTime / (UINT32_MAX + 1));\n            const lower = Math.floor(baseMediaDecodeTime % (UINT32_MAX + 1));\n            writeUint32(tfdt, 4, upper);\n            writeUint32(tfdt, 8, lower);\n          }\n        }\n      });\n    });\n  });\n}\n\n// TODO: Check if the last moof+mdat pair is part of the valid range\nexport function segmentValidRange(data: Uint8Array): SegmentedRange {\n  const segmentedRange: SegmentedRange = {\n    valid: null,\n    remainder: null,\n  };\n\n  const moofs = findBox(data, ['moof']);\n  if (moofs.length < 2) {\n    segmentedRange.remainder = data;\n    return segmentedRange;\n  }\n  const last = moofs[moofs.length - 1];\n  // Offset by 8 bytes; findBox offsets the start by as much\n  segmentedRange.valid = sliceUint8(data, 0, last.byteOffset - 8);\n  segmentedRange.remainder = sliceUint8(data, last.byteOffset - 8);\n  return segmentedRange;\n}\n\nexport interface SegmentedRange {\n  valid: Uint8Array | null;\n  remainder: Uint8Array | null;\n}\n\nexport function appendUint8Array(\n  data1: Uint8Array,\n  data2: Uint8Array,\n): Uint8Array {\n  const temp = new Uint8Array(data1.length + data2.length);\n  temp.set(data1);\n  temp.set(data2, data1.length);\n\n  return temp;\n}\n\nexport interface IEmsgParsingData {\n  schemeIdUri: string;\n  value: string;\n  timeScale: number;\n  presentationTimeDelta?: number;\n  presentationTime?: number;\n  eventDuration: number;\n  id: number;\n  payload: Uint8Array;\n}\n\nexport function parseSamples(\n  timeOffset: number,\n  track: PassthroughTrack,\n): UserdataSample[] {\n  const seiSamples = [] as UserdataSample[];\n  const videoData = track.samples;\n  const timescale = track.timescale;\n  const trackId = track.id;\n  let isHEVCFlavor = false;\n\n  const moofs = findBox(videoData, ['moof']);\n  moofs.map((moof) => {\n    const moofOffset = moof.byteOffset - 8;\n    const trafs = findBox(moof, ['traf']);\n    trafs.map((traf) => {\n      // get the base media decode time from the tfdt\n      const baseTime = findBox(traf, ['tfdt']).map((tfdt) => {\n        const version = tfdt[0];\n        let result = readUint32(tfdt, 4);\n        if (version === 1) {\n          result *= Math.pow(2, 32);\n          result += readUint32(tfdt, 8);\n        }\n        return result / timescale;\n      })[0];\n\n      if (baseTime !== undefined) {\n        timeOffset = baseTime;\n      }\n\n      return findBox(traf, ['tfhd']).map((tfhd) => {\n        const id = readUint32(tfhd, 4);\n        const tfhdFlags = readUint32(tfhd, 0) & 0xffffff;\n        const baseDataOffsetPresent = (tfhdFlags & 0x000001) !== 0;\n        const sampleDescriptionIndexPresent = (tfhdFlags & 0x000002) !== 0;\n        const defaultSampleDurationPresent = (tfhdFlags & 0x000008) !== 0;\n        let defaultSampleDuration = 0;\n        const defaultSampleSizePresent = (tfhdFlags & 0x000010) !== 0;\n        let defaultSampleSize = 0;\n        const defaultSampleFlagsPresent = (tfhdFlags & 0x000020) !== 0;\n        let tfhdOffset = 8;\n\n        if (id === trackId) {\n          if (baseDataOffsetPresent) {\n            tfhdOffset += 8;\n          }\n          if (sampleDescriptionIndexPresent) {\n            tfhdOffset += 4;\n          }\n          if (defaultSampleDurationPresent) {\n            defaultSampleDuration = readUint32(tfhd, tfhdOffset);\n            tfhdOffset += 4;\n          }\n          if (defaultSampleSizePresent) {\n            defaultSampleSize = readUint32(tfhd, tfhdOffset);\n            tfhdOffset += 4;\n          }\n          if (defaultSampleFlagsPresent) {\n            tfhdOffset += 4;\n          }\n          if (track.type === 'video') {\n            isHEVCFlavor = isHEVC(track.codec);\n          }\n\n          findBox(traf, ['trun']).map((trun) => {\n            const version = trun[0];\n            const flags = readUint32(trun, 0) & 0xffffff;\n            const dataOffsetPresent = (flags & 0x000001) !== 0;\n            let dataOffset = 0;\n            const firstSampleFlagsPresent = (flags & 0x000004) !== 0;\n            const sampleDurationPresent = (flags & 0x000100) !== 0;\n            let sampleDuration = 0;\n            const sampleSizePresent = (flags & 0x000200) !== 0;\n            let sampleSize = 0;\n            const sampleFlagsPresent = (flags & 0x000400) !== 0;\n            const sampleCompositionOffsetsPresent = (flags & 0x000800) !== 0;\n            let compositionOffset = 0;\n            const sampleCount = readUint32(trun, 4);\n            let trunOffset = 8; // past version, flags, and sample count\n\n            if (dataOffsetPresent) {\n              dataOffset = readUint32(trun, trunOffset);\n              trunOffset += 4;\n            }\n            if (firstSampleFlagsPresent) {\n              trunOffset += 4;\n            }\n\n            let sampleOffset = dataOffset + moofOffset;\n\n            for (let ix = 0; ix < sampleCount; ix++) {\n              if (sampleDurationPresent) {\n                sampleDuration = readUint32(trun, trunOffset);\n                trunOffset += 4;\n              } else {\n                sampleDuration = defaultSampleDuration;\n              }\n              if (sampleSizePresent) {\n                sampleSize = readUint32(trun, trunOffset);\n                trunOffset += 4;\n              } else {\n                sampleSize = defaultSampleSize;\n              }\n              if (sampleFlagsPresent) {\n                trunOffset += 4;\n              }\n              if (sampleCompositionOffsetsPresent) {\n                if (version === 0) {\n                  compositionOffset = readUint32(trun, trunOffset);\n                } else {\n                  compositionOffset = readSint32(trun, trunOffset);\n                }\n                trunOffset += 4;\n              }\n              if (track.type === ElementaryStreamTypes.VIDEO) {\n                let naluTotalSize = 0;\n                while (naluTotalSize < sampleSize) {\n                  const naluSize = readUint32(videoData, sampleOffset);\n                  sampleOffset += 4;\n                  if (isSEIMessage(isHEVCFlavor, videoData[sampleOffset])) {\n                    const data = videoData.subarray(\n                      sampleOffset,\n                      sampleOffset + naluSize,\n                    );\n                    parseSEIMessageFromNALu(\n                      data,\n                      isHEVCFlavor ? 2 : 1,\n                      timeOffset + compositionOffset / timescale,\n                      seiSamples,\n                    );\n                  }\n                  sampleOffset += naluSize;\n                  naluTotalSize += naluSize + 4;\n                }\n              }\n\n              timeOffset += sampleDuration / timescale;\n            }\n          });\n        }\n      });\n    });\n  });\n  return seiSamples;\n}\n\nfunction isHEVC(codec: string) {\n  if (!codec) {\n    return false;\n  }\n  const delimit = codec.indexOf('.');\n  const baseCodec = delimit < 0 ? codec : codec.substring(0, delimit);\n  return (\n    baseCodec === 'hvc1' ||\n    baseCodec === 'hev1' ||\n    // Dolby Vision\n    baseCodec === 'dvh1' ||\n    baseCodec === 'dvhe'\n  );\n}\n\nfunction isSEIMessage(isHEVCFlavor: boolean, naluHeader: number) {\n  if (isHEVCFlavor) {\n    const naluType = (naluHeader >> 1) & 0x3f;\n    return naluType === 39 || naluType === 40;\n  } else {\n    const naluType = naluHeader & 0x1f;\n    return naluType === 6;\n  }\n}\n\nexport function parseSEIMessageFromNALu(\n  unescapedData: Uint8Array,\n  headerSize: number,\n  pts: number,\n  samples: UserdataSample[],\n) {\n  const data = discardEPB(unescapedData);\n  let seiPtr = 0;\n  // skip nal header\n  seiPtr += headerSize;\n  let payloadType = 0;\n  let payloadSize = 0;\n  let b = 0;\n\n  while (seiPtr < data.length) {\n    payloadType = 0;\n    do {\n      if (seiPtr >= data.length) {\n        break;\n      }\n      b = data[seiPtr++];\n      payloadType += b;\n    } while (b === 0xff);\n\n    // Parse payload size.\n    payloadSize = 0;\n    do {\n      if (seiPtr >= data.length) {\n        break;\n      }\n      b = data[seiPtr++];\n      payloadSize += b;\n    } while (b === 0xff);\n\n    const leftOver = data.length - seiPtr;\n    // Create a variable to process the payload\n    let payPtr = seiPtr;\n\n    // Increment the seiPtr to the end of the payload\n    if (payloadSize < leftOver) {\n      seiPtr += payloadSize;\n    } else if (payloadSize > leftOver) {\n      // Some type of corruption has happened?\n      logger.error(\n        `Malformed SEI payload. ${payloadSize} is too small, only ${leftOver} bytes left to parse.`,\n      );\n      // We might be able to parse some data, but let's be safe and ignore it.\n      break;\n    }\n\n    if (payloadType === 4) {\n      const countryCode = data[payPtr++];\n      if (countryCode === 181) {\n        const providerCode = readUint16(data, payPtr);\n        payPtr += 2;\n\n        if (providerCode === 49) {\n          const userStructure = readUint32(data, payPtr);\n          payPtr += 4;\n\n          if (userStructure === 0x47413934) {\n            const userDataType = data[payPtr++];\n\n            // Raw CEA-608 bytes wrapped in CEA-708 packet\n            if (userDataType === 3) {\n              const firstByte = data[payPtr++];\n              const totalCCs = 0x1f & firstByte;\n              const enabled = 0x40 & firstByte;\n              const totalBytes = enabled ? 2 + totalCCs * 3 : 0;\n              const byteArray = new Uint8Array(totalBytes);\n              if (enabled) {\n                byteArray[0] = firstByte;\n                for (let i = 1; i < totalBytes; i++) {\n                  byteArray[i] = data[payPtr++];\n                }\n              }\n\n              samples.push({\n                type: userDataType,\n                payloadType,\n                pts,\n                bytes: byteArray,\n              });\n            }\n          }\n        }\n      }\n    } else if (payloadType === 5) {\n      if (payloadSize > 16) {\n        const uuidStrArray: Array<string> = [];\n        for (let i = 0; i < 16; i++) {\n          const b = data[payPtr++].toString(16);\n          uuidStrArray.push(b.length == 1 ? '0' + b : b);\n\n          if (i === 3 || i === 5 || i === 7 || i === 9) {\n            uuidStrArray.push('-');\n          }\n        }\n        const length = payloadSize - 16;\n        const userDataBytes = new Uint8Array(length);\n        for (let i = 0; i < length; i++) {\n          userDataBytes[i] = data[payPtr++];\n        }\n\n        samples.push({\n          payloadType,\n          pts,\n          uuid: uuidStrArray.join(''),\n          userData: utf8ArrayToStr(userDataBytes),\n          userDataBytes,\n        });\n      }\n    }\n  }\n}\n\n/**\n * remove Emulation Prevention bytes from a RBSP\n */\nexport function discardEPB(data: Uint8Array): Uint8Array {\n  const length = data.byteLength;\n  const EPBPositions = [] as Array<number>;\n  let i = 1;\n\n  // Find all `Emulation Prevention Bytes`\n  while (i < length - 2) {\n    if (data[i] === 0 && data[i + 1] === 0 && data[i + 2] === 0x03) {\n      EPBPositions.push(i + 2);\n      i += 2;\n    } else {\n      i++;\n    }\n  }\n\n  // If no Emulation Prevention Bytes were found just return the original\n  // array\n  if (EPBPositions.length === 0) {\n    return data;\n  }\n\n  // Create a new array to hold the NAL unit data\n  const newLength = length - EPBPositions.length;\n  const newData = new Uint8Array(newLength);\n  let sourceIndex = 0;\n\n  for (i = 0; i < newLength; sourceIndex++, i++) {\n    if (sourceIndex === EPBPositions[0]) {\n      // Skip this byte\n      sourceIndex++;\n      // Remove this position index\n      EPBPositions.shift();\n    }\n    newData[i] = data[sourceIndex];\n  }\n  return newData;\n}\n\nexport function parseEmsg(data: Uint8Array): IEmsgParsingData {\n  const version = data[0];\n  let schemeIdUri: string = '';\n  let value: string = '';\n  let timeScale: number = 0;\n  let presentationTimeDelta: number = 0;\n  let presentationTime: number = 0;\n  let eventDuration: number = 0;\n  let id: number = 0;\n  let offset: number = 0;\n\n  if (version === 0) {\n    while (bin2str(data.subarray(offset, offset + 1)) !== '\\0') {\n      schemeIdUri += bin2str(data.subarray(offset, offset + 1));\n      offset += 1;\n    }\n\n    schemeIdUri += bin2str(data.subarray(offset, offset + 1));\n    offset += 1;\n\n    while (bin2str(data.subarray(offset, offset + 1)) !== '\\0') {\n      value += bin2str(data.subarray(offset, offset + 1));\n      offset += 1;\n    }\n\n    value += bin2str(data.subarray(offset, offset + 1));\n    offset += 1;\n\n    timeScale = readUint32(data, 12);\n    presentationTimeDelta = readUint32(data, 16);\n    eventDuration = readUint32(data, 20);\n    id = readUint32(data, 24);\n    offset = 28;\n  } else if (version === 1) {\n    offset += 4;\n    timeScale = readUint32(data, offset);\n    offset += 4;\n    const leftPresentationTime = readUint32(data, offset);\n    offset += 4;\n    const rightPresentationTime = readUint32(data, offset);\n    offset += 4;\n    presentationTime = 2 ** 32 * leftPresentationTime + rightPresentationTime;\n    if (!Number.isSafeInteger(presentationTime)) {\n      presentationTime = Number.MAX_SAFE_INTEGER;\n      logger.warn(\n        'Presentation time exceeds safe integer limit and wrapped to max safe integer in parsing emsg box',\n      );\n    }\n\n    eventDuration = readUint32(data, offset);\n    offset += 4;\n    id = readUint32(data, offset);\n    offset += 4;\n\n    while (bin2str(data.subarray(offset, offset + 1)) !== '\\0') {\n      schemeIdUri += bin2str(data.subarray(offset, offset + 1));\n      offset += 1;\n    }\n\n    schemeIdUri += bin2str(data.subarray(offset, offset + 1));\n    offset += 1;\n\n    while (bin2str(data.subarray(offset, offset + 1)) !== '\\0') {\n      value += bin2str(data.subarray(offset, offset + 1));\n      offset += 1;\n    }\n\n    value += bin2str(data.subarray(offset, offset + 1));\n    offset += 1;\n  }\n  const payload = data.subarray(offset, data.byteLength);\n\n  return {\n    schemeIdUri,\n    value,\n    timeScale,\n    presentationTime,\n    presentationTimeDelta,\n    eventDuration,\n    id,\n    payload,\n  };\n}\n\nexport function mp4Box(type: ArrayLike<number>, ...payload: Uint8Array[]) {\n  const len = payload.length;\n  let size = 8;\n  let i = len;\n  while (i--) {\n    size += payload[i].byteLength;\n  }\n  const result = new Uint8Array(size);\n  result[0] = (size >> 24) & 0xff;\n  result[1] = (size >> 16) & 0xff;\n  result[2] = (size >> 8) & 0xff;\n  result[3] = size & 0xff;\n  result.set(type, 4);\n  for (i = 0, size = 8; i < len; i++) {\n    result.set(payload[i], size);\n    size += payload[i].byteLength;\n  }\n  return result;\n}\n\nexport function mp4pssh(\n  systemId: Uint8Array,\n  keyids: Array<Uint8Array> | null,\n  data: Uint8Array,\n) {\n  if (systemId.byteLength !== 16) {\n    throw new RangeError('Invalid system id');\n  }\n  let version;\n  let kids;\n  if (keyids) {\n    version = 1;\n    kids = new Uint8Array(keyids.length * 16);\n    for (let ix = 0; ix < keyids.length; ix++) {\n      const k = keyids[ix]; // uint8array\n      if (k.byteLength !== 16) {\n        throw new RangeError('Invalid key');\n      }\n      kids.set(k, ix * 16);\n    }\n  } else {\n    version = 0;\n    kids = new Uint8Array();\n  }\n  let kidCount;\n  if (version > 0) {\n    kidCount = new Uint8Array(4);\n    if (keyids!.length > 0) {\n      new DataView(kidCount.buffer).setUint32(0, keyids!.length, false);\n    }\n  } else {\n    kidCount = new Uint8Array();\n  }\n  const dataSize = new Uint8Array(4);\n  if (data && data.byteLength > 0) {\n    new DataView(dataSize.buffer).setUint32(0, data.byteLength, false);\n  }\n  return mp4Box(\n    [112, 115, 115, 104],\n    new Uint8Array([\n      version,\n      0x00,\n      0x00,\n      0x00, // Flags\n    ]),\n    systemId, // 16 bytes\n    kidCount,\n    kids,\n    dataSize,\n    data || new Uint8Array(),\n  );\n}\n\nexport type PsshData = {\n  version: 0 | 1;\n  systemId: KeySystemIds;\n  kids: null | Uint8Array[];\n  data: null | Uint8Array;\n  offset: number;\n  size: number;\n};\n\nexport type PsshInvalidResult = {\n  systemId?: undefined;\n  offset: number;\n  size: number;\n};\n\nexport function parseMultiPssh(\n  initData: ArrayBuffer,\n): (PsshData | PsshInvalidResult)[] {\n  const results: (PsshData | PsshInvalidResult)[] = [];\n  if (initData instanceof ArrayBuffer) {\n    const length = initData.byteLength;\n    let offset = 0;\n    while (offset + 32 < length) {\n      const view = new DataView(initData, offset);\n      const pssh = parsePssh(view);\n      results.push(pssh);\n      offset += pssh.size;\n    }\n  }\n  return results;\n}\n\nfunction parsePssh(view: DataView): PsshData | PsshInvalidResult {\n  const size = view.getUint32(0);\n  const offset = view.byteOffset;\n  const length = view.byteLength;\n  if (length < size) {\n    return {\n      offset,\n      size: length,\n    };\n  }\n  const type = view.getUint32(4);\n  if (type !== 0x70737368) {\n    return { offset, size };\n  }\n  const version = view.getUint32(8) >>> 24;\n  if (version !== 0 && version !== 1) {\n    return { offset, size };\n  }\n  const buffer = view.buffer;\n  const systemId = Hex.hexDump(\n    new Uint8Array(buffer, offset + 12, 16),\n  ) as KeySystemIds;\n  const dataSizeOrKidCount = view.getUint32(28);\n  let kids: null | Uint8Array[] = null;\n  let data: null | Uint8Array = null;\n  if (version === 0) {\n    if (size - 32 < dataSizeOrKidCount || dataSizeOrKidCount < 22) {\n      return { offset, size };\n    }\n    data = new Uint8Array(buffer, offset + 32, dataSizeOrKidCount);\n  } else if (version === 1) {\n    if (\n      !dataSizeOrKidCount ||\n      length < offset + 32 + dataSizeOrKidCount * 16 + 16\n    ) {\n      return { offset, size };\n    }\n    kids = [];\n    for (let i = 0; i < dataSizeOrKidCount; i++) {\n      kids.push(new Uint8Array(buffer, offset + 32 + i * 16, 16));\n    }\n  }\n  return {\n    version,\n    systemId,\n    kids,\n    data,\n    offset,\n    size,\n  };\n}\n", "import {\n  changeEndianness,\n  convertDataUriToArrayBytes,\n} from '../utils/keysystem-util';\nimport { KeySystemFormats } from '../utils/mediakeys-helper';\nimport { mp4pssh } from '../utils/mp4-tools';\nimport { logger } from '../utils/logger';\nimport { base64Decode } from '../utils/numeric-encoding-utils';\n\nlet keyUriToKeyIdMap: { [uri: string]: Uint8Array } = {};\n\nexport interface DecryptData {\n  uri: string;\n  method: string;\n  keyFormat: string;\n  keyFormatVersions: number[];\n  iv: Uint8Array | null;\n  key: Uint8Array | null;\n  keyId: Uint8Array | null;\n  pssh: Uint8Array | null;\n  encrypted: boolean;\n  isCommonEncryption: boolean;\n}\n\nexport class LevelKey implements DecryptData {\n  public readonly uri: string;\n  public readonly method: string;\n  public readonly keyFormat: string;\n  public readonly keyFormatVersions: number[];\n  public readonly encrypted: boolean;\n  public readonly isCommonEncryption: boolean;\n  public iv: Uint8Array | null = null;\n  public key: Uint8Array | null = null;\n  public keyId: Uint8Array | null = null;\n  public pssh: Uint8Array | null = null;\n\n  static clearKeyUriToKeyIdMap() {\n    keyUriToKeyIdMap = {};\n  }\n\n  constructor(\n    method: string,\n    uri: string,\n    format: string,\n    formatversions: number[] = [1],\n    iv: Uint8Array | null = null,\n  ) {\n    this.method = method;\n    this.uri = uri;\n    this.keyFormat = format;\n    this.keyFormatVersions = formatversions;\n    this.iv = iv;\n    this.encrypted = method ? method !== 'NONE' : false;\n    this.isCommonEncryption = this.encrypted && method !== 'AES-128';\n  }\n\n  public isSupported(): boolean {\n    // If it's Segment encryption or No encryption, just select that key system\n    if (this.method) {\n      if (this.method === 'AES-128' || this.method === 'NONE') {\n        return true;\n      }\n      if (this.keyFormat === 'identity') {\n        // Maintain support for clear SAMPLE-AES with MPEG-3 TS\n        return this.method === 'SAMPLE-AES';\n      } else if (__USE_EME_DRM__) {\n        switch (this.keyFormat) {\n          case KeySystemFormats.FAIRPLAY:\n          case KeySystemFormats.WIDEVINE:\n          case KeySystemFormats.PLAYREADY:\n          case KeySystemFormats.CLEARKEY:\n            return (\n              [\n                'ISO-23001-7',\n                'SAMPLE-AES',\n                'SAMPLE-AES-CENC',\n                'SAMPLE-AES-CTR',\n              ].indexOf(this.method) !== -1\n            );\n        }\n      }\n    }\n    return false;\n  }\n\n  public getDecryptData(sn: number | 'initSegment'): LevelKey | null {\n    if (!this.encrypted || !this.uri) {\n      return null;\n    }\n\n    if (this.method === 'AES-128' && this.uri && !this.iv) {\n      if (typeof sn !== 'number') {\n        // We are fetching decryption data for a initialization segment\n        // If the segment was encrypted with AES-128\n        // It must have an IV defined. We cannot substitute the Segment Number in.\n        if (this.method === 'AES-128' && !this.iv) {\n          logger.warn(\n            `missing IV for initialization segment with method=\"${this.method}\" - compliance issue`,\n          );\n        }\n        // Explicitly set sn to resulting value from implicit conversions 'initSegment' values for IV generation.\n        sn = 0;\n      }\n      const iv = createInitializationVector(sn);\n      const decryptdata = new LevelKey(\n        this.method,\n        this.uri,\n        'identity',\n        this.keyFormatVersions,\n        iv,\n      );\n      return decryptdata;\n    }\n\n    if (!__USE_EME_DRM__) {\n      return this;\n    }\n\n    // Initialize keyId if possible\n    const keyBytes = convertDataUriToArrayBytes(this.uri);\n    if (keyBytes) {\n      switch (this.keyFormat) {\n        case KeySystemFormats.WIDEVINE:\n          this.pssh = keyBytes;\n          // In case of widevine keyID is embedded in PSSH box. Read Key ID.\n          if (keyBytes.length >= 22) {\n            this.keyId = keyBytes.subarray(\n              keyBytes.length - 22,\n              keyBytes.length - 6,\n            );\n          }\n          break;\n        case KeySystemFormats.PLAYREADY: {\n          const PlayReadyKeySystemUUID = new Uint8Array([\n            0x9a, 0x04, 0xf0, 0x79, 0x98, 0x40, 0x42, 0x86, 0xab, 0x92, 0xe6,\n            0x5b, 0xe0, 0x88, 0x5f, 0x95,\n          ]);\n\n          this.pssh = mp4pssh(PlayReadyKeySystemUUID, null, keyBytes);\n\n          const keyBytesUtf16 = new Uint16Array(\n            keyBytes.buffer,\n            keyBytes.byteOffset,\n            keyBytes.byteLength / 2,\n          );\n          const keyByteStr = String.fromCharCode.apply(\n            null,\n            Array.from(keyBytesUtf16),\n          );\n\n          // Parse Playready WRMHeader XML\n          const xmlKeyBytes = keyByteStr.substring(\n            keyByteStr.indexOf('<'),\n            keyByteStr.length,\n          );\n          const parser = new DOMParser();\n          const xmlDoc = parser.parseFromString(xmlKeyBytes, 'text/xml');\n          const keyData = xmlDoc.getElementsByTagName('KID')[0];\n          if (keyData) {\n            const keyId = keyData.childNodes[0]\n              ? keyData.childNodes[0].nodeValue\n              : keyData.getAttribute('VALUE');\n            if (keyId) {\n              const keyIdArray = base64Decode(keyId).subarray(0, 16);\n              // KID value in PRO is a base64-encoded little endian GUID interpretation of UUID\n              // KID value in tenc is a big endian UUID GUID interpretation of UUID\n              changeEndianness(keyIdArray);\n              this.keyId = keyIdArray;\n            }\n          }\n          break;\n        }\n        default: {\n          let keydata = keyBytes.subarray(0, 16);\n          if (keydata.length !== 16) {\n            const padded = new Uint8Array(16);\n            padded.set(keydata, 16 - keydata.length);\n            keydata = padded;\n          }\n          this.keyId = keydata;\n          break;\n        }\n      }\n    }\n\n    // Default behavior: assign a new keyId for each uri\n    if (!this.keyId || this.keyId.byteLength !== 16) {\n      let keyId = keyUriToKeyIdMap[this.uri];\n      if (!keyId) {\n        const val =\n          Object.keys(keyUriToKeyIdMap).length % Number.MAX_SAFE_INTEGER;\n        keyId = new Uint8Array(16);\n        const dv = new DataView(keyId.buffer, 12, 4); // Just set the last 4 bytes\n        dv.setUint32(0, val);\n        keyUriToKeyIdMap[this.uri] = keyId;\n      }\n      this.keyId = keyId;\n    }\n\n    return this;\n  }\n}\n\nfunction createInitializationVector(segmentNumber: number): Uint8Array {\n  const uint8View = new Uint8Array(16);\n  for (let i = 12; i < 16; i++) {\n    uint8View[i] = (segmentNumber >> (8 * (15 - i))) & 0xff;\n  }\n  return uint8View;\n}\n", "import type { AttrList } from './attr-list';\nimport type { ParsedMultivariantPlaylist } from '../loader/m3u8-parser';\nimport type { LevelDetails } from '../loader/level-details';\nimport type { VariableMap } from '../types/level';\n\nconst VARIABLE_REPLACEMENT_REGEX = /\\{\\$([a-zA-Z0-9-_]+)\\}/g;\n\nexport function hasVariableReferences(str: string): boolean {\n  return VARIABLE_REPLACEMENT_REGEX.test(str);\n}\n\nexport function substituteVariablesInAttributes(\n  parsed: Pick<\n    ParsedMultivariantPlaylist | LevelDetails,\n    'variableList' | 'hasVariableRefs' | 'playlistParsingError'\n  >,\n  attr: AttrList,\n  attributeNames: string[],\n) {\n  if (parsed.variableList !== null || parsed.hasVariableRefs) {\n    for (let i = attributeNames.length; i--; ) {\n      const name = attributeNames[i];\n      const value = attr[name];\n      if (value) {\n        attr[name] = substituteVariables(parsed, value);\n      }\n    }\n  }\n}\n\nexport function substituteVariables(\n  parsed: Pick<\n    ParsedMultivariantPlaylist | LevelDetails,\n    'variableList' | 'hasVariableRefs' | 'playlistParsingError'\n  >,\n  value: string,\n): string {\n  if (parsed.variableList !== null || parsed.hasVariableRefs) {\n    const variableList = parsed.variableList;\n    return value.replace(\n      VARIABLE_REPLACEMENT_REGEX,\n      (variableReference: string) => {\n        const variableName = variableReference.substring(\n          2,\n          variableReference.length - 1,\n        );\n        const variableValue = variableList?.[variableName];\n        if (variableValue === undefined) {\n          parsed.playlistParsingError ||= new Error(\n            `Missing preceding EXT-X-DEFINE tag for Variable Reference: \"${variableName}\"`,\n          );\n          return variableReference;\n        }\n        return variableValue;\n      },\n    );\n  }\n  return value;\n}\n\nexport function addVariableDefinition(\n  parsed: Pick<\n    ParsedMultivariantPlaylist | LevelDetails,\n    'variableList' | 'playlistParsingError'\n  >,\n  attr: AttrList,\n  parentUrl: string,\n) {\n  let variableList = parsed.variableList;\n  if (!variableList) {\n    parsed.variableList = variableList = {};\n  }\n  let NAME: string;\n  let VALUE;\n  if ('QUERYPARAM' in attr) {\n    NAME = attr.QUERYPARAM;\n    try {\n      const searchParams = new self.URL(parentUrl).searchParams;\n      if (searchParams.has(NAME)) {\n        VALUE = searchParams.get(NAME);\n      } else {\n        throw new Error(\n          `\"${NAME}\" does not match any query parameter in URI: \"${parentUrl}\"`,\n        );\n      }\n    } catch (error) {\n      parsed.playlistParsingError ||= new Error(\n        `EXT-X-DEFINE QUERYPARAM: ${error.message}`,\n      );\n    }\n  } else {\n    NAME = attr.NAME;\n    VALUE = attr.VALUE;\n  }\n  if (NAME in variableList) {\n    parsed.playlistParsingError ||= new Error(\n      `EXT-X-DEFINE duplicate Variable Name declarations: \"${NAME}\"`,\n    );\n  } else {\n    variableList[NAME] = VALUE || '';\n  }\n}\n\nexport function importVariableDefinition(\n  parsed: Pick<\n    ParsedMultivariantPlaylist | LevelDetails,\n    'variableList' | 'playlistParsingError'\n  >,\n  attr: AttrList,\n  sourceVariableList: VariableMap | null,\n) {\n  const IMPORT = attr.IMPORT;\n  if (sourceVariableList && IMPORT in sourceVariableList) {\n    let variableList = parsed.variableList;\n    if (!variableList) {\n      parsed.variableList = variableList = {};\n    }\n    variableList[IMPORT] = sourceVariableList[IMPORT];\n  } else {\n    parsed.playlistParsingError ||= new Error(\n      `EXT-X-DEFINE IMPORT attribute not found in Multivariant Playlist: \"${IMPORT}\"`,\n    );\n  }\n}\n", "/**\n * MediaSource helper\n */\n\nexport function getMediaSource(\n  preferManagedMediaSource = true,\n): typeof MediaSource | undefined {\n  if (typeof self === 'undefined') return undefined;\n  const mms =\n    (preferManagedMediaSource || !self.MediaSource) &&\n    ((self as any).ManagedMediaSource as undefined | typeof MediaSource);\n  return (\n    mms ||\n    self.MediaSource ||\n    ((self as any).WebKitMediaSource as typeof MediaSource)\n  );\n}\n\nexport function isManagedMediaSource(source: typeof MediaSource | undefined) {\n  return (\n    typeof self !== 'undefined' && source === (self as any).ManagedMediaSource\n  );\n}\n", "import { getMediaSource } from './mediasource-helper';\n\n// from http://mp4ra.org/codecs.html\n// values indicate codec selection preference (lower is higher priority)\nconst sampleEntryCodesISO = {\n  audio: {\n    a3ds: 1,\n    'ac-3': 0.95,\n    'ac-4': 1,\n    alac: 0.9,\n    alaw: 1,\n    dra1: 1,\n    'dts+': 1,\n    'dts-': 1,\n    dtsc: 1,\n    dtse: 1,\n    dtsh: 1,\n    'ec-3': 0.9,\n    enca: 1,\n    fLaC: 0.9, // MP4-RA listed codec entry for FLAC\n    flac: 0.9, // legacy browser codec name for FLAC\n    FLAC: 0.9, // some manifests may list \"FLAC\" with Apple's tools\n    g719: 1,\n    g726: 1,\n    m4ae: 1,\n    mha1: 1,\n    mha2: 1,\n    mhm1: 1,\n    mhm2: 1,\n    mlpa: 1,\n    mp4a: 1,\n    'raw ': 1,\n    Opus: 1,\n    opus: 1, // browsers expect this to be lowercase despite MP4RA says 'Opus'\n    samr: 1,\n    sawb: 1,\n    sawp: 1,\n    sevc: 1,\n    sqcp: 1,\n    ssmv: 1,\n    twos: 1,\n    ulaw: 1,\n  },\n  video: {\n    avc1: 1,\n    avc2: 1,\n    avc3: 1,\n    avc4: 1,\n    avcp: 1,\n    av01: 0.8,\n    drac: 1,\n    dva1: 1,\n    dvav: 1,\n    dvh1: 0.7,\n    dvhe: 0.7,\n    encv: 1,\n    hev1: 0.75,\n    hvc1: 0.75,\n    mjp2: 1,\n    mp4v: 1,\n    mvc1: 1,\n    mvc2: 1,\n    mvc3: 1,\n    mvc4: 1,\n    resv: 1,\n    rv60: 1,\n    s263: 1,\n    svc1: 1,\n    svc2: 1,\n    'vc-1': 1,\n    vp08: 1,\n    vp09: 0.9,\n  },\n  text: {\n    stpp: 1,\n    wvtt: 1,\n  },\n} as const;\n\nexport type CodecType = 'audio' | 'video';\n\nexport function isCodecType(codec: string, type: CodecType): boolean {\n  const typeCodes = sampleEntryCodesISO[type];\n  return !!typeCodes && !!typeCodes[codec.slice(0, 4)];\n}\n\nexport function areCodecsMediaSourceSupported(\n  codecs: string,\n  type: CodecType,\n  preferManagedMediaSource = true,\n): boolean {\n  return !codecs\n    .split(',')\n    .some(\n      (codec) =>\n        !isCodecMediaSourceSupported(codec, type, preferManagedMediaSource),\n    );\n}\n\nfunction isCodecMediaSourceSupported(\n  codec: string,\n  type: CodecType,\n  preferManagedMediaSource = true,\n): boolean {\n  const MediaSource = getMediaSource(preferManagedMediaSource);\n  return MediaSource?.isTypeSupported(mimeTypeForCodec(codec, type)) ?? false;\n}\n\nexport function mimeTypeForCodec(codec: string, type: CodecType): string {\n  return `${type}/mp4;codecs=\"${codec}\"`;\n}\n\nexport function videoCodecPreferenceValue(\n  videoCodec: string | undefined,\n): number {\n  if (videoCodec) {\n    const fourCC = videoCodec.substring(0, 4);\n    return sampleEntryCodesISO.video[fourCC];\n  }\n  return 2;\n}\n\nexport function codecsSetSelectionPreferenceValue(codecSet: string): number {\n  return codecSet.split(',').reduce((num, fourCC) => {\n    const preferenceValue = sampleEntryCodesISO.video[fourCC];\n    if (preferenceValue) {\n      return (preferenceValue * 2 + num) / (num ? 3 : 2);\n    }\n    return (sampleEntryCodesISO.audio[fourCC] + num) / (num ? 2 : 1);\n  }, 0);\n}\n\ninterface CodecNameCache {\n  flac?: string;\n  opus?: string;\n}\n\nconst CODEC_COMPATIBLE_NAMES: CodecNameCache = {};\n\ntype LowerCaseCodecType = 'flac' | 'opus';\n\nfunction getCodecCompatibleNameLower(\n  lowerCaseCodec: LowerCaseCodecType,\n  preferManagedMediaSource = true,\n): string {\n  if (CODEC_COMPATIBLE_NAMES[lowerCaseCodec]) {\n    return CODEC_COMPATIBLE_NAMES[lowerCaseCodec]!;\n  }\n\n  // Idealy fLaC and Opus would be first (spec-compliant) but\n  // some browsers will report that fLaC is supported then fail.\n  // see: https://bugs.chromium.org/p/chromium/issues/detail?id=1422728\n  const codecsToCheck = {\n    flac: ['flac', 'fLaC', 'FLAC'],\n    opus: ['opus', 'Opus'],\n  }[lowerCaseCodec];\n\n  for (let i = 0; i < codecsToCheck.length; i++) {\n    if (\n      isCodecMediaSourceSupported(\n        codecsToCheck[i],\n        'audio',\n        preferManagedMediaSource,\n      )\n    ) {\n      CODEC_COMPATIBLE_NAMES[lowerCaseCodec] = codecsToCheck[i];\n      return codecsToCheck[i];\n    }\n  }\n\n  return lowerCaseCodec;\n}\n\nconst AUDIO_CODEC_REGEXP = /flac|opus/i;\nexport function getCodecCompatibleName(\n  codec: string,\n  preferManagedMediaSource = true,\n): string {\n  return codec.replace(AUDIO_CODEC_REGEXP, (m) =>\n    getCodecCompatibleNameLower(\n      m.toLowerCase() as LowerCaseCodecType,\n      preferManagedMediaSource,\n    ),\n  );\n}\n\nexport function pickMostCompleteCodecName(\n  parsedCodec: string,\n  levelCodec: string | undefined,\n): string | undefined {\n  // Parsing of mp4a codecs strings in mp4-tools from media is incomplete as of d8c6c7a\n  // so use level codec is parsed codec is unavailable or incomplete\n  if (parsedCodec && parsedCodec !== 'mp4a') {\n    return parsedCodec;\n  }\n  return levelCodec ? levelCodec.split(',')[0] : levelCodec;\n}\n\nexport function convertAVC1ToAVCOTI(codec: string) {\n  // Convert avc1 codec string from RFC-4281 to RFC-6381 for MediaSource.isTypeSupported\n  // Examples: avc1.66.30 to avc1.42001e and avc1.77.30,avc1.66.30 to avc1.4d001e,avc1.42001e.\n  const codecs = codec.split(',');\n  for (let i = 0; i < codecs.length; i++) {\n    const avcdata = codecs[i].split('.');\n    if (avcdata.length > 2) {\n      let result = avcdata.shift() + '.';\n      result += parseInt(avcdata.shift() as string).toString(16);\n      result += (\n        '000' + parseInt(avcdata.shift() as string).toString(16)\n      ).slice(-4);\n      codecs[i] = result;\n    }\n  }\n  return codecs.join(',');\n}\n", "import { buildAbsoluteURL } from 'url-toolkit';\nimport { DateRange } from './date-range';\nimport { Fragment, Part } from './fragment';\nimport { LevelDetails } from './level-details';\nimport { LevelKey } from './level-key';\nimport { AttrList } from '../utils/attr-list';\nimport { logger } from '../utils/logger';\nimport {\n  addVariableDefinition,\n  hasVariableReferences,\n  importVariableDefinition,\n  substituteVariables,\n  substituteVariablesInAttributes,\n} from '../utils/variable-substitution';\nimport { isCodecType } from '../utils/codecs';\nimport type { CodecType } from '../utils/codecs';\nimport type { MediaPlaylist, MediaAttributes } from '../types/media-playlist';\nimport type { PlaylistLevelType } from '../types/loader';\nimport type { LevelAttributes, LevelParsed, VariableMap } from '../types/level';\nimport type { ContentSteeringOptions } from '../types/events';\n\ntype M3U8ParserFragments = Array<Fragment | null>;\n\nexport type ParsedMultivariantPlaylist = {\n  contentSteering: ContentSteeringOptions | null;\n  levels: LevelParsed[];\n  playlistParsingError: Error | null;\n  sessionData: Record<string, AttrList> | null;\n  sessionKeys: LevelKey[] | null;\n  startTimeOffset: number | null;\n  variableList: VariableMap | null;\n  hasVariableRefs: boolean;\n};\n\ntype ParsedMultivariantMediaOptions = {\n  AUDIO?: MediaPlaylist[];\n  SUBTITLES?: MediaPlaylist[];\n  'CLOSED-CAPTIONS'?: MediaPlaylist[];\n};\n\nconst MASTER_PLAYLIST_REGEX =\n  /#EXT-X-STREAM-INF:([^\\r\\n]*)(?:[\\r\\n](?:#[^\\r\\n]*)?)*([^\\r\\n]+)|#EXT-X-(SESSION-DATA|SESSION-KEY|DEFINE|CONTENT-STEERING|START):([^\\r\\n]*)[\\r\\n]+/g;\nconst MASTER_PLAYLIST_MEDIA_REGEX = /#EXT-X-MEDIA:(.*)/g;\n\nconst IS_MEDIA_PLAYLIST = /^#EXT(?:INF|-X-TARGETDURATION):/m; // Handle empty Media Playlist (first EXTINF not signaled, but TARGETDURATION present)\n\nconst LEVEL_PLAYLIST_REGEX_FAST = new RegExp(\n  [\n    /#EXTINF:\\s*(\\d*(?:\\.\\d+)?)(?:,(.*)\\s+)?/.source, // duration (#EXTINF:<duration>,<title>), group 1 => duration, group 2 => title\n    /(?!#) *(\\S[^\\r\\n]*)/.source, // segment URI, group 3 => the URI (note newline is not eaten)\n    /#EXT-X-BYTERANGE:*(.+)/.source, // next segment's byterange, group 4 => range spec (x@y)\n    /#EXT-X-PROGRAM-DATE-TIME:(.+)/.source, // next segment's program date/time group 5 => the datetime spec\n    /#.*/.source, // All other non-segment oriented tags will match with all groups empty\n  ].join('|'),\n  'g',\n);\n\nconst LEVEL_PLAYLIST_REGEX_SLOW = new RegExp(\n  [\n    /#(EXTM3U)/.source,\n    /#EXT-X-(DATERANGE|DEFINE|KEY|MAP|PART|PART-INF|PLAYLIST-TYPE|PRELOAD-HINT|RENDITION-REPORT|SERVER-CONTROL|SKIP|START):(.+)/\n      .source,\n    /#EXT-X-(BITRATE|DISCONTINUITY-SEQUENCE|MEDIA-SEQUENCE|TARGETDURATION|VERSION): *(\\d+)/\n      .source,\n    /#EXT-X-(DISCONTINUITY|ENDLIST|GAP|INDEPENDENT-SEGMENTS)/.source,\n    /(#)([^:]*):(.*)/.source,\n    /(#)(.*)(?:.*)\\r?\\n?/.source,\n  ].join('|'),\n);\n\nexport default class M3U8Parser {\n  static findGroup(\n    groups: (\n      | { id?: string; audioCodec?: string }\n      | { id?: string; textCodec?: string }\n    )[],\n    mediaGroupId: string,\n  ):\n    | { id?: string; audioCodec?: string }\n    | { id?: string; textCodec?: string }\n    | undefined {\n    for (let i = 0; i < groups.length; i++) {\n      const group = groups[i];\n      if (group.id === mediaGroupId) {\n        return group;\n      }\n    }\n  }\n\n  static resolve(url, baseUrl) {\n    return buildAbsoluteURL(baseUrl, url, { alwaysNormalize: true });\n  }\n\n  static isMediaPlaylist(str: string): boolean {\n    return IS_MEDIA_PLAYLIST.test(str);\n  }\n\n  static parseMasterPlaylist(\n    string: string,\n    baseurl: string,\n  ): ParsedMultivariantPlaylist {\n    const hasVariableRefs = __USE_VARIABLE_SUBSTITUTION__\n      ? hasVariableReferences(string)\n      : false;\n    const parsed: ParsedMultivariantPlaylist = {\n      contentSteering: null,\n      levels: [],\n      playlistParsingError: null,\n      sessionData: null,\n      sessionKeys: null,\n      startTimeOffset: null,\n      variableList: null,\n      hasVariableRefs,\n    };\n    const levelsWithKnownCodecs: LevelParsed[] = [];\n\n    MASTER_PLAYLIST_REGEX.lastIndex = 0;\n\n    let result: RegExpExecArray | null;\n    while ((result = MASTER_PLAYLIST_REGEX.exec(string)) != null) {\n      if (result[1]) {\n        // '#EXT-X-STREAM-INF' is found, parse level tag  in group 1\n        const attrs = new AttrList(result[1]) as LevelAttributes;\n        if (__USE_VARIABLE_SUBSTITUTION__) {\n          substituteVariablesInAttributes(parsed, attrs, [\n            'CODECS',\n            'SUPPLEMENTAL-CODECS',\n            'ALLOWED-CPC',\n            'PATHWAY-ID',\n            'STABLE-VARIANT-ID',\n            'AUDIO',\n            'VIDEO',\n            'SUBTITLES',\n            'CLOSED-CAPTIONS',\n            'NAME',\n          ]);\n        }\n        const uri = __USE_VARIABLE_SUBSTITUTION__\n          ? substituteVariables(parsed, result[2])\n          : result[2];\n        const level: LevelParsed = {\n          attrs,\n          bitrate:\n            attrs.decimalInteger('BANDWIDTH') ||\n            attrs.decimalInteger('AVERAGE-BANDWIDTH'),\n          name: attrs.NAME,\n          url: M3U8Parser.resolve(uri, baseurl),\n        };\n\n        const resolution = attrs.decimalResolution('RESOLUTION');\n        if (resolution) {\n          level.width = resolution.width;\n          level.height = resolution.height;\n        }\n\n        setCodecs(attrs.CODECS, level);\n\n        if (!level.unknownCodecs?.length) {\n          levelsWithKnownCodecs.push(level);\n        }\n\n        parsed.levels.push(level);\n      } else if (result[3]) {\n        const tag = result[3];\n        const attributes = result[4];\n        switch (tag) {\n          case 'SESSION-DATA': {\n            // #EXT-X-SESSION-DATA\n            const sessionAttrs = new AttrList(attributes);\n            if (__USE_VARIABLE_SUBSTITUTION__) {\n              substituteVariablesInAttributes(parsed, sessionAttrs, [\n                'DATA-ID',\n                'LANGUAGE',\n                'VALUE',\n                'URI',\n              ]);\n            }\n            const dataId = sessionAttrs['DATA-ID'];\n            if (dataId) {\n              if (parsed.sessionData === null) {\n                parsed.sessionData = {};\n              }\n              parsed.sessionData[dataId] = sessionAttrs;\n            }\n            break;\n          }\n          case 'SESSION-KEY': {\n            // #EXT-X-SESSION-KEY\n            const sessionKey = parseKey(attributes, baseurl, parsed);\n            if (sessionKey.encrypted && sessionKey.isSupported()) {\n              if (parsed.sessionKeys === null) {\n                parsed.sessionKeys = [];\n              }\n              parsed.sessionKeys.push(sessionKey);\n            } else {\n              logger.warn(\n                `[Keys] Ignoring invalid EXT-X-SESSION-KEY tag: \"${attributes}\"`,\n              );\n            }\n            break;\n          }\n          case 'DEFINE': {\n            // #EXT-X-DEFINE\n            if (__USE_VARIABLE_SUBSTITUTION__) {\n              const variableAttributes = new AttrList(attributes);\n              substituteVariablesInAttributes(parsed, variableAttributes, [\n                'NAME',\n                'VALUE',\n                'QUERYPARAM',\n              ]);\n              addVariableDefinition(parsed, variableAttributes, baseurl);\n            }\n            break;\n          }\n          case 'CONTENT-STEERING': {\n            // #EXT-X-CONTENT-STEERING\n            const contentSteeringAttributes = new AttrList(attributes);\n            if (__USE_VARIABLE_SUBSTITUTION__) {\n              substituteVariablesInAttributes(\n                parsed,\n                contentSteeringAttributes,\n                ['SERVER-URI', 'PATHWAY-ID'],\n              );\n            }\n            parsed.contentSteering = {\n              uri: M3U8Parser.resolve(\n                contentSteeringAttributes['SERVER-URI'],\n                baseurl,\n              ),\n              pathwayId: contentSteeringAttributes['PATHWAY-ID'] || '.',\n            };\n            break;\n          }\n          case 'START': {\n            // #EXT-X-START\n            parsed.startTimeOffset = parseStartTimeOffset(attributes);\n            break;\n          }\n          default:\n            break;\n        }\n      }\n    }\n    // Filter out levels with unknown codecs if it does not remove all levels\n    const stripUnknownCodecLevels =\n      levelsWithKnownCodecs.length > 0 &&\n      levelsWithKnownCodecs.length < parsed.levels.length;\n\n    parsed.levels = stripUnknownCodecLevels\n      ? levelsWithKnownCodecs\n      : parsed.levels;\n    if (parsed.levels.length === 0) {\n      parsed.playlistParsingError = new Error('no levels found in manifest');\n    }\n\n    return parsed;\n  }\n\n  static parseMasterPlaylistMedia(\n    string: string,\n    baseurl: string,\n    parsed: ParsedMultivariantPlaylist,\n  ): ParsedMultivariantMediaOptions {\n    let result: RegExpExecArray | null;\n    const results: ParsedMultivariantMediaOptions = {};\n    const levels = parsed.levels;\n    const groupsByType = {\n      AUDIO: levels.map((level: LevelParsed) => ({\n        id: level.attrs.AUDIO,\n        audioCodec: level.audioCodec,\n      })),\n      SUBTITLES: levels.map((level: LevelParsed) => ({\n        id: level.attrs.SUBTITLES,\n        textCodec: level.textCodec,\n      })),\n      'CLOSED-CAPTIONS': [],\n    };\n    let id = 0;\n    MASTER_PLAYLIST_MEDIA_REGEX.lastIndex = 0;\n    while ((result = MASTER_PLAYLIST_MEDIA_REGEX.exec(string)) !== null) {\n      const attrs = new AttrList(result[1]) as MediaAttributes;\n      const type = attrs.TYPE;\n      if (type) {\n        const groups: (typeof groupsByType)[keyof typeof groupsByType] =\n          groupsByType[type];\n        const medias: MediaPlaylist[] = results[type] || [];\n        results[type] = medias;\n        if (__USE_VARIABLE_SUBSTITUTION__) {\n          substituteVariablesInAttributes(parsed, attrs, [\n            'URI',\n            'GROUP-ID',\n            'LANGUAGE',\n            'ASSOC-LANGUAGE',\n            'STABLE-RENDITION-ID',\n            'NAME',\n            'INSTREAM-ID',\n            'CHARACTERISTICS',\n            'CHANNELS',\n          ]);\n        }\n        const lang = attrs.LANGUAGE;\n        const assocLang = attrs['ASSOC-LANGUAGE'];\n        const channels = attrs.CHANNELS;\n        const characteristics = attrs.CHARACTERISTICS;\n        const instreamId = attrs['INSTREAM-ID'];\n        const media: MediaPlaylist = {\n          attrs,\n          bitrate: 0,\n          id: id++,\n          groupId: attrs['GROUP-ID'] || '',\n          name: attrs.NAME || lang || '',\n          type,\n          default: attrs.bool('DEFAULT'),\n          autoselect: attrs.bool('AUTOSELECT'),\n          forced: attrs.bool('FORCED'),\n          lang,\n          url: attrs.URI ? M3U8Parser.resolve(attrs.URI, baseurl) : '',\n        };\n        if (assocLang) {\n          media.assocLang = assocLang;\n        }\n        if (channels) {\n          media.channels = channels;\n        }\n        if (characteristics) {\n          media.characteristics = characteristics;\n        }\n        if (instreamId) {\n          media.instreamId = instreamId;\n        }\n\n        if (groups?.length) {\n          // If there are audio or text groups signalled in the manifest, let's look for a matching codec string for this track\n          // If we don't find the track signalled, lets use the first audio groups codec we have\n          // Acting as a best guess\n          const groupCodec =\n            M3U8Parser.findGroup(groups, media.groupId as string) || groups[0];\n          assignCodec(media, groupCodec, 'audioCodec');\n          assignCodec(media, groupCodec, 'textCodec');\n        }\n\n        medias.push(media);\n      }\n    }\n    return results;\n  }\n\n  static parseLevelPlaylist(\n    string: string,\n    baseurl: string,\n    id: number,\n    type: PlaylistLevelType,\n    levelUrlId: number,\n    multivariantVariableList: VariableMap | null,\n  ): LevelDetails {\n    const level = new LevelDetails(baseurl);\n    const fragments: M3U8ParserFragments = level.fragments;\n    // The most recent init segment seen (applies to all subsequent segments)\n    let currentInitSegment: Fragment | null = null;\n    let currentSN = 0;\n    let currentPart = 0;\n    let totalduration = 0;\n    let discontinuityCounter = 0;\n    let prevFrag: Fragment | null = null;\n    let frag: Fragment = new Fragment(type, baseurl);\n    let result: RegExpExecArray | RegExpMatchArray | null;\n    let i: number;\n    let levelkeys: { [key: string]: LevelKey } | undefined;\n    let firstPdtIndex = -1;\n    let createNextFrag = false;\n    let nextByteRange: string | null = null;\n\n    LEVEL_PLAYLIST_REGEX_FAST.lastIndex = 0;\n    level.m3u8 = string;\n    level.hasVariableRefs = __USE_VARIABLE_SUBSTITUTION__\n      ? hasVariableReferences(string)\n      : false;\n\n    while ((result = LEVEL_PLAYLIST_REGEX_FAST.exec(string)) !== null) {\n      if (createNextFrag) {\n        createNextFrag = false;\n        frag = new Fragment(type, baseurl);\n        // setup the next fragment for part loading\n        frag.start = totalduration;\n        frag.sn = currentSN;\n        frag.cc = discontinuityCounter;\n        frag.level = id;\n        if (currentInitSegment) {\n          frag.initSegment = currentInitSegment;\n          frag.rawProgramDateTime = currentInitSegment.rawProgramDateTime;\n          currentInitSegment.rawProgramDateTime = null;\n          if (nextByteRange) {\n            frag.setByteRange(nextByteRange);\n            nextByteRange = null;\n          }\n        }\n      }\n\n      const duration = result[1];\n      if (duration) {\n        // INF\n        frag.duration = parseFloat(duration);\n        // avoid sliced strings    https://github.com/video-dev/hls.js/issues/939\n        const title = (' ' + result[2]).slice(1);\n        frag.title = title || null;\n        frag.tagList.push(title ? ['INF', duration, title] : ['INF', duration]);\n      } else if (result[3]) {\n        // url\n        if (Number.isFinite(frag.duration)) {\n          frag.start = totalduration;\n          if (levelkeys) {\n            setFragLevelKeys(frag, levelkeys, level);\n          }\n          frag.sn = currentSN;\n          frag.level = id;\n          frag.cc = discontinuityCounter;\n          fragments.push(frag);\n          // avoid sliced strings    https://github.com/video-dev/hls.js/issues/939\n          const uri = (' ' + result[3]).slice(1);\n          frag.relurl = __USE_VARIABLE_SUBSTITUTION__\n            ? substituteVariables(level, uri)\n            : uri;\n          assignProgramDateTime(frag, prevFrag);\n          prevFrag = frag;\n          totalduration += frag.duration;\n          currentSN++;\n          currentPart = 0;\n          createNextFrag = true;\n        }\n      } else if (result[4]) {\n        // X-BYTERANGE\n        const data = (' ' + result[4]).slice(1);\n        if (prevFrag) {\n          frag.setByteRange(data, prevFrag);\n        } else {\n          frag.setByteRange(data);\n        }\n      } else if (result[5]) {\n        // PROGRAM-DATE-TIME\n        // avoid sliced strings    https://github.com/video-dev/hls.js/issues/939\n        frag.rawProgramDateTime = (' ' + result[5]).slice(1);\n        frag.tagList.push(['PROGRAM-DATE-TIME', frag.rawProgramDateTime]);\n        if (firstPdtIndex === -1) {\n          firstPdtIndex = fragments.length;\n        }\n      } else {\n        result = result[0].match(LEVEL_PLAYLIST_REGEX_SLOW);\n        if (!result) {\n          logger.warn('No matches on slow regex match for level playlist!');\n          continue;\n        }\n        for (i = 1; i < result.length; i++) {\n          if (typeof result[i] !== 'undefined') {\n            break;\n          }\n        }\n\n        // avoid sliced strings    https://github.com/video-dev/hls.js/issues/939\n        const tag = (' ' + result[i]).slice(1);\n        const value1 = (' ' + result[i + 1]).slice(1);\n        const value2 = result[i + 2] ? (' ' + result[i + 2]).slice(1) : '';\n\n        switch (tag) {\n          case 'PLAYLIST-TYPE':\n            level.type = value1.toUpperCase();\n            break;\n          case 'MEDIA-SEQUENCE':\n            currentSN = level.startSN = parseInt(value1);\n            break;\n          case 'SKIP': {\n            const skipAttrs = new AttrList(value1);\n            if (__USE_VARIABLE_SUBSTITUTION__) {\n              substituteVariablesInAttributes(level, skipAttrs, [\n                'RECENTLY-REMOVED-DATERANGES',\n              ]);\n            }\n            const skippedSegments =\n              skipAttrs.decimalInteger('SKIPPED-SEGMENTS');\n            if (Number.isFinite(skippedSegments)) {\n              level.skippedSegments = skippedSegments;\n              // This will result in fragments[] containing undefined values, which we will fill in with `mergeDetails`\n              for (let i = skippedSegments; i--; ) {\n                fragments.unshift(null);\n              }\n              currentSN += skippedSegments;\n            }\n            const recentlyRemovedDateranges = skipAttrs.enumeratedString(\n              'RECENTLY-REMOVED-DATERANGES',\n            );\n            if (recentlyRemovedDateranges) {\n              level.recentlyRemovedDateranges =\n                recentlyRemovedDateranges.split('\\t');\n            }\n            break;\n          }\n          case 'TARGETDURATION':\n            level.targetduration = Math.max(parseInt(value1), 1);\n            break;\n          case 'VERSION':\n            level.version = parseInt(value1);\n            break;\n          case 'INDEPENDENT-SEGMENTS':\n          case 'EXTM3U':\n            break;\n          case 'ENDLIST':\n            level.live = false;\n            break;\n          case '#':\n            if (value1 || value2) {\n              frag.tagList.push(value2 ? [value1, value2] : [value1]);\n            }\n            break;\n          case 'DISCONTINUITY':\n            discontinuityCounter++;\n            frag.tagList.push(['DIS']);\n            break;\n          case 'GAP':\n            frag.gap = true;\n            frag.tagList.push([tag]);\n            break;\n          case 'BITRATE':\n            frag.tagList.push([tag, value1]);\n            break;\n          case 'DATERANGE': {\n            const dateRangeAttr = new AttrList(value1);\n            if (__USE_VARIABLE_SUBSTITUTION__) {\n              substituteVariablesInAttributes(level, dateRangeAttr, [\n                'ID',\n                'CLASS',\n                'START-DATE',\n                'END-DATE',\n                'SCTE35-CMD',\n                'SCTE35-OUT',\n                'SCTE35-IN',\n              ]);\n              substituteVariablesInAttributes(\n                level,\n                dateRangeAttr,\n                dateRangeAttr.clientAttrs,\n              );\n            }\n            const dateRange = new DateRange(\n              dateRangeAttr,\n              level.dateRanges[dateRangeAttr.ID],\n            );\n            if (dateRange.isValid || level.skippedSegments) {\n              level.dateRanges[dateRange.id] = dateRange;\n            } else {\n              logger.warn(`Ignoring invalid DATERANGE tag: \"${value1}\"`);\n            }\n            // Add to fragment tag list for backwards compatibility (< v1.2.0)\n            frag.tagList.push(['EXT-X-DATERANGE', value1]);\n            break;\n          }\n          case 'DEFINE': {\n            if (__USE_VARIABLE_SUBSTITUTION__) {\n              const variableAttributes = new AttrList(value1);\n              substituteVariablesInAttributes(level, variableAttributes, [\n                'NAME',\n                'VALUE',\n                'IMPORT',\n                'QUERYPARAM',\n              ]);\n              if ('IMPORT' in variableAttributes) {\n                importVariableDefinition(\n                  level,\n                  variableAttributes,\n                  multivariantVariableList,\n                );\n              } else {\n                addVariableDefinition(level, variableAttributes, baseurl);\n              }\n            }\n            break;\n          }\n\n          case 'DISCONTINUITY-SEQUENCE':\n            discontinuityCounter = parseInt(value1);\n            break;\n          case 'KEY': {\n            const levelKey = parseKey(value1, baseurl, level);\n            if (levelKey.isSupported()) {\n              if (levelKey.method === 'NONE') {\n                levelkeys = undefined;\n                break;\n              }\n              if (!levelkeys) {\n                levelkeys = {};\n              }\n              if (levelkeys[levelKey.keyFormat]) {\n                levelkeys = Object.assign({}, levelkeys);\n              }\n              levelkeys[levelKey.keyFormat] = levelKey;\n            } else {\n              logger.warn(`[Keys] Ignoring invalid EXT-X-KEY tag: \"${value1}\"`);\n            }\n            break;\n          }\n          case 'START':\n            level.startTimeOffset = parseStartTimeOffset(value1);\n            break;\n          case 'MAP': {\n            const mapAttrs = new AttrList(value1);\n            if (__USE_VARIABLE_SUBSTITUTION__) {\n              substituteVariablesInAttributes(level, mapAttrs, [\n                'BYTERANGE',\n                'URI',\n              ]);\n            }\n            if (frag.duration) {\n              // Initial segment tag is after segment duration tag.\n              //   #EXTINF: 6.0\n              //   #EXT-X-MAP:URI=\"init.mp4\n              const init = new Fragment(type, baseurl);\n              setInitSegment(init, mapAttrs, id, levelkeys);\n              currentInitSegment = init;\n              frag.initSegment = currentInitSegment;\n              if (\n                currentInitSegment.rawProgramDateTime &&\n                !frag.rawProgramDateTime\n              ) {\n                frag.rawProgramDateTime = currentInitSegment.rawProgramDateTime;\n              }\n            } else {\n              // Initial segment tag is before segment duration tag\n              // Handle case where EXT-X-MAP is declared after EXT-X-BYTERANGE\n              const end = frag.byteRangeEndOffset;\n              if (end) {\n                const start = frag.byteRangeStartOffset as number;\n                nextByteRange = `${end - start}@${start}`;\n              } else {\n                nextByteRange = null;\n              }\n              setInitSegment(frag, mapAttrs, id, levelkeys);\n              currentInitSegment = frag;\n              createNextFrag = true;\n            }\n            break;\n          }\n          case 'SERVER-CONTROL': {\n            const serverControlAttrs = new AttrList(value1);\n            level.canBlockReload = serverControlAttrs.bool('CAN-BLOCK-RELOAD');\n            level.canSkipUntil = serverControlAttrs.optionalFloat(\n              'CAN-SKIP-UNTIL',\n              0,\n            );\n            level.canSkipDateRanges =\n              level.canSkipUntil > 0 &&\n              serverControlAttrs.bool('CAN-SKIP-DATERANGES');\n            level.partHoldBack = serverControlAttrs.optionalFloat(\n              'PART-HOLD-BACK',\n              0,\n            );\n            level.holdBack = serverControlAttrs.optionalFloat('HOLD-BACK', 0);\n            break;\n          }\n          case 'PART-INF': {\n            const partInfAttrs = new AttrList(value1);\n            level.partTarget = partInfAttrs.decimalFloatingPoint('PART-TARGET');\n            break;\n          }\n          case 'PART': {\n            let partList = level.partList;\n            if (!partList) {\n              partList = level.partList = [];\n            }\n            const previousFragmentPart =\n              currentPart > 0 ? partList[partList.length - 1] : undefined;\n            const index = currentPart++;\n            const partAttrs = new AttrList(value1);\n            if (__USE_VARIABLE_SUBSTITUTION__) {\n              substituteVariablesInAttributes(level, partAttrs, [\n                'BYTERANGE',\n                'URI',\n              ]);\n            }\n            const part = new Part(\n              partAttrs,\n              frag,\n              baseurl,\n              index,\n              previousFragmentPart,\n            );\n            partList.push(part);\n            frag.duration += part.duration;\n            break;\n          }\n          case 'PRELOAD-HINT': {\n            const preloadHintAttrs = new AttrList(value1);\n            if (__USE_VARIABLE_SUBSTITUTION__) {\n              substituteVariablesInAttributes(level, preloadHintAttrs, ['URI']);\n            }\n            level.preloadHint = preloadHintAttrs;\n            break;\n          }\n          case 'RENDITION-REPORT': {\n            const renditionReportAttrs = new AttrList(value1);\n            if (__USE_VARIABLE_SUBSTITUTION__) {\n              substituteVariablesInAttributes(level, renditionReportAttrs, [\n                'URI',\n              ]);\n            }\n            level.renditionReports = level.renditionReports || [];\n            level.renditionReports.push(renditionReportAttrs);\n            break;\n          }\n          default:\n            logger.warn(`line parsed but not handled: ${result}`);\n            break;\n        }\n      }\n    }\n    if (prevFrag && !prevFrag.relurl) {\n      fragments.pop();\n      totalduration -= prevFrag.duration;\n      if (level.partList) {\n        level.fragmentHint = prevFrag;\n      }\n    } else if (level.partList) {\n      assignProgramDateTime(frag, prevFrag);\n      frag.cc = discontinuityCounter;\n      level.fragmentHint = frag;\n      if (levelkeys) {\n        setFragLevelKeys(frag, levelkeys, level);\n      }\n    }\n    const fragmentLength = fragments.length;\n    const firstFragment = fragments[0];\n    const lastFragment = fragments[fragmentLength - 1];\n    totalduration += level.skippedSegments * level.targetduration;\n    if (totalduration > 0 && fragmentLength && lastFragment) {\n      level.averagetargetduration = totalduration / fragmentLength;\n      const lastSn = lastFragment.sn;\n      level.endSN = lastSn !== 'initSegment' ? lastSn : 0;\n      if (!level.live) {\n        lastFragment.endList = true;\n      }\n      if (firstFragment) {\n        level.startCC = firstFragment.cc;\n      }\n    } else {\n      level.endSN = 0;\n      level.startCC = 0;\n    }\n    if (level.fragmentHint) {\n      totalduration += level.fragmentHint.duration;\n    }\n    level.totalduration = totalduration;\n    level.endCC = discontinuityCounter;\n\n    /**\n     * Backfill any missing PDT values\n     * \"If the first EXT-X-PROGRAM-DATE-TIME tag in a Playlist appears after\n     * one or more Media Segment URIs, the client SHOULD extrapolate\n     * backward from that tag (using EXTINF durations and/or media\n     * timestamps) to associate dates with those segments.\"\n     * We have already extrapolated forward, but all fragments up to the first instance of PDT do not have their PDTs\n     * computed.\n     */\n    if (firstPdtIndex > 0) {\n      backfillProgramDateTimes(fragments, firstPdtIndex);\n    }\n\n    return level;\n  }\n}\n\nfunction parseKey(\n  keyTagAttributes: string,\n  baseurl: string,\n  parsed: ParsedMultivariantPlaylist | LevelDetails,\n): LevelKey {\n  // https://tools.ietf.org/html/rfc8216#section-4.3.2.4\n  const keyAttrs = new AttrList(keyTagAttributes);\n  if (__USE_VARIABLE_SUBSTITUTION__) {\n    substituteVariablesInAttributes(parsed, keyAttrs, [\n      'KEYFORMAT',\n      'KEYFORMATVERSIONS',\n      'URI',\n      'IV',\n      'URI',\n    ]);\n  }\n  const decryptmethod = keyAttrs.METHOD ?? '';\n  const decrypturi = keyAttrs.URI;\n  const decryptiv = keyAttrs.hexadecimalInteger('IV');\n  const decryptkeyformatversions = keyAttrs.KEYFORMATVERSIONS;\n  // From RFC: This attribute is OPTIONAL; its absence indicates an implicit value of \"identity\".\n  const decryptkeyformat = keyAttrs.KEYFORMAT ?? 'identity';\n\n  if (decrypturi && keyAttrs.IV && !decryptiv) {\n    logger.error(`Invalid IV: ${keyAttrs.IV}`);\n  }\n  // If decrypturi is a URI with a scheme, then baseurl will be ignored\n  // No uri is allowed when METHOD is NONE\n  const resolvedUri = decrypturi ? M3U8Parser.resolve(decrypturi, baseurl) : '';\n  const keyFormatVersions = (\n    decryptkeyformatversions ? decryptkeyformatversions : '1'\n  )\n    .split('/')\n    .map(Number)\n    .filter(Number.isFinite);\n\n  return new LevelKey(\n    decryptmethod,\n    resolvedUri,\n    decryptkeyformat,\n    keyFormatVersions,\n    decryptiv,\n  );\n}\n\nfunction parseStartTimeOffset(startAttributes: string): number | null {\n  const startAttrs = new AttrList(startAttributes);\n  const startTimeOffset = startAttrs.decimalFloatingPoint('TIME-OFFSET');\n  if (Number.isFinite(startTimeOffset)) {\n    return startTimeOffset;\n  }\n  return null;\n}\n\nfunction setCodecs(\n  codecsAttributeValue: string | undefined,\n  level: LevelParsed,\n) {\n  let codecs = (codecsAttributeValue || '').split(/[ ,]+/).filter((c) => c);\n  ['video', 'audio', 'text'].forEach((type: CodecType) => {\n    const filtered = codecs.filter((codec) => isCodecType(codec, type));\n    if (filtered.length) {\n      // Comma separated list of all codecs for type\n      level[`${type}Codec`] = filtered.join(',');\n      // Remove known codecs so that only unknownCodecs are left after iterating through each type\n      codecs = codecs.filter((codec) => filtered.indexOf(codec) === -1);\n    }\n  });\n  level.unknownCodecs = codecs;\n}\n\nfunction assignCodec(\n  media: MediaPlaylist,\n  groupItem: { audioCodec?: string; textCodec?: string },\n  codecProperty: 'audioCodec' | 'textCodec',\n) {\n  const codecValue = groupItem[codecProperty];\n  if (codecValue) {\n    media[codecProperty] = codecValue;\n  }\n}\n\nfunction backfillProgramDateTimes(\n  fragments: M3U8ParserFragments,\n  firstPdtIndex: number,\n) {\n  let fragPrev = fragments[firstPdtIndex] as Fragment;\n  for (let i = firstPdtIndex; i--; ) {\n    const frag = fragments[i];\n    // Exit on delta-playlist skipped segments\n    if (!frag) {\n      return;\n    }\n    frag.programDateTime =\n      (fragPrev.programDateTime as number) - frag.duration * 1000;\n    fragPrev = frag;\n  }\n}\n\nfunction assignProgramDateTime(frag, prevFrag) {\n  if (frag.rawProgramDateTime) {\n    frag.programDateTime = Date.parse(frag.rawProgramDateTime);\n  } else if (prevFrag?.programDateTime) {\n    frag.programDateTime = prevFrag.endProgramDateTime;\n  }\n\n  if (!Number.isFinite(frag.programDateTime)) {\n    frag.programDateTime = null;\n    frag.rawProgramDateTime = null;\n  }\n}\n\nfunction setInitSegment(\n  frag: Fragment,\n  mapAttrs: AttrList,\n  id: number,\n  levelkeys: { [key: string]: LevelKey } | undefined,\n) {\n  frag.relurl = mapAttrs.URI;\n  if (mapAttrs.BYTERANGE) {\n    frag.setByteRange(mapAttrs.BYTERANGE);\n  }\n  frag.level = id;\n  frag.sn = 'initSegment';\n  if (levelkeys) {\n    frag.levelkeys = levelkeys;\n  }\n  frag.initSegment = null;\n}\n\nfunction setFragLevelKeys(\n  frag: Fragment,\n  levelkeys: { [key: string]: LevelKey },\n  level: LevelDetails,\n) {\n  frag.levelkeys = levelkeys;\n  const { encryptedFragments } = level;\n  if (\n    (!encryptedFragments.length ||\n      encryptedFragments[encryptedFragments.length - 1].levelkeys !==\n        levelkeys) &&\n    Object.keys(levelkeys).some(\n      (format) => levelkeys![format].isCommonEncryption,\n    )\n  ) {\n    encryptedFragments.push(frag);\n  }\n}\n", "import type { LoaderConfig } from '../config';\nimport type { Fragment } from '../loader/fragment';\nimport type { Part } from '../loader/fragment';\nimport type { KeyLoaderInfo } from '../loader/key-loader';\nimport type { LevelDetails } from '../loader/level-details';\nimport type { HlsUrlParameters } from './level';\n\nexport interface LoaderContext {\n  // target URL\n  url: string;\n  // loader response type (arraybuffer or default response type for playlist)\n  responseType: string;\n  // headers\n  headers?: Record<string, string>;\n  // start byte range offset\n  rangeStart?: number;\n  // end byte range offset\n  rangeEnd?: number;\n  // true if onProgress should report partial chunk of loaded content\n  progressData?: boolean;\n}\n\nexport interface FragmentLoaderContext extends LoaderContext {\n  frag: Fragment;\n  part: Part | null;\n  resetIV?: boolean;\n}\n\nexport interface KeyLoaderContext extends LoaderContext {\n  keyInfo: KeyLoaderInfo;\n  frag: Fragment;\n}\n\nexport interface LoaderConfiguration {\n  // LoaderConfig policy that overrides required settings\n  loadPolicy: LoaderConfig;\n  /**\n   * @deprecated use LoaderConfig timeoutRetry and errorRetry maxNumRetry\n   */\n  // Max number of load retries\n  maxRetry: number;\n  /**\n   * @deprecated use LoaderConfig maxTimeToFirstByteMs and maxLoadTimeMs\n   */\n  // Timeout after which `onTimeOut` callback will be triggered\n  //  when loading has not finished after that delay\n  timeout: number;\n  /**\n   * @deprecated use LoaderConfig timeoutRetry and errorRetry retryDelayMs\n   */\n  // Delay between an I/O error and following connection retry (ms).\n  // This to avoid spamming the server\n  retryDelay: number;\n  /**\n   * @deprecated use LoaderConfig timeoutRetry and errorRetry maxRetryDelayMs\n   */\n  // max connection retry delay (ms)\n  maxRetryDelay: number;\n  // When streaming progressively, this is the minimum chunk size required to emit a PROGRESS event\n  highWaterMark?: number;\n}\n\nexport interface LoaderResponse {\n  url: string;\n  data?: string | ArrayBuffer | Object;\n  // Errors can include HTTP status code and error message\n  // Successful responses should include status code 200\n  code?: number;\n  text?: string;\n}\n\nexport interface LoaderStats {\n  aborted: boolean;\n  loaded: number;\n  retry: number;\n  total: number;\n  chunkCount: number;\n  bwEstimate: number;\n  loading: HlsProgressivePerformanceTiming;\n  parsing: HlsPerformanceTiming;\n  buffering: HlsProgressivePerformanceTiming;\n}\n\nexport interface HlsPerformanceTiming {\n  start: number;\n  end: number;\n}\n\nexport interface HlsChunkPerformanceTiming extends HlsPerformanceTiming {\n  executeStart: number;\n  executeEnd: number;\n}\n\nexport interface HlsProgressivePerformanceTiming extends HlsPerformanceTiming {\n  first: number;\n}\n\nexport type LoaderOnSuccess<T extends LoaderContext> = (\n  response: LoaderResponse,\n  stats: LoaderStats,\n  context: T,\n  networkDetails: any,\n) => void;\n\nexport type LoaderOnProgress<T extends LoaderContext> = (\n  stats: LoaderStats,\n  context: T,\n  data: string | ArrayBuffer,\n  networkDetails: any,\n) => void;\n\nexport type LoaderOnError<T extends LoaderContext> = (\n  error: {\n    // error status code\n    code: number;\n    // error description\n    text: string;\n  },\n  context: T,\n  networkDetails: any,\n  stats: LoaderStats,\n) => void;\n\nexport type LoaderOnTimeout<T extends LoaderContext> = (\n  stats: LoaderStats,\n  context: T,\n  networkDetails: any,\n) => void;\n\nexport type LoaderOnAbort<T extends LoaderContext> = (\n  stats: LoaderStats,\n  context: T,\n  networkDetails: any,\n) => void;\n\nexport interface LoaderCallbacks<T extends LoaderContext> {\n  onSuccess: LoaderOnSuccess<T>;\n  onError: LoaderOnError<T>;\n  onTimeout: LoaderOnTimeout<T>;\n  onAbort?: LoaderOnAbort<T>;\n  onProgress?: LoaderOnProgress<T>;\n}\n\nexport interface Loader<T extends LoaderContext> {\n  destroy(): void;\n  abort(): void;\n  load(\n    context: T,\n    config: LoaderConfiguration,\n    callbacks: LoaderCallbacks<T>,\n  ): void;\n  /**\n   * `getCacheAge()` is called by hls.js to get the duration that a given object\n   * has been sitting in a cache proxy when playing live.  If implemented,\n   * this should return a value in seconds.\n   *\n   * For HTTP based loaders, this should return the contents of the \"age\" header.\n   *\n   * @returns time object being lodaded\n   */\n  getCacheAge?: () => number | null;\n  getResponseHeader?: (name: string) => string | null;\n  context: T | null;\n  stats: LoaderStats;\n}\n\nexport const enum PlaylistContextType {\n  MANIFEST = 'manifest',\n  LEVEL = 'level',\n  AUDIO_TRACK = 'audioTrack',\n  SUBTITLE_TRACK = 'subtitleTrack',\n}\n\nexport const enum PlaylistLevelType {\n  MAIN = 'main',\n  AUDIO = 'audio',\n  SUBTITLE = 'subtitle',\n}\n\nexport interface PlaylistLoaderContext extends LoaderContext {\n  type: PlaylistContextType;\n  // the level index to load\n  level: number | null;\n  // level or track id from LevelLoadingData / TrackLoadingData\n  id: number | null;\n  // Media Playlist Group ID\n  groupId?: string;\n  // Content Steering Pathway ID (or undefined for default Pathway \".\")\n  pathwayId?: string;\n  // internal representation of a parsed m3u8 level playlist\n  levelDetails?: LevelDetails;\n  // Blocking playlist request delivery directives (or null id none were added to playlist url\n  deliveryDirectives: HlsUrlParameters | null;\n}\n", "/**\n * PlaylistLoader - delegate for media manifest/playlist loading tasks. Takes care of parsing media to internal data-models.\n *\n * Once loaded, dispatches events with parsed data-models of manifest/levels/audio/subtitle tracks.\n *\n * Uses loader(s) set in config to do actual internal loading of resource tasks.\n */\n\nimport { Events } from '../events';\nimport { ErrorDetails, ErrorTypes } from '../errors';\nimport { logger } from '../utils/logger';\nimport M3U8Parser from './m3u8-parser';\nimport type { LevelParsed, VariableMap } from '../types/level';\nimport type {\n  Loader,\n  LoaderCallbacks,\n  LoaderConfiguration,\n  LoaderContext,\n  LoaderResponse,\n  LoaderStats,\n  PlaylistLoaderContext,\n} from '../types/loader';\nimport { PlaylistContextType, PlaylistLevelType } from '../types/loader';\nimport { LevelDetails } from './level-details';\nimport { AttrList } from '../utils/attr-list';\nimport type Hls from '../hls';\nimport type {\n  ErrorData,\n  LevelLoadingData,\n  ManifestLoadingData,\n  TrackLoadingData,\n} from '../types/events';\nimport type { NetworkComponentAPI } from '../types/component-api';\nimport type { MediaAttributes } from '../types/media-playlist';\nimport type { LoaderConfig, RetryConfig } from '../config';\n\nfunction mapContextToLevelType(\n  context: PlaylistLoaderContext,\n): PlaylistLevelType {\n  const { type } = context;\n\n  switch (type) {\n    case PlaylistContextType.AUDIO_TRACK:\n      return PlaylistLevelType.AUDIO;\n    case PlaylistContextType.SUBTITLE_TRACK:\n      return PlaylistLevelType.SUBTITLE;\n    default:\n      return PlaylistLevelType.MAIN;\n  }\n}\n\nfunction getResponseUrl(\n  response: LoaderResponse,\n  context: PlaylistLoaderContext,\n): string {\n  let url = response.url;\n  // responseURL not supported on some browsers (it is used to detect URL redirection)\n  // data-uri mode also not supported (but no need to detect redirection)\n  if (url === undefined || url.indexOf('data:') === 0) {\n    // fallback to initial URL\n    url = context.url;\n  }\n  return url;\n}\n\nclass PlaylistLoader implements NetworkComponentAPI {\n  private readonly hls: Hls;\n  private readonly loaders: {\n    [key: string]: Loader<LoaderContext>;\n  } = Object.create(null);\n  private variableList: VariableMap | null = null;\n\n  constructor(hls: Hls) {\n    this.hls = hls;\n    this.registerListeners();\n  }\n\n  public startLoad(startPosition: number): void {}\n\n  public stopLoad(): void {\n    this.destroyInternalLoaders();\n  }\n\n  private registerListeners() {\n    const { hls } = this;\n    hls.on(Events.MANIFEST_LOADING, this.onManifestLoading, this);\n    hls.on(Events.LEVEL_LOADING, this.onLevelLoading, this);\n    hls.on(Events.AUDIO_TRACK_LOADING, this.onAudioTrackLoading, this);\n    hls.on(Events.SUBTITLE_TRACK_LOADING, this.onSubtitleTrackLoading, this);\n  }\n\n  private unregisterListeners() {\n    const { hls } = this;\n    hls.off(Events.MANIFEST_LOADING, this.onManifestLoading, this);\n    hls.off(Events.LEVEL_LOADING, this.onLevelLoading, this);\n    hls.off(Events.AUDIO_TRACK_LOADING, this.onAudioTrackLoading, this);\n    hls.off(Events.SUBTITLE_TRACK_LOADING, this.onSubtitleTrackLoading, this);\n  }\n\n  /**\n   * Returns defaults or configured loader-type overloads (pLoader and loader config params)\n   */\n  private createInternalLoader(\n    context: PlaylistLoaderContext,\n  ): Loader<LoaderContext> {\n    const config = this.hls.config;\n    const PLoader = config.pLoader;\n    const Loader = config.loader;\n    const InternalLoader = PLoader || Loader;\n    const loader = new InternalLoader(config) as Loader<PlaylistLoaderContext>;\n\n    this.loaders[context.type] = loader;\n    return loader;\n  }\n\n  private getInternalLoader(\n    context: PlaylistLoaderContext,\n  ): Loader<LoaderContext> | undefined {\n    return this.loaders[context.type];\n  }\n\n  private resetInternalLoader(contextType): void {\n    if (this.loaders[contextType]) {\n      delete this.loaders[contextType];\n    }\n  }\n\n  /**\n   * Call `destroy` on all internal loader instances mapped (one per context type)\n   */\n  private destroyInternalLoaders(): void {\n    for (const contextType in this.loaders) {\n      const loader = this.loaders[contextType];\n      if (loader) {\n        loader.destroy();\n      }\n\n      this.resetInternalLoader(contextType);\n    }\n  }\n\n  public destroy(): void {\n    this.variableList = null;\n    this.unregisterListeners();\n    this.destroyInternalLoaders();\n  }\n\n  private onManifestLoading(\n    event: Events.MANIFEST_LOADING,\n    data: ManifestLoadingData,\n  ) {\n    const { url } = data;\n    this.variableList = null;\n    this.load({\n      id: null,\n      level: 0,\n      responseType: 'text',\n      type: PlaylistContextType.MANIFEST,\n      url,\n      deliveryDirectives: null,\n    });\n  }\n\n  private onLevelLoading(event: Events.LEVEL_LOADING, data: LevelLoadingData) {\n    const { id, level, pathwayId, url, deliveryDirectives } = data;\n    this.load({\n      id,\n      level,\n      pathwayId,\n      responseType: 'text',\n      type: PlaylistContextType.LEVEL,\n      url,\n      deliveryDirectives,\n    });\n  }\n\n  private onAudioTrackLoading(\n    event: Events.AUDIO_TRACK_LOADING,\n    data: TrackLoadingData,\n  ) {\n    const { id, groupId, url, deliveryDirectives } = data;\n    this.load({\n      id,\n      groupId,\n      level: null,\n      responseType: 'text',\n      type: PlaylistContextType.AUDIO_TRACK,\n      url,\n      deliveryDirectives,\n    });\n  }\n\n  private onSubtitleTrackLoading(\n    event: Events.SUBTITLE_TRACK_LOADING,\n    data: TrackLoadingData,\n  ) {\n    const { id, groupId, url, deliveryDirectives } = data;\n    this.load({\n      id,\n      groupId,\n      level: null,\n      responseType: 'text',\n      type: PlaylistContextType.SUBTITLE_TRACK,\n      url,\n      deliveryDirectives,\n    });\n  }\n\n  private load(context: PlaylistLoaderContext): void {\n    const config = this.hls.config;\n\n    // logger.debug(`[playlist-loader]: Loading playlist of type ${context.type}, level: ${context.level}, id: ${context.id}`);\n\n    // Check if a loader for this context already exists\n    let loader = this.getInternalLoader(context);\n    if (loader) {\n      const loaderContext = loader.context as PlaylistLoaderContext;\n      if (\n        loaderContext &&\n        loaderContext.url === context.url &&\n        loaderContext.level === context.level\n      ) {\n        // same URL can't overlap\n        logger.trace('[playlist-loader]: playlist request ongoing');\n        return;\n      }\n      logger.log(\n        `[playlist-loader]: aborting previous loader for type: ${context.type}`,\n      );\n      loader.abort();\n    }\n\n    // apply different configs for retries depending on\n    // context (manifest, level, audio/subs playlist)\n    let loadPolicy: LoaderConfig;\n    if (context.type === PlaylistContextType.MANIFEST) {\n      loadPolicy = config.manifestLoadPolicy.default;\n    } else {\n      loadPolicy = Object.assign({}, config.playlistLoadPolicy.default, {\n        timeoutRetry: null,\n        errorRetry: null,\n      });\n    }\n    loader = this.createInternalLoader(context);\n\n    // Override level/track timeout for LL-HLS requests\n    // (the default of 10000ms is counter productive to blocking playlist reload requests)\n    if (Number.isFinite(context.deliveryDirectives?.part)) {\n      let levelDetails: LevelDetails | undefined;\n      if (\n        context.type === PlaylistContextType.LEVEL &&\n        context.level !== null\n      ) {\n        levelDetails = this.hls.levels[context.level].details;\n      } else if (\n        context.type === PlaylistContextType.AUDIO_TRACK &&\n        context.id !== null\n      ) {\n        levelDetails = this.hls.audioTracks[context.id].details;\n      } else if (\n        context.type === PlaylistContextType.SUBTITLE_TRACK &&\n        context.id !== null\n      ) {\n        levelDetails = this.hls.subtitleTracks[context.id].details;\n      }\n      if (levelDetails) {\n        const partTarget = levelDetails.partTarget;\n        const targetDuration = levelDetails.targetduration;\n        if (partTarget && targetDuration) {\n          const maxLowLatencyPlaylistRefresh =\n            Math.max(partTarget * 3, targetDuration * 0.8) * 1000;\n          loadPolicy = Object.assign({}, loadPolicy, {\n            maxTimeToFirstByteMs: Math.min(\n              maxLowLatencyPlaylistRefresh,\n              loadPolicy.maxTimeToFirstByteMs,\n            ),\n            maxLoadTimeMs: Math.min(\n              maxLowLatencyPlaylistRefresh,\n              loadPolicy.maxTimeToFirstByteMs,\n            ),\n          });\n        }\n      }\n    }\n\n    const legacyRetryCompatibility: RetryConfig | Record<string, void> =\n      loadPolicy.errorRetry || loadPolicy.timeoutRetry || {};\n    const loaderConfig: LoaderConfiguration = {\n      loadPolicy,\n      timeout: loadPolicy.maxLoadTimeMs,\n      maxRetry: legacyRetryCompatibility.maxNumRetry || 0,\n      retryDelay: legacyRetryCompatibility.retryDelayMs || 0,\n      maxRetryDelay: legacyRetryCompatibility.maxRetryDelayMs || 0,\n    };\n\n    const loaderCallbacks: LoaderCallbacks<PlaylistLoaderContext> = {\n      onSuccess: (response, stats, context, networkDetails) => {\n        const loader = this.getInternalLoader(context) as\n          | Loader<PlaylistLoaderContext>\n          | undefined;\n        this.resetInternalLoader(context.type);\n\n        const string = response.data as string;\n\n        // Validate if it is an M3U8 at all\n        if (string.indexOf('#EXTM3U') !== 0) {\n          this.handleManifestParsingError(\n            response,\n            context,\n            new Error('no EXTM3U delimiter'),\n            networkDetails || null,\n            stats,\n          );\n          return;\n        }\n\n        stats.parsing.start = performance.now();\n        if (M3U8Parser.isMediaPlaylist(string)) {\n          this.handleTrackOrLevelPlaylist(\n            response,\n            stats,\n            context,\n            networkDetails || null,\n            loader,\n          );\n        } else {\n          this.handleMasterPlaylist(response, stats, context, networkDetails);\n        }\n      },\n      onError: (response, context, networkDetails, stats) => {\n        this.handleNetworkError(\n          context,\n          networkDetails,\n          false,\n          response,\n          stats,\n        );\n      },\n      onTimeout: (stats, context, networkDetails) => {\n        this.handleNetworkError(\n          context,\n          networkDetails,\n          true,\n          undefined,\n          stats,\n        );\n      },\n    };\n\n    // logger.debug(`[playlist-loader]: Calling internal loader delegate for URL: ${context.url}`);\n\n    loader.load(context, loaderConfig, loaderCallbacks);\n  }\n\n  private handleMasterPlaylist(\n    response: LoaderResponse,\n    stats: LoaderStats,\n    context: PlaylistLoaderContext,\n    networkDetails: any,\n  ): void {\n    const hls = this.hls;\n    const string = response.data as string;\n\n    const url = getResponseUrl(response, context);\n\n    const parsedResult = M3U8Parser.parseMasterPlaylist(string, url);\n\n    if (parsedResult.playlistParsingError) {\n      this.handleManifestParsingError(\n        response,\n        context,\n        parsedResult.playlistParsingError,\n        networkDetails,\n        stats,\n      );\n      return;\n    }\n\n    const {\n      contentSteering,\n      levels,\n      sessionData,\n      sessionKeys,\n      startTimeOffset,\n      variableList,\n    } = parsedResult;\n\n    this.variableList = variableList;\n\n    const {\n      AUDIO: audioTracks = [],\n      SUBTITLES: subtitles,\n      'CLOSED-CAPTIONS': captions,\n    } = M3U8Parser.parseMasterPlaylistMedia(string, url, parsedResult);\n\n    if (audioTracks.length) {\n      // check if we have found an audio track embedded in main playlist (audio track without URI attribute)\n      const embeddedAudioFound: boolean = audioTracks.some(\n        (audioTrack) => !audioTrack.url,\n      );\n\n      // if no embedded audio track defined, but audio codec signaled in quality level,\n      // we need to signal this main audio track this could happen with playlists with\n      // alt audio rendition in which quality levels (main)\n      // contains both audio+video. but with mixed audio track not signaled\n      if (\n        !embeddedAudioFound &&\n        levels[0].audioCodec &&\n        !levels[0].attrs.AUDIO\n      ) {\n        logger.log(\n          '[playlist-loader]: audio codec signaled in quality level, but no embedded audio track signaled, create one',\n        );\n        audioTracks.unshift({\n          type: 'main',\n          name: 'main',\n          groupId: 'main',\n          default: false,\n          autoselect: false,\n          forced: false,\n          id: -1,\n          attrs: new AttrList({}) as MediaAttributes,\n          bitrate: 0,\n          url: '',\n        });\n      }\n    }\n\n    hls.trigger(Events.MANIFEST_LOADED, {\n      levels,\n      audioTracks,\n      subtitles,\n      captions,\n      contentSteering,\n      url,\n      stats,\n      networkDetails,\n      sessionData,\n      sessionKeys,\n      startTimeOffset,\n      variableList,\n    });\n  }\n\n  private handleTrackOrLevelPlaylist(\n    response: LoaderResponse,\n    stats: LoaderStats,\n    context: PlaylistLoaderContext,\n    networkDetails: any,\n    loader: Loader<PlaylistLoaderContext> | undefined,\n  ): void {\n    const hls = this.hls;\n    const { id, level, type } = context;\n\n    const url = getResponseUrl(response, context);\n    const levelUrlId = 0;\n    const levelId = Number.isFinite(level as number)\n      ? (level as number)\n      : Number.isFinite(id as number)\n        ? (id as number)\n        : 0;\n    const levelType = mapContextToLevelType(context);\n    const levelDetails: LevelDetails = M3U8Parser.parseLevelPlaylist(\n      response.data as string,\n      url,\n      levelId,\n      levelType,\n      levelUrlId,\n      this.variableList,\n    );\n\n    // We have done our first request (Manifest-type) and receive\n    // not a master playlist but a chunk-list (track/level)\n    // We fire the manifest-loaded event anyway with the parsed level-details\n    // by creating a single-level structure for it.\n    if (type === PlaylistContextType.MANIFEST) {\n      const singleLevel: LevelParsed = {\n        attrs: new AttrList({}),\n        bitrate: 0,\n        details: levelDetails,\n        name: '',\n        url,\n      };\n\n      hls.trigger(Events.MANIFEST_LOADED, {\n        levels: [singleLevel],\n        audioTracks: [],\n        url,\n        stats,\n        networkDetails,\n        sessionData: null,\n        sessionKeys: null,\n        contentSteering: null,\n        startTimeOffset: null,\n        variableList: null,\n      });\n    }\n\n    // save parsing time\n    stats.parsing.end = performance.now();\n\n    // extend the context with the new levelDetails property\n    context.levelDetails = levelDetails;\n\n    this.handlePlaylistLoaded(\n      levelDetails,\n      response,\n      stats,\n      context,\n      networkDetails,\n      loader,\n    );\n  }\n\n  private handleManifestParsingError(\n    response: LoaderResponse,\n    context: PlaylistLoaderContext,\n    error: Error,\n    networkDetails: any,\n    stats: LoaderStats,\n  ): void {\n    this.hls.trigger(Events.ERROR, {\n      type: ErrorTypes.NETWORK_ERROR,\n      details: ErrorDetails.MANIFEST_PARSING_ERROR,\n      fatal: context.type === PlaylistContextType.MANIFEST,\n      url: response.url,\n      err: error,\n      error,\n      reason: error.message,\n      response,\n      context,\n      networkDetails,\n      stats,\n    });\n  }\n\n  private handleNetworkError(\n    context: PlaylistLoaderContext,\n    networkDetails: any,\n    timeout = false,\n    response: { code: number; text: string } | undefined,\n    stats: LoaderStats,\n  ): void {\n    let message = `A network ${\n      timeout\n        ? 'timeout'\n        : 'error' + (response ? ' (status ' + response.code + ')' : '')\n    } occurred while loading ${context.type}`;\n    if (context.type === PlaylistContextType.LEVEL) {\n      message += `: ${context.level} id: ${context.id}`;\n    } else if (\n      context.type === PlaylistContextType.AUDIO_TRACK ||\n      context.type === PlaylistContextType.SUBTITLE_TRACK\n    ) {\n      message += ` id: ${context.id} group-id: \"${context.groupId}\"`;\n    }\n    const error = new Error(message);\n    logger.warn(`[playlist-loader]: ${message}`);\n    let details = ErrorDetails.UNKNOWN;\n    let fatal = false;\n\n    const loader = this.getInternalLoader(context);\n\n    switch (context.type) {\n      case PlaylistContextType.MANIFEST:\n        details = timeout\n          ? ErrorDetails.MANIFEST_LOAD_TIMEOUT\n          : ErrorDetails.MANIFEST_LOAD_ERROR;\n        fatal = true;\n        break;\n      case PlaylistContextType.LEVEL:\n        details = timeout\n          ? ErrorDetails.LEVEL_LOAD_TIMEOUT\n          : ErrorDetails.LEVEL_LOAD_ERROR;\n        fatal = false;\n        break;\n      case PlaylistContextType.AUDIO_TRACK:\n        details = timeout\n          ? ErrorDetails.AUDIO_TRACK_LOAD_TIMEOUT\n          : ErrorDetails.AUDIO_TRACK_LOAD_ERROR;\n        fatal = false;\n        break;\n      case PlaylistContextType.SUBTITLE_TRACK:\n        details = timeout\n          ? ErrorDetails.SUBTITLE_TRACK_LOAD_TIMEOUT\n          : ErrorDetails.SUBTITLE_LOAD_ERROR;\n        fatal = false;\n        break;\n    }\n\n    if (loader) {\n      this.resetInternalLoader(context.type);\n    }\n\n    const errorData: ErrorData = {\n      type: ErrorTypes.NETWORK_ERROR,\n      details,\n      fatal,\n      url: context.url,\n      loader,\n      context,\n      error,\n      networkDetails,\n      stats,\n    };\n\n    if (response) {\n      const url = networkDetails?.url || context.url;\n      errorData.response = { url, data: undefined as any, ...response };\n    }\n\n    this.hls.trigger(Events.ERROR, errorData);\n  }\n\n  private handlePlaylistLoaded(\n    levelDetails: LevelDetails,\n    response: LoaderResponse,\n    stats: LoaderStats,\n    context: PlaylistLoaderContext,\n    networkDetails: any,\n    loader: Loader<PlaylistLoaderContext> | undefined,\n  ): void {\n    const hls = this.hls;\n    const { type, level, id, groupId, deliveryDirectives } = context;\n    const url = getResponseUrl(response, context);\n    const parent = mapContextToLevelType(context);\n    const levelIndex =\n      typeof context.level === 'number' && parent === PlaylistLevelType.MAIN\n        ? (level as number)\n        : undefined;\n    if (!levelDetails.fragments.length) {\n      const error = new Error('No Segments found in Playlist');\n      hls.trigger(Events.ERROR, {\n        type: ErrorTypes.NETWORK_ERROR,\n        details: ErrorDetails.LEVEL_EMPTY_ERROR,\n        fatal: false,\n        url,\n        error,\n        reason: error.message,\n        response,\n        context,\n        level: levelIndex,\n        parent,\n        networkDetails,\n        stats,\n      });\n      return;\n    }\n    if (!levelDetails.targetduration) {\n      levelDetails.playlistParsingError = new Error('Missing Target Duration');\n    }\n    const error = levelDetails.playlistParsingError;\n    if (error) {\n      hls.trigger(Events.ERROR, {\n        type: ErrorTypes.NETWORK_ERROR,\n        details: ErrorDetails.LEVEL_PARSING_ERROR,\n        fatal: false,\n        url,\n        error,\n        reason: error.message,\n        response,\n        context,\n        level: levelIndex,\n        parent,\n        networkDetails,\n        stats,\n      });\n      return;\n    }\n\n    if (levelDetails.live && loader) {\n      if (loader.getCacheAge) {\n        levelDetails.ageHeader = loader.getCacheAge() || 0;\n      }\n      if (!loader.getCacheAge || isNaN(levelDetails.ageHeader)) {\n        levelDetails.ageHeader = 0;\n      }\n    }\n\n    switch (type) {\n      case PlaylistContextType.MANIFEST:\n      case PlaylistContextType.LEVEL:\n        hls.trigger(Events.LEVEL_LOADED, {\n          details: levelDetails,\n          level: levelIndex || 0,\n          id: id || 0,\n          stats,\n          networkDetails,\n          deliveryDirectives,\n        });\n        break;\n      case PlaylistContextType.AUDIO_TRACK:\n        hls.trigger(Events.AUDIO_TRACK_LOADED, {\n          details: levelDetails,\n          id: id || 0,\n          groupId: groupId || '',\n          stats,\n          networkDetails,\n          deliveryDirectives,\n        });\n        break;\n      case PlaylistContextType.SUBTITLE_TRACK:\n        hls.trigger(Events.SUBTITLE_TRACK_LOADED, {\n          details: levelDetails,\n          id: id || 0,\n          groupId: groupId || '',\n          stats,\n          networkDetails,\n          deliveryDirectives,\n        });\n        break;\n    }\n  }\n}\n\nexport default PlaylistLoader;\n", "import { logger } from './logger';\n\nexport function sendAddTrackEvent(track: TextTrack, videoEl: HTMLMediaElement) {\n  let event: Event;\n  try {\n    event = new Event('addtrack');\n  } catch (err) {\n    // for IE11\n    event = document.createEvent('Event');\n    event.initEvent('addtrack', false, false);\n  }\n  (event as any).track = track;\n  videoEl.dispatchEvent(event);\n}\n\nexport function addCueToTrack(track: TextTrack, cue: VTTCue) {\n  // Sometimes there are cue overlaps on segmented vtts so the same\n  // cue can appear more than once in different vtt files.\n  // This avoid showing duplicated cues with same timecode and text.\n  const mode = track.mode;\n  if (mode === 'disabled') {\n    track.mode = 'hidden';\n  }\n  if (track.cues && !track.cues.getCueById(cue.id)) {\n    try {\n      track.addCue(cue);\n      if (!track.cues.getCueById(cue.id)) {\n        throw new Error(`addCue is failed for: ${cue}`);\n      }\n    } catch (err) {\n      logger.debug(`[texttrack-utils]: ${err}`);\n      try {\n        const textTrackCue = new (self.TextTrackCue as any)(\n          cue.startTime,\n          cue.endTime,\n          cue.text,\n        );\n        textTrackCue.id = cue.id;\n        track.addCue(textTrackCue);\n      } catch (err2) {\n        logger.debug(\n          `[texttrack-utils]: Legacy TextTrackCue fallback failed: ${err2}`,\n        );\n      }\n    }\n  }\n  if (mode === 'disabled') {\n    track.mode = mode;\n  }\n}\n\nexport function clearCurrentCues(track: TextTrack) {\n  // When track.mode is disabled, track.cues will be null.\n  // To guarantee the removal of cues, we need to temporarily\n  // change the mode to hidden\n  const mode = track.mode;\n  if (mode === 'disabled') {\n    track.mode = 'hidden';\n  }\n  if (track.cues) {\n    for (let i = track.cues.length; i--; ) {\n      track.removeCue(track.cues[i]);\n    }\n  }\n  if (mode === 'disabled') {\n    track.mode = mode;\n  }\n}\n\nexport function removeCuesInRange(\n  track: TextTrack,\n  start: number,\n  end: number,\n  predicate?: (cue: TextTrackCue) => boolean,\n) {\n  const mode = track.mode;\n  if (mode === 'disabled') {\n    track.mode = 'hidden';\n  }\n\n  if (track.cues && track.cues.length > 0) {\n    const cues = getCuesInRange(track.cues, start, end);\n    for (let i = 0; i < cues.length; i++) {\n      if (!predicate || predicate(cues[i])) {\n        track.removeCue(cues[i]);\n      }\n    }\n  }\n  if (mode === 'disabled') {\n    track.mode = mode;\n  }\n}\n\n// Find first cue starting after given time.\n// Modified version of binary search O(log(n)).\nfunction getFirstCueIndexAfterTime(\n  cues: TextTrackCueList | TextTrackCue[],\n  time: number,\n): number {\n  // If first cue starts after time, start there\n  if (time < cues[0].startTime) {\n    return 0;\n  }\n  // If the last cue ends before time there is no overlap\n  const len = cues.length - 1;\n  if (time > cues[len].endTime) {\n    return -1;\n  }\n\n  let left = 0;\n  let right = len;\n\n  while (left <= right) {\n    const mid = Math.floor((right + left) / 2);\n\n    if (time < cues[mid].startTime) {\n      right = mid - 1;\n    } else if (time > cues[mid].startTime && left < len) {\n      left = mid + 1;\n    } else {\n      // If it's not lower or higher, it must be equal.\n      return mid;\n    }\n  }\n  // At this point, left and right have swapped.\n  // No direct match was found, left or right element must be the closest. Check which one has the smallest diff.\n  return cues[left].startTime - time < time - cues[right].startTime\n    ? left\n    : right;\n}\n\nexport function getCuesInRange(\n  cues: TextTrackCueList | TextTrackCue[],\n  start: number,\n  end: number,\n): TextTrackCue[] {\n  const cuesFound: TextTrackCue[] = [];\n  const firstCueInRange = getFirstCueIndexAfterTime(cues, start);\n  if (firstCueInRange > -1) {\n    for (let i = firstCueInRange, len = cues.length; i < len; i++) {\n      const cue = cues[i];\n      if (cue.startTime >= start && cue.endTime <= end) {\n        cuesFound.push(cue);\n      } else if (cue.startTime > end) {\n        return cuesFound;\n      }\n    }\n  }\n  return cuesFound;\n}\n\nexport function filterSubtitleTracks(\n  textTrackList: TextTrackList,\n): TextTrack[] {\n  const tracks: TextTrack[] = [];\n  for (let i = 0; i < textTrackList.length; i++) {\n    const track = textTrackList[i];\n    // Edge adds a track without a label; we don't want to use it\n    if (\n      (track.kind === 'subtitles' || track.kind === 'captions') &&\n      track.label\n    ) {\n      tracks.push(textTrackList[i]);\n    }\n  }\n  return tracks;\n}\n", "import type { RationalTimestamp } from '../utils/timescale-conversion';\n\nexport interface Demuxer {\n  demux(\n    data: Uint8Array,\n    timeOffset: number,\n    isSampleAes?: boolean,\n    flush?: boolean,\n  ): DemuxerResult;\n  demuxSampleAes(\n    data: Uint8Array,\n    keyData: KeyData,\n    timeOffset: number,\n  ): Promise<DemuxerResult>;\n  flush(timeOffset?: number): DemuxerResult | Promise<DemuxerResult>;\n  destroy(): void;\n  resetInitSegment(\n    initSegment: Uint8Array | undefined,\n    audioCodec: string | undefined,\n    videoCodec: string | undefined,\n    trackDuration: number,\n  );\n  resetTimeStamp(defaultInitPTS?: RationalTimestamp | null): void;\n  resetContiguity(): void;\n}\n\nexport interface DemuxerResult {\n  audioTrack: DemuxedAudioTrack;\n  videoTrack: DemuxedVideoTrackBase;\n  id3Track: DemuxedMetadataTrack;\n  textTrack: DemuxedUserdataTrack;\n}\n\nexport interface DemuxedTrack {\n  type: string;\n  id: number;\n  pid: number;\n  inputTimeScale: number;\n  sequenceNumber: number;\n  samples:\n    | AudioSample[]\n    | VideoSample[]\n    | MetadataSample[]\n    | UserdataSample[]\n    | Uint8Array;\n  timescale?: number;\n  container?: string;\n  dropped: number;\n  duration?: number;\n  pesData?: ElementaryStreamData | null;\n  codec?: string;\n}\n\nexport interface PassthroughTrack extends DemuxedTrack {\n  sampleDuration: number;\n  samples: Uint8Array;\n  timescale: number;\n  duration: number;\n  codec: string;\n}\nexport interface DemuxedAudioTrack extends DemuxedTrack {\n  config?: number[] | Uint8Array;\n  samplerate?: number;\n  segmentCodec?: string;\n  channelCount?: number;\n  manifestCodec?: string;\n  samples: AudioSample[];\n}\n\nexport interface DemuxedVideoTrackBase extends DemuxedTrack {\n  width?: number;\n  height?: number;\n  pixelRatio?: [number, number];\n  audFound?: boolean;\n  pps?: Uint8Array[];\n  sps?: Uint8Array[];\n  naluState?: number;\n  segmentCodec?: string;\n  manifestCodec?: string;\n  samples: VideoSample[] | Uint8Array;\n}\n\nexport interface DemuxedVideoTrack extends DemuxedVideoTrackBase {\n  samples: VideoSample[];\n}\n\nexport interface DemuxedMetadataTrack extends DemuxedTrack {\n  samples: MetadataSample[];\n}\n\nexport interface DemuxedUserdataTrack extends DemuxedTrack {\n  samples: UserdataSample[];\n}\n\nexport const enum MetadataSchema {\n  audioId3 = 'org.id3',\n  dateRange = 'com.apple.quicktime.HLS',\n  emsg = 'https://aomedia.org/emsg/ID3',\n}\nexport interface MetadataSample {\n  pts: number;\n  dts: number;\n  duration: number;\n  len?: number;\n  data: Uint8Array;\n  type: MetadataSchema;\n}\n\nexport interface UserdataSample {\n  pts: number;\n  bytes?: Uint8Array;\n  type?: number;\n  payloadType?: number;\n  uuid?: string;\n  userData?: string;\n  userDataBytes?: Uint8Array;\n}\n\nexport interface VideoSample {\n  dts: number;\n  pts: number;\n  key: boolean;\n  frame: boolean;\n  units: VideoSampleUnit[];\n  debug: string;\n  length: number;\n}\n\nexport interface VideoSampleUnit {\n  data: Uint8Array;\n  type: number;\n  state?: number;\n}\n\nexport type AudioSample = {\n  unit: Uint8Array;\n  pts: number;\n};\n\nexport type AudioFrame = {\n  sample: AudioSample;\n  length: number;\n  missing: number;\n};\n\nexport interface ElementaryStreamData {\n  data: Uint8Array[];\n  size: number;\n}\n\nexport interface KeyData {\n  method: string;\n  key: Uint8Array;\n  iv: Uint8Array;\n}\n", "import { Events } from '../events';\nimport {\n  sendAddTrackEvent,\n  clearCurrentCues,\n  removeCuesInRange,\n} from '../utils/texttrack-utils';\nimport * as ID3 from '../demux/id3';\nimport {\n  DateRange,\n  isDateRangeCueAttribute,\n  isSCTE35Attribute,\n} from '../loader/date-range';\nimport { MetadataSchema } from '../types/demuxer';\nimport type {\n  BufferFlushingData,\n  FragParsingMetadataData,\n  LevelUpdatedData,\n  MediaAttachedData,\n} from '../types/events';\nimport type { ComponentAPI } from '../types/component-api';\nimport type Hls from '../hls';\n\ndeclare global {\n  interface Window {\n    WebKitDataCue: VTTCue | void;\n  }\n}\n\nconst MIN_CUE_DURATION = 0.25;\n\nfunction getCueClass(): typeof VTTCue | typeof TextTrackCue | undefined {\n  if (typeof self === 'undefined') return undefined;\n  return self.VTTCue || self.TextTrackCue;\n}\n\nfunction createCueWithDataFields(\n  Cue: typeof VTTCue | typeof TextTrackCue,\n  startTime: number,\n  endTime: number,\n  data: Object,\n  type?: string,\n): VTTCue | TextTrackCue | undefined {\n  let cue = new Cue(startTime, endTime, '');\n  try {\n    (cue as any).value = data;\n    if (type) {\n      (cue as any).type = type;\n    }\n  } catch (e) {\n    cue = new Cue(\n      startTime,\n      endTime,\n      JSON.stringify(type ? { type, ...data } : data),\n    );\n  }\n  return cue;\n}\n\n// VTTCue latest draft allows an infinite duration, fallback\n// to MAX_VALUE if necessary\nconst MAX_CUE_ENDTIME = (() => {\n  const Cue = getCueClass();\n  try {\n    Cue && new Cue(0, Number.POSITIVE_INFINITY, '');\n  } catch (e) {\n    return Number.MAX_VALUE;\n  }\n  return Number.POSITIVE_INFINITY;\n})();\n\nfunction dateRangeDateToTimelineSeconds(date: Date, offset: number): number {\n  return date.getTime() / 1000 - offset;\n}\n\nfunction hexToArrayBuffer(str): ArrayBuffer {\n  return Uint8Array.from(\n    str\n      .replace(/^0x/, '')\n      .replace(/([\\da-fA-F]{2}) ?/g, '0x$1 ')\n      .replace(/ +$/, '')\n      .split(' '),\n  ).buffer;\n}\nclass ID3TrackController implements ComponentAPI {\n  private hls: Hls;\n  private id3Track: TextTrack | null = null;\n  private media: HTMLMediaElement | null = null;\n  private dateRangeCuesAppended: Record<\n    string,\n    {\n      cues: Record<string, VTTCue | TextTrackCue>;\n      dateRange: DateRange;\n      durationKnown: boolean;\n    }\n  > = {};\n\n  constructor(hls) {\n    this.hls = hls;\n    this._registerListeners();\n  }\n\n  destroy() {\n    this._unregisterListeners();\n    this.id3Track = null;\n    this.media = null;\n    this.dateRangeCuesAppended = {};\n    // @ts-ignore\n    this.hls = null;\n  }\n\n  private _registerListeners() {\n    const { hls } = this;\n    hls.on(Events.MEDIA_ATTACHED, this.onMediaAttached, this);\n    hls.on(Events.MEDIA_DETACHING, this.onMediaDetaching, this);\n    hls.on(Events.MANIFEST_LOADING, this.onManifestLoading, this);\n    hls.on(Events.FRAG_PARSING_METADATA, this.onFragParsingMetadata, this);\n    hls.on(Events.BUFFER_FLUSHING, this.onBufferFlushing, this);\n    hls.on(Events.LEVEL_UPDATED, this.onLevelUpdated, this);\n  }\n\n  private _unregisterListeners() {\n    const { hls } = this;\n    hls.off(Events.MEDIA_ATTACHED, this.onMediaAttached, this);\n    hls.off(Events.MEDIA_DETACHING, this.onMediaDetaching, this);\n    hls.off(Events.MANIFEST_LOADING, this.onManifestLoading, this);\n    hls.off(Events.FRAG_PARSING_METADATA, this.onFragParsingMetadata, this);\n    hls.off(Events.BUFFER_FLUSHING, this.onBufferFlushing, this);\n    hls.off(Events.LEVEL_UPDATED, this.onLevelUpdated, this);\n  }\n\n  // Add ID3 metatadata text track.\n  protected onMediaAttached(\n    event: Events.MEDIA_ATTACHED,\n    data: MediaAttachedData,\n  ): void {\n    this.media = data.media;\n  }\n\n  protected onMediaDetaching(): void {\n    if (!this.id3Track) {\n      return;\n    }\n    clearCurrentCues(this.id3Track);\n    this.id3Track = null;\n    this.media = null;\n    this.dateRangeCuesAppended = {};\n  }\n\n  private onManifestLoading() {\n    this.dateRangeCuesAppended = {};\n  }\n\n  createTrack(media: HTMLMediaElement): TextTrack {\n    const track = this.getID3Track(media.textTracks) as TextTrack;\n    track.mode = 'hidden';\n    return track;\n  }\n\n  getID3Track(textTracks: TextTrackList): TextTrack | void {\n    if (!this.media) {\n      return;\n    }\n    for (let i = 0; i < textTracks.length; i++) {\n      const textTrack: TextTrack = textTracks[i];\n      if (textTrack.kind === 'metadata' && textTrack.label === 'id3') {\n        // send 'addtrack' when reusing the textTrack for metadata,\n        // same as what we do for captions\n        sendAddTrackEvent(textTrack, this.media);\n\n        return textTrack;\n      }\n    }\n    return this.media.addTextTrack('metadata', 'id3');\n  }\n\n  onFragParsingMetadata(\n    event: Events.FRAG_PARSING_METADATA,\n    data: FragParsingMetadataData,\n  ) {\n    if (!this.media) {\n      return;\n    }\n\n    const {\n      hls: {\n        config: { enableEmsgMetadataCues, enableID3MetadataCues },\n      },\n    } = this;\n    if (!enableEmsgMetadataCues && !enableID3MetadataCues) {\n      return;\n    }\n\n    const { samples } = data;\n\n    // create track dynamically\n    if (!this.id3Track) {\n      this.id3Track = this.createTrack(this.media);\n    }\n\n    const Cue = getCueClass();\n    if (!Cue) {\n      return;\n    }\n\n    for (let i = 0; i < samples.length; i++) {\n      const type = samples[i].type;\n      if (\n        (type === MetadataSchema.emsg && !enableEmsgMetadataCues) ||\n        !enableID3MetadataCues\n      ) {\n        continue;\n      }\n\n      const frames = ID3.getID3Frames(samples[i].data);\n      if (frames) {\n        const startTime = samples[i].pts;\n        let endTime: number = startTime + samples[i].duration;\n\n        if (endTime > MAX_CUE_ENDTIME) {\n          endTime = MAX_CUE_ENDTIME;\n        }\n\n        const timeDiff = endTime - startTime;\n        if (timeDiff <= 0) {\n          endTime = startTime + MIN_CUE_DURATION;\n        }\n\n        for (let j = 0; j < frames.length; j++) {\n          const frame = frames[j];\n          // Safari doesn't put the timestamp frame in the TextTrack\n          if (!ID3.isTimeStampFrame(frame)) {\n            // add a bounds to any unbounded cues\n            this.updateId3CueEnds(startTime, type);\n            const cue = createCueWithDataFields(\n              Cue,\n              startTime,\n              endTime,\n              frame,\n              type,\n            );\n            if (cue) {\n              this.id3Track.addCue(cue);\n            }\n          }\n        }\n      }\n    }\n  }\n\n  updateId3CueEnds(startTime: number, type: MetadataSchema) {\n    const cues = this.id3Track?.cues;\n    if (cues) {\n      for (let i = cues.length; i--; ) {\n        const cue = cues[i] as any;\n        if (\n          cue.type === type &&\n          cue.startTime < startTime &&\n          cue.endTime === MAX_CUE_ENDTIME\n        ) {\n          cue.endTime = startTime;\n        }\n      }\n    }\n  }\n\n  onBufferFlushing(\n    event: Events.BUFFER_FLUSHING,\n    { startOffset, endOffset, type }: BufferFlushingData,\n  ) {\n    const { id3Track, hls } = this;\n    if (!hls) {\n      return;\n    }\n\n    const {\n      config: { enableEmsgMetadataCues, enableID3MetadataCues },\n    } = hls;\n    if (id3Track && (enableEmsgMetadataCues || enableID3MetadataCues)) {\n      let predicate;\n\n      if (type === 'audio') {\n        predicate = (cue) =>\n          (cue as any).type === MetadataSchema.audioId3 &&\n          enableID3MetadataCues;\n      } else if (type === 'video') {\n        predicate = (cue) =>\n          (cue as any).type === MetadataSchema.emsg && enableEmsgMetadataCues;\n      } else {\n        predicate = (cue) =>\n          ((cue as any).type === MetadataSchema.audioId3 &&\n            enableID3MetadataCues) ||\n          ((cue as any).type === MetadataSchema.emsg && enableEmsgMetadataCues);\n      }\n      removeCuesInRange(id3Track, startOffset, endOffset, predicate);\n    }\n  }\n\n  onLevelUpdated(event: Events.LEVEL_UPDATED, { details }: LevelUpdatedData) {\n    if (\n      !this.media ||\n      !details.hasProgramDateTime ||\n      !this.hls.config.enableDateRangeMetadataCues\n    ) {\n      return;\n    }\n    const { dateRangeCuesAppended, id3Track } = this;\n    const { dateRanges } = details;\n    const ids = Object.keys(dateRanges);\n    // Remove cues from track not found in details.dateRanges\n    if (id3Track) {\n      const idsToRemove = Object.keys(dateRangeCuesAppended).filter(\n        (id) => !ids.includes(id),\n      );\n      for (let i = idsToRemove.length; i--; ) {\n        const id = idsToRemove[i];\n        Object.keys(dateRangeCuesAppended[id].cues).forEach((key) => {\n          id3Track.removeCue(dateRangeCuesAppended[id].cues[key]);\n        });\n        delete dateRangeCuesAppended[id];\n      }\n    }\n    // Exit if the playlist does not have Date Ranges or does not have Program Date Time\n    const lastFragment = details.fragments[details.fragments.length - 1];\n    if (ids.length === 0 || !Number.isFinite(lastFragment?.programDateTime)) {\n      return;\n    }\n\n    if (!this.id3Track) {\n      this.id3Track = this.createTrack(this.media);\n    }\n\n    const dateTimeOffset =\n      (lastFragment.programDateTime as number) / 1000 - lastFragment.start;\n    const Cue = getCueClass();\n\n    for (let i = 0; i < ids.length; i++) {\n      const id = ids[i];\n      const dateRange = dateRanges[id];\n      const startTime = dateRangeDateToTimelineSeconds(\n        dateRange.startDate,\n        dateTimeOffset,\n      );\n\n      // Process DateRanges to determine end-time (known DURATION, END-DATE, or END-ON-NEXT)\n      const appendedDateRangeCues = dateRangeCuesAppended[id];\n      const cues = appendedDateRangeCues?.cues || {};\n      let durationKnown = appendedDateRangeCues?.durationKnown || false;\n      let endTime = MAX_CUE_ENDTIME;\n      const endDate = dateRange.endDate;\n      if (endDate) {\n        endTime = dateRangeDateToTimelineSeconds(endDate, dateTimeOffset);\n        durationKnown = true;\n      } else if (dateRange.endOnNext && !durationKnown) {\n        const nextDateRangeWithSameClass = ids.reduce(\n          (candidateDateRange: DateRange | null, id) => {\n            if (id !== dateRange.id) {\n              const otherDateRange = dateRanges[id];\n              if (\n                otherDateRange.class === dateRange.class &&\n                otherDateRange.startDate > dateRange.startDate &&\n                (!candidateDateRange ||\n                  dateRange.startDate < candidateDateRange.startDate)\n              ) {\n                return otherDateRange;\n              }\n            }\n            return candidateDateRange;\n          },\n          null,\n        );\n        if (nextDateRangeWithSameClass) {\n          endTime = dateRangeDateToTimelineSeconds(\n            nextDateRangeWithSameClass.startDate,\n            dateTimeOffset,\n          );\n          durationKnown = true;\n        }\n      }\n\n      // Create TextTrack Cues for each MetadataGroup Item (select DateRange attribute)\n      // This is to emulate Safari HLS playback handling of DateRange tags\n      const attributes = Object.keys(dateRange.attr);\n      for (let j = 0; j < attributes.length; j++) {\n        const key = attributes[j];\n        if (!isDateRangeCueAttribute(key)) {\n          continue;\n        }\n        const cue = cues[key];\n        if (cue) {\n          if (durationKnown && !appendedDateRangeCues.durationKnown) {\n            cue.endTime = endTime;\n          }\n        } else if (Cue) {\n          let data = dateRange.attr[key];\n          if (isSCTE35Attribute(key)) {\n            data = hexToArrayBuffer(data);\n          }\n          const cue = createCueWithDataFields(\n            Cue,\n            startTime,\n            endTime,\n            { key, data },\n            MetadataSchema.dateRange,\n          );\n          if (cue) {\n            cue.id = id;\n            this.id3Track.addCue(cue);\n            cues[key] = cue;\n          }\n        }\n      }\n\n      // Keep track of processed DateRanges by ID for updating cues with new DateRange tag attributes\n      dateRangeCuesAppended[id] = {\n        cues,\n        dateRange,\n        durationKnown,\n      };\n    }\n  }\n}\n\nexport default ID3TrackController;\n", "import { LevelDetails } from '../loader/level-details';\nimport { ErrorDetails } from '../errors';\nimport { Events } from '../events';\nimport type {\n  ErrorData,\n  LevelUpdatedData,\n  MediaAttachingData,\n} from '../types/events';\nimport { logger } from '../utils/logger';\nimport type { ComponentAPI } from '../types/component-api';\nimport type Hls from '../hls';\nimport type { HlsConfig } from '../config';\n\nexport default class LatencyController implements ComponentAPI {\n  private hls: Hls;\n  private readonly config: HlsConfig;\n  private media: HTMLMediaElement | null = null;\n  private levelDetails: LevelDetails | null = null;\n  private currentTime: number = 0;\n  private stallCount: number = 0;\n  private _latency: number | null = null;\n  private timeupdateHandler = () => this.timeupdate();\n\n  constructor(hls: Hls) {\n    this.hls = hls;\n    this.config = hls.config;\n    this.registerListeners();\n  }\n\n  get latency(): number {\n    return this._latency || 0;\n  }\n\n  get maxLatency(): number {\n    const { config, levelDetails } = this;\n    if (config.liveMaxLatencyDuration !== undefined) {\n      return config.liveMaxLatencyDuration;\n    }\n    return levelDetails\n      ? config.liveMaxLatencyDurationCount * levelDetails.targetduration\n      : 0;\n  }\n\n  get targetLatency(): number | null {\n    const { levelDetails } = this;\n    if (levelDetails === null) {\n      return null;\n    }\n    const { holdBack, partHoldBack, targetduration } = levelDetails;\n    const { liveSyncDuration, liveSyncDurationCount, lowLatencyMode } =\n      this.config;\n    const userConfig = this.hls.userConfig;\n    let targetLatency = lowLatencyMode ? partHoldBack || holdBack : holdBack;\n    if (\n      userConfig.liveSyncDuration ||\n      userConfig.liveSyncDurationCount ||\n      targetLatency === 0\n    ) {\n      targetLatency =\n        liveSyncDuration !== undefined\n          ? liveSyncDuration\n          : liveSyncDurationCount * targetduration;\n    }\n    const maxLiveSyncOnStallIncrease = targetduration;\n    const liveSyncOnStallIncrease = 1.0;\n    return (\n      targetLatency +\n      Math.min(\n        this.stallCount * liveSyncOnStallIncrease,\n        maxLiveSyncOnStallIncrease,\n      )\n    );\n  }\n\n  get liveSyncPosition(): number | null {\n    const liveEdge = this.estimateLiveEdge();\n    const targetLatency = this.targetLatency;\n    const levelDetails = this.levelDetails;\n    if (liveEdge === null || targetLatency === null || levelDetails === null) {\n      return null;\n    }\n    const edge = levelDetails.edge;\n    const syncPosition = liveEdge - targetLatency - this.edgeStalled;\n    const min = edge - levelDetails.totalduration;\n    const max =\n      edge -\n      ((this.config.lowLatencyMode && levelDetails.partTarget) ||\n        levelDetails.targetduration);\n    return Math.min(Math.max(min, syncPosition), max);\n  }\n\n  get drift(): number {\n    const { levelDetails } = this;\n    if (levelDetails === null) {\n      return 1;\n    }\n    return levelDetails.drift;\n  }\n\n  get edgeStalled(): number {\n    const { levelDetails } = this;\n    if (levelDetails === null) {\n      return 0;\n    }\n    const maxLevelUpdateAge =\n      ((this.config.lowLatencyMode && levelDetails.partTarget) ||\n        levelDetails.targetduration) * 3;\n    return Math.max(levelDetails.age - maxLevelUpdateAge, 0);\n  }\n\n  private get forwardBufferLength(): number {\n    const { media, levelDetails } = this;\n    if (!media || !levelDetails) {\n      return 0;\n    }\n    const bufferedRanges = media.buffered.length;\n    return (\n      (bufferedRanges\n        ? media.buffered.end(bufferedRanges - 1)\n        : levelDetails.edge) - this.currentTime\n    );\n  }\n\n  public destroy(): void {\n    this.unregisterListeners();\n    this.onMediaDetaching();\n    this.levelDetails = null;\n    // @ts-ignore\n    this.hls = this.timeupdateHandler = null;\n  }\n\n  private registerListeners() {\n    this.hls.on(Events.MEDIA_ATTACHED, this.onMediaAttached, this);\n    this.hls.on(Events.MEDIA_DETACHING, this.onMediaDetaching, this);\n    this.hls.on(Events.MANIFEST_LOADING, this.onManifestLoading, this);\n    this.hls.on(Events.LEVEL_UPDATED, this.onLevelUpdated, this);\n    this.hls.on(Events.ERROR, this.onError, this);\n  }\n\n  private unregisterListeners() {\n    this.hls.off(Events.MEDIA_ATTACHED, this.onMediaAttached, this);\n    this.hls.off(Events.MEDIA_DETACHING, this.onMediaDetaching, this);\n    this.hls.off(Events.MANIFEST_LOADING, this.onManifestLoading, this);\n    this.hls.off(Events.LEVEL_UPDATED, this.onLevelUpdated, this);\n    this.hls.off(Events.ERROR, this.onError, this);\n  }\n\n  private onMediaAttached(\n    event: Events.MEDIA_ATTACHED,\n    data: MediaAttachingData,\n  ) {\n    this.media = data.media;\n    this.media.addEventListener('timeupdate', this.timeupdateHandler);\n  }\n\n  private onMediaDetaching() {\n    if (this.media) {\n      this.media.removeEventListener('timeupdate', this.timeupdateHandler);\n      this.media = null;\n    }\n  }\n\n  private onManifestLoading() {\n    this.levelDetails = null;\n    this._latency = null;\n    this.stallCount = 0;\n  }\n\n  private onLevelUpdated(\n    event: Events.LEVEL_UPDATED,\n    { details }: LevelUpdatedData,\n  ) {\n    this.levelDetails = details;\n    if (details.advanced) {\n      this.timeupdate();\n    }\n    if (!details.live && this.media) {\n      this.media.removeEventListener('timeupdate', this.timeupdateHandler);\n    }\n  }\n\n  private onError(event: Events.ERROR, data: ErrorData) {\n    if (data.details !== ErrorDetails.BUFFER_STALLED_ERROR) {\n      return;\n    }\n    this.stallCount++;\n    if (this.levelDetails?.live) {\n      logger.warn(\n        '[playback-rate-controller]: Stall detected, adjusting target latency',\n      );\n    }\n  }\n\n  private timeupdate() {\n    const { media, levelDetails } = this;\n    if (!media || !levelDetails) {\n      return;\n    }\n    this.currentTime = media.currentTime;\n\n    const latency = this.computeLatency();\n    if (latency === null) {\n      return;\n    }\n    this._latency = latency;\n\n    // Adapt playbackRate to meet target latency in low-latency mode\n    const { lowLatencyMode, maxLiveSyncPlaybackRate } = this.config;\n    if (\n      !lowLatencyMode ||\n      maxLiveSyncPlaybackRate === 1 ||\n      !levelDetails.live\n    ) {\n      return;\n    }\n    const targetLatency = this.targetLatency;\n    if (targetLatency === null) {\n      return;\n    }\n    const distanceFromTarget = latency - targetLatency;\n    // Only adjust playbackRate when within one target duration of targetLatency\n    // and more than one second from under-buffering.\n    // Playback further than one target duration from target can be considered DVR playback.\n    const liveMinLatencyDuration = Math.min(\n      this.maxLatency,\n      targetLatency + levelDetails.targetduration,\n    );\n    const inLiveRange = distanceFromTarget < liveMinLatencyDuration;\n\n    if (\n      inLiveRange &&\n      distanceFromTarget > 0.05 &&\n      this.forwardBufferLength > 1\n    ) {\n      const max = Math.min(2, Math.max(1.0, maxLiveSyncPlaybackRate));\n      const rate =\n        Math.round(\n          (2 / (1 + Math.exp(-0.75 * distanceFromTarget - this.edgeStalled))) *\n            20,\n        ) / 20;\n      media.playbackRate = Math.min(max, Math.max(1, rate));\n    } else if (media.playbackRate !== 1 && media.playbackRate !== 0) {\n      media.playbackRate = 1;\n    }\n  }\n\n  private estimateLiveEdge(): number | null {\n    const { levelDetails } = this;\n    if (levelDetails === null) {\n      return null;\n    }\n    return levelDetails.edge + levelDetails.age;\n  }\n\n  private computeLatency(): number | null {\n    const liveEdge = this.estimateLiveEdge();\n    if (liveEdge === null) {\n      return null;\n    }\n    return liveEdge - this.currentTime;\n  }\n}\n", "import type { MediaPlaylist } from './media-playlist';\nimport type { LevelDetails } from '../loader/level-details';\nimport type { AttrList } from '../utils/attr-list';\nimport type { MediaDecodingInfo } from '../utils/mediacapabilities-helper';\n\nexport interface LevelParsed {\n  attrs: LevelAttributes;\n  audioCodec?: string;\n  bitrate: number;\n  details?: LevelDetails;\n  height?: number;\n  id?: number;\n  name: string;\n  textCodec?: string;\n  unknownCodecs?: string[];\n  url: string;\n  videoCodec?: string;\n  width?: number;\n}\n\nexport interface LevelAttributes extends AttrList {\n  'ALLOWED-CPC'?: string;\n  AUDIO?: string;\n  'AVERAGE-BANDWIDTH'?: string;\n  BANDWIDTH?: string;\n  'CLOSED-CAPTIONS'?: string;\n  CODECS?: string;\n  'FRAME-RATE'?: string;\n  'HDCP-LEVEL'?: 'TYPE-0' | 'TYPE-1' | 'NONE';\n  'PATHWAY-ID'?: string;\n  RESOLUTION?: string;\n  SCORE?: string;\n  'STABLE-VARIANT-ID'?: string;\n  SUBTITLES?: string;\n  'SUPPLEMENTAL-CODECS'?: string;\n  VIDEO?: string;\n  'VIDEO-RANGE'?: VideoRange;\n}\n\nexport const HdcpLevels = ['NONE', 'TYPE-0', 'TYPE-1', null] as const;\nexport type HdcpLevel = (typeof HdcpLevels)[number];\n\nexport function isHdcpLevel(value: any): value is HdcpLevel {\n  return HdcpLevels.indexOf(value) > -1;\n}\n\nexport const VideoRangeValues = ['SDR', 'PQ', 'HLG'] as const;\nexport type VideoRange = (typeof VideoRangeValues)[number];\n\nexport function isVideoRange(value: any): value is VideoRange {\n  return !!value && VideoRangeValues.indexOf(value) > -1;\n}\n\nexport type VariableMap = Record<string, string>;\n\nexport const enum HlsSkip {\n  No = '',\n  Yes = 'YES',\n  v2 = 'v2',\n}\n\nexport function getSkipValue(details: LevelDetails): HlsSkip {\n  const { canSkipUntil, canSkipDateRanges, age } = details;\n  // A Client SHOULD NOT request a Playlist Delta Update unless it already\n  // has a version of the Playlist that is no older than one-half of the Skip Boundary.\n  // @see: https://datatracker.ietf.org/doc/html/draft-pantos-hls-rfc8216bis#section-6.3.7\n  const playlistRecentEnough = age < canSkipUntil / 2;\n  if (canSkipUntil && playlistRecentEnough) {\n    if (canSkipDateRanges) {\n      return HlsSkip.v2;\n    }\n    return HlsSkip.Yes;\n  }\n  return HlsSkip.No;\n}\n\nexport class HlsUrlParameters {\n  msn?: number;\n  part?: number;\n  skip?: HlsSkip;\n\n  constructor(msn?: number, part?: number, skip?: HlsSkip) {\n    this.msn = msn;\n    this.part = part;\n    this.skip = skip;\n  }\n\n  addDirectives(uri: string): string | never {\n    const url: URL = new self.URL(uri);\n    if (this.msn !== undefined) {\n      url.searchParams.set('_HLS_msn', this.msn.toString());\n    }\n    if (this.part !== undefined) {\n      url.searchParams.set('_HLS_part', this.part.toString());\n    }\n    if (this.skip) {\n      url.searchParams.set('_HLS_skip', this.skip);\n    }\n    return url.href;\n  }\n}\n\nexport class Level {\n  public readonly _attrs: LevelAttributes[];\n  public readonly audioCodec: string | undefined;\n  public readonly bitrate: number;\n  public readonly codecSet: string;\n  public readonly url: string[];\n  public readonly frameRate: number;\n  public readonly height: number;\n  public readonly id: number;\n  public readonly name: string;\n  public readonly videoCodec: string | undefined;\n  public readonly width: number;\n  public details?: LevelDetails;\n  public fragmentError: number = 0;\n  public loadError: number = 0;\n  public loaded?: { bytes: number; duration: number };\n  public realBitrate: number = 0;\n  public supportedPromise?: Promise<MediaDecodingInfo>;\n  public supportedResult?: MediaDecodingInfo;\n  private _avgBitrate: number = 0;\n  private _audioGroups?: (string | undefined)[];\n  private _subtitleGroups?: (string | undefined)[];\n  // Deprecated (retained for backwards compatibility)\n  private readonly _urlId: number = 0;\n\n  constructor(data: LevelParsed | MediaPlaylist) {\n    this.url = [data.url];\n    this._attrs = [data.attrs];\n    this.bitrate = data.bitrate;\n    if (data.details) {\n      this.details = data.details;\n    }\n    this.id = data.id || 0;\n    this.name = data.name;\n    this.width = data.width || 0;\n    this.height = data.height || 0;\n    this.frameRate = data.attrs.optionalFloat('FRAME-RATE', 0);\n    this._avgBitrate = data.attrs.decimalInteger('AVERAGE-BANDWIDTH');\n    this.audioCodec = data.audioCodec;\n    this.videoCodec = data.videoCodec;\n    this.codecSet = [data.videoCodec, data.audioCodec]\n      .filter((c) => !!c)\n      .map((s: string) => s.substring(0, 4))\n      .join(',');\n    this.addGroupId('audio', data.attrs.AUDIO);\n    this.addGroupId('text', data.attrs.SUBTITLES);\n  }\n\n  get maxBitrate(): number {\n    return Math.max(this.realBitrate, this.bitrate);\n  }\n\n  get averageBitrate(): number {\n    return this._avgBitrate || this.realBitrate || this.bitrate;\n  }\n\n  get attrs(): LevelAttributes {\n    return this._attrs[0];\n  }\n\n  get codecs(): string {\n    return this.attrs.CODECS || '';\n  }\n\n  get pathwayId(): string {\n    return this.attrs['PATHWAY-ID'] || '.';\n  }\n\n  get videoRange(): VideoRange {\n    return this.attrs['VIDEO-RANGE'] || 'SDR';\n  }\n\n  get score(): number {\n    return this.attrs.optionalFloat('SCORE', 0);\n  }\n\n  get uri(): string {\n    return this.url[0] || '';\n  }\n\n  hasAudioGroup(groupId: string | undefined): boolean {\n    return hasGroup(this._audioGroups, groupId);\n  }\n\n  hasSubtitleGroup(groupId: string | undefined): boolean {\n    return hasGroup(this._subtitleGroups, groupId);\n  }\n\n  get audioGroups(): (string | undefined)[] | undefined {\n    return this._audioGroups;\n  }\n\n  get subtitleGroups(): (string | undefined)[] | undefined {\n    return this._subtitleGroups;\n  }\n\n  addGroupId(type: string, groupId: string | undefined) {\n    if (!groupId) {\n      return;\n    }\n    if (type === 'audio') {\n      let audioGroups = this._audioGroups;\n      if (!audioGroups) {\n        audioGroups = this._audioGroups = [];\n      }\n      if (audioGroups.indexOf(groupId) === -1) {\n        audioGroups.push(groupId);\n      }\n    } else if (type === 'text') {\n      let subtitleGroups = this._subtitleGroups;\n      if (!subtitleGroups) {\n        subtitleGroups = this._subtitleGroups = [];\n      }\n      if (subtitleGroups.indexOf(groupId) === -1) {\n        subtitleGroups.push(groupId);\n      }\n    }\n  }\n\n  // Deprecated methods (retained for backwards compatibility)\n  get urlId(): number {\n    return 0;\n  }\n\n  set urlId(value: number) {}\n\n  get audioGroupIds(): (string | undefined)[] | undefined {\n    return this.audioGroups ? [this.audioGroupId] : undefined;\n  }\n\n  get textGroupIds(): (string | undefined)[] | undefined {\n    return this.subtitleGroups ? [this.textGroupId] : undefined;\n  }\n\n  get audioGroupId(): string | undefined {\n    return this.audioGroups?.[0];\n  }\n\n  get textGroupId(): string | undefined {\n    return this.subtitleGroups?.[0];\n  }\n\n  addFallback() {}\n}\n\nfunction hasGroup(\n  groups: (string | undefined)[] | undefined,\n  groupId: string | undefined,\n): boolean {\n  if (!groupId || !groups) {\n    return false;\n  }\n  return groups.indexOf(groupId) !== -1;\n}\n", "/**\n * Provides methods dealing with playlist sliding and drift\n */\n\nimport { logger } from './logger';\nimport { Fragment, Part } from '../loader/fragment';\nimport { LevelDetails } from '../loader/level-details';\nimport type { Level } from '../types/level';\nimport { DateRange } from '../loader/date-range';\n\ntype FragmentIntersection = (oldFrag: Fragment, newFrag: Fragment) => void;\ntype PartIntersection = (oldPart: Part, newPart: Part) => void;\n\nexport function updatePTS(\n  fragments: Fragment[],\n  fromIdx: number,\n  toIdx: number,\n): void {\n  const fragFrom = fragments[fromIdx];\n  const fragTo = fragments[toIdx];\n  updateFromToPTS(fragFrom, fragTo);\n}\n\nfunction updateFromToPTS(fragFrom: Fragment, fragTo: Fragment) {\n  const fragToPTS = fragTo.startPTS as number;\n  // if we know startPTS[toIdx]\n  if (Number.isFinite(fragToPTS)) {\n    // update fragment duration.\n    // it helps to fix drifts between playlist reported duration and fragment real duration\n    let duration: number = 0;\n    let frag: Fragment;\n    if (fragTo.sn > fragFrom.sn) {\n      duration = fragToPTS - fragFrom.start;\n      frag = fragFrom;\n    } else {\n      duration = fragFrom.start - fragToPTS;\n      frag = fragTo;\n    }\n    if (frag.duration !== duration) {\n      frag.duration = duration;\n    }\n    // we dont know startPTS[toIdx]\n  } else if (fragTo.sn > fragFrom.sn) {\n    const contiguous = fragFrom.cc === fragTo.cc;\n    // TODO: With part-loading end/durations we need to confirm the whole fragment is loaded before using (or setting) minEndPTS\n    if (contiguous && fragFrom.minEndPTS) {\n      fragTo.start = fragFrom.start + (fragFrom.minEndPTS - fragFrom.start);\n    } else {\n      fragTo.start = fragFrom.start + fragFrom.duration;\n    }\n  } else {\n    fragTo.start = Math.max(fragFrom.start - fragTo.duration, 0);\n  }\n}\n\nexport function updateFragPTSDTS(\n  details: LevelDetails | undefined,\n  frag: Fragment,\n  startPTS: number,\n  endPTS: number,\n  startDTS: number,\n  endDTS: number,\n): number {\n  const parsedMediaDuration = endPTS - startPTS;\n  if (parsedMediaDuration <= 0) {\n    logger.warn('Fragment should have a positive duration', frag);\n    endPTS = startPTS + frag.duration;\n    endDTS = startDTS + frag.duration;\n  }\n  let maxStartPTS = startPTS;\n  let minEndPTS = endPTS;\n  const fragStartPts = frag.startPTS as number;\n  const fragEndPts = frag.endPTS as number;\n  if (Number.isFinite(fragStartPts)) {\n    // delta PTS between audio and video\n    const deltaPTS = Math.abs(fragStartPts - startPTS);\n    if (!Number.isFinite(frag.deltaPTS as number)) {\n      frag.deltaPTS = deltaPTS;\n    } else {\n      frag.deltaPTS = Math.max(deltaPTS, frag.deltaPTS as number);\n    }\n\n    maxStartPTS = Math.max(startPTS, fragStartPts);\n    startPTS = Math.min(startPTS, fragStartPts);\n    startDTS = Math.min(startDTS, frag.startDTS);\n\n    minEndPTS = Math.min(endPTS, fragEndPts);\n    endPTS = Math.max(endPTS, fragEndPts);\n    endDTS = Math.max(endDTS, frag.endDTS);\n  }\n\n  const drift = startPTS - frag.start;\n  if (frag.start !== 0) {\n    frag.start = startPTS;\n  }\n  frag.duration = endPTS - frag.start;\n  frag.startPTS = startPTS;\n  frag.maxStartPTS = maxStartPTS;\n  frag.startDTS = startDTS;\n  frag.endPTS = endPTS;\n  frag.minEndPTS = minEndPTS;\n  frag.endDTS = endDTS;\n\n  const sn = frag.sn as number; // 'initSegment'\n  // exit if sn out of range\n  if (!details || sn < details.startSN || sn > details.endSN) {\n    return 0;\n  }\n  let i;\n  const fragIdx = sn - details.startSN;\n  const fragments = details.fragments;\n  // update frag reference in fragments array\n  // rationale is that fragments array might not contain this frag object.\n  // this will happen if playlist has been refreshed between frag loading and call to updateFragPTSDTS()\n  // if we don't update frag, we won't be able to propagate PTS info on the playlist\n  // resulting in invalid sliding computation\n  fragments[fragIdx] = frag;\n  // adjust fragment PTS/duration from seqnum-1 to frag 0\n  for (i = fragIdx; i > 0; i--) {\n    updateFromToPTS(fragments[i], fragments[i - 1]);\n  }\n\n  // adjust fragment PTS/duration from seqnum to last frag\n  for (i = fragIdx; i < fragments.length - 1; i++) {\n    updateFromToPTS(fragments[i], fragments[i + 1]);\n  }\n  if (details.fragmentHint) {\n    updateFromToPTS(fragments[fragments.length - 1], details.fragmentHint);\n  }\n\n  details.PTSKnown = details.alignedSliding = true;\n  return drift;\n}\n\nexport function mergeDetails(\n  oldDetails: LevelDetails,\n  newDetails: LevelDetails,\n): void {\n  // Track the last initSegment processed. Initialize it to the last one on the timeline.\n  let currentInitSegment: Fragment | null = null;\n  const oldFragments = oldDetails.fragments;\n  for (let i = oldFragments.length - 1; i >= 0; i--) {\n    const oldInit = oldFragments[i].initSegment;\n    if (oldInit) {\n      currentInitSegment = oldInit;\n      break;\n    }\n  }\n\n  if (oldDetails.fragmentHint) {\n    // prevent PTS and duration from being adjusted on the next hint\n    delete oldDetails.fragmentHint.endPTS;\n  }\n  // check if old/new playlists have fragments in common\n  // loop through overlapping SN and update startPTS , cc, and duration if any found\n  let ccOffset = 0;\n  let PTSFrag;\n  mapFragmentIntersection(\n    oldDetails,\n    newDetails,\n    (oldFrag: Fragment, newFrag: Fragment) => {\n      if (oldFrag.relurl) {\n        // Do not compare CC if the old fragment has no url. This is a level.fragmentHint used by LL-HLS parts.\n        // It maybe be off by 1 if it was created before any parts or discontinuity tags were appended to the end\n        // of the playlist.\n        ccOffset = oldFrag.cc - newFrag.cc;\n      }\n      if (\n        Number.isFinite(oldFrag.startPTS) &&\n        Number.isFinite(oldFrag.endPTS)\n      ) {\n        newFrag.start = newFrag.startPTS = oldFrag.startPTS as number;\n        newFrag.startDTS = oldFrag.startDTS;\n        newFrag.maxStartPTS = oldFrag.maxStartPTS;\n\n        newFrag.endPTS = oldFrag.endPTS;\n        newFrag.endDTS = oldFrag.endDTS;\n        newFrag.minEndPTS = oldFrag.minEndPTS;\n        newFrag.duration =\n          (oldFrag.endPTS as number) - (oldFrag.startPTS as number);\n\n        if (newFrag.duration) {\n          PTSFrag = newFrag;\n        }\n\n        // PTS is known when any segment has startPTS and endPTS\n        newDetails.PTSKnown = newDetails.alignedSliding = true;\n      }\n      newFrag.elementaryStreams = oldFrag.elementaryStreams;\n      newFrag.loader = oldFrag.loader;\n      newFrag.stats = oldFrag.stats;\n      if (oldFrag.initSegment) {\n        newFrag.initSegment = oldFrag.initSegment;\n        currentInitSegment = oldFrag.initSegment;\n      }\n    },\n  );\n\n  if (currentInitSegment) {\n    const fragmentsToCheck = newDetails.fragmentHint\n      ? newDetails.fragments.concat(newDetails.fragmentHint)\n      : newDetails.fragments;\n    fragmentsToCheck.forEach((frag) => {\n      if (\n        frag &&\n        (!frag.initSegment ||\n          frag.initSegment.relurl === currentInitSegment?.relurl)\n      ) {\n        frag.initSegment = currentInitSegment;\n      }\n    });\n  }\n\n  if (newDetails.skippedSegments) {\n    newDetails.deltaUpdateFailed = newDetails.fragments.some((frag) => !frag);\n    if (newDetails.deltaUpdateFailed) {\n      logger.warn(\n        '[level-helper] Previous playlist missing segments skipped in delta playlist',\n      );\n      for (let i = newDetails.skippedSegments; i--; ) {\n        newDetails.fragments.shift();\n      }\n      newDetails.startSN = newDetails.fragments[0].sn as number;\n      newDetails.startCC = newDetails.fragments[0].cc;\n    } else if (newDetails.canSkipDateRanges) {\n      newDetails.dateRanges = mergeDateRanges(\n        oldDetails.dateRanges,\n        newDetails.dateRanges,\n        newDetails.recentlyRemovedDateranges,\n      );\n    }\n  }\n\n  const newFragments = newDetails.fragments;\n  if (ccOffset) {\n    logger.warn('discontinuity sliding from playlist, take drift into account');\n    for (let i = 0; i < newFragments.length; i++) {\n      newFragments[i].cc += ccOffset;\n    }\n  }\n  if (newDetails.skippedSegments) {\n    newDetails.startCC = newDetails.fragments[0].cc;\n  }\n\n  // Merge parts\n  mapPartIntersection(\n    oldDetails.partList,\n    newDetails.partList,\n    (oldPart: Part, newPart: Part) => {\n      newPart.elementaryStreams = oldPart.elementaryStreams;\n      newPart.stats = oldPart.stats;\n    },\n  );\n\n  // if at least one fragment contains PTS info, recompute PTS information for all fragments\n  if (PTSFrag) {\n    updateFragPTSDTS(\n      newDetails,\n      PTSFrag,\n      PTSFrag.startPTS,\n      PTSFrag.endPTS,\n      PTSFrag.startDTS,\n      PTSFrag.endDTS,\n    );\n  } else {\n    // ensure that delta is within oldFragments range\n    // also adjust sliding in case delta is 0 (we could have old=[50-60] and new=old=[50-61])\n    // in that case we also need to adjust start offset of all fragments\n    adjustSliding(oldDetails, newDetails);\n  }\n\n  if (newFragments.length) {\n    newDetails.totalduration = newDetails.edge - newFragments[0].start;\n  }\n\n  newDetails.driftStartTime = oldDetails.driftStartTime;\n  newDetails.driftStart = oldDetails.driftStart;\n  const advancedDateTime = newDetails.advancedDateTime;\n  if (newDetails.advanced && advancedDateTime) {\n    const edge = newDetails.edge;\n    if (!newDetails.driftStart) {\n      newDetails.driftStartTime = advancedDateTime;\n      newDetails.driftStart = edge;\n    }\n    newDetails.driftEndTime = advancedDateTime;\n    newDetails.driftEnd = edge;\n  } else {\n    newDetails.driftEndTime = oldDetails.driftEndTime;\n    newDetails.driftEnd = oldDetails.driftEnd;\n    newDetails.advancedDateTime = oldDetails.advancedDateTime;\n  }\n}\n\nfunction mergeDateRanges(\n  oldDateRanges: Record<string, DateRange>,\n  deltaDateRanges: Record<string, DateRange>,\n  recentlyRemovedDateranges: string[] | undefined,\n): Record<string, DateRange> {\n  const dateRanges = Object.assign({}, oldDateRanges);\n  if (recentlyRemovedDateranges) {\n    recentlyRemovedDateranges.forEach((id) => {\n      delete dateRanges[id];\n    });\n  }\n  Object.keys(deltaDateRanges).forEach((id) => {\n    const dateRange = new DateRange(deltaDateRanges[id].attr, dateRanges[id]);\n    if (dateRange.isValid) {\n      dateRanges[id] = dateRange;\n    } else {\n      logger.warn(\n        `Ignoring invalid Playlist Delta Update DATERANGE tag: \"${JSON.stringify(\n          deltaDateRanges[id].attr,\n        )}\"`,\n      );\n    }\n  });\n  return dateRanges;\n}\n\nexport function mapPartIntersection(\n  oldParts: Part[] | null,\n  newParts: Part[] | null,\n  intersectionFn: PartIntersection,\n) {\n  if (oldParts && newParts) {\n    let delta = 0;\n    for (let i = 0, len = oldParts.length; i <= len; i++) {\n      const oldPart = oldParts[i];\n      const newPart = newParts[i + delta];\n      if (\n        oldPart &&\n        newPart &&\n        oldPart.index === newPart.index &&\n        oldPart.fragment.sn === newPart.fragment.sn\n      ) {\n        intersectionFn(oldPart, newPart);\n      } else {\n        delta--;\n      }\n    }\n  }\n}\n\nexport function mapFragmentIntersection(\n  oldDetails: LevelDetails,\n  newDetails: LevelDetails,\n  intersectionFn: FragmentIntersection,\n): void {\n  const skippedSegments = newDetails.skippedSegments;\n  const start =\n    Math.max(oldDetails.startSN, newDetails.startSN) - newDetails.startSN;\n  const end =\n    (oldDetails.fragmentHint ? 1 : 0) +\n    (skippedSegments\n      ? newDetails.endSN\n      : Math.min(oldDetails.endSN, newDetails.endSN)) -\n    newDetails.startSN;\n  const delta = newDetails.startSN - oldDetails.startSN;\n  const newFrags = newDetails.fragmentHint\n    ? newDetails.fragments.concat(newDetails.fragmentHint)\n    : newDetails.fragments;\n  const oldFrags = oldDetails.fragmentHint\n    ? oldDetails.fragments.concat(oldDetails.fragmentHint)\n    : oldDetails.fragments;\n\n  for (let i = start; i <= end; i++) {\n    const oldFrag = oldFrags[delta + i];\n    let newFrag = newFrags[i];\n    if (skippedSegments && !newFrag && i < skippedSegments) {\n      // Fill in skipped segments in delta playlist\n      newFrag = newDetails.fragments[i] = oldFrag;\n    }\n    if (oldFrag && newFrag) {\n      intersectionFn(oldFrag, newFrag);\n    }\n  }\n}\n\nexport function adjustSliding(\n  oldDetails: LevelDetails,\n  newDetails: LevelDetails,\n): void {\n  const delta =\n    newDetails.startSN + newDetails.skippedSegments - oldDetails.startSN;\n  const oldFragments = oldDetails.fragments;\n  if (delta < 0 || delta >= oldFragments.length) {\n    return;\n  }\n  addSliding(newDetails, oldFragments[delta].start);\n}\n\nexport function addSliding(details: LevelDetails, start: number) {\n  if (start) {\n    const fragments = details.fragments;\n    for (let i = details.skippedSegments; i < fragments.length; i++) {\n      fragments[i].start += start;\n    }\n    if (details.fragmentHint) {\n      details.fragmentHint.start += start;\n    }\n  }\n}\n\nexport function computeReloadInterval(\n  newDetails: LevelDetails,\n  distanceToLiveEdgeMs: number = Infinity,\n): number {\n  let reloadInterval = 1000 * newDetails.targetduration;\n\n  if (newDetails.updated) {\n    // Use last segment duration when shorter than target duration and near live edge\n    const fragments = newDetails.fragments;\n    const liveEdgeMaxTargetDurations = 4;\n    if (\n      fragments.length &&\n      reloadInterval * liveEdgeMaxTargetDurations > distanceToLiveEdgeMs\n    ) {\n      const lastSegmentDuration =\n        fragments[fragments.length - 1].duration * 1000;\n      if (lastSegmentDuration < reloadInterval) {\n        reloadInterval = lastSegmentDuration;\n      }\n    }\n  } else {\n    // estimate = 'miss half average';\n    // follow HLS Spec, If the client reloads a Playlist file and finds that it has not\n    // changed then it MUST wait for a period of one-half the target\n    // duration before retrying.\n    reloadInterval /= 2;\n  }\n\n  return Math.round(reloadInterval);\n}\n\nexport function getFragmentWithSN(\n  level: Level,\n  sn: number,\n  fragCurrent: Fragment | null,\n): Fragment | null {\n  if (!level?.details) {\n    return null;\n  }\n  const levelDetails = level.details;\n  let fragment: Fragment | undefined =\n    levelDetails.fragments[sn - levelDetails.startSN];\n  if (fragment) {\n    return fragment;\n  }\n  fragment = levelDetails.fragmentHint;\n  if (fragment && fragment.sn === sn) {\n    return fragment;\n  }\n  if (sn < levelDetails.startSN && fragCurrent && fragCurrent.sn === sn) {\n    return fragCurrent;\n  }\n  return null;\n}\n\nexport function getPartWith(\n  level: Level,\n  sn: number,\n  partIndex: number,\n): Part | null {\n  if (!level?.details) {\n    return null;\n  }\n  return findPart(level.details?.partList, sn, partIndex);\n}\n\nexport function findPart(\n  partList: Part[] | null | undefined,\n  sn: number,\n  partIndex: number,\n): Part | null {\n  if (partList) {\n    for (let i = partList.length; i--; ) {\n      const part = partList[i];\n      if (part.index === partIndex && part.fragment.sn === sn) {\n        return part;\n      }\n    }\n  }\n  return null;\n}\n\nexport function reassignFragmentLevelIndexes(levels: Level[]) {\n  levels.forEach((level, index) => {\n    const { details } = level;\n    if (details?.fragments) {\n      details.fragments.forEach((fragment) => {\n        fragment.level = index;\n      });\n    }\n  });\n}\n", "import { ErrorDetails } from '../errors';\nimport type { LoadPolicy, LoaderConfig, RetryConfig } from '../config';\nimport type { ErrorData } from '../types/events';\nimport type { LoaderResponse } from '../types/loader';\n\nexport function isTimeoutError(error: ErrorData): boolean {\n  switch (error.details) {\n    case ErrorDetails.FRAG_LOAD_TIMEOUT:\n    case ErrorDetails.KEY_LOAD_TIMEOUT:\n    case ErrorDetails.LEVEL_LOAD_TIMEOUT:\n    case ErrorDetails.MANIFEST_LOAD_TIMEOUT:\n      return true;\n  }\n  return false;\n}\n\nexport function getRetryConfig(\n  loadPolicy: LoadPolicy,\n  error: ErrorData,\n): RetryConfig | null {\n  const isTimeout = isTimeoutError(error);\n  return loadPolicy.default[`${isTimeout ? 'timeout' : 'error'}Retry`];\n}\n\nexport function getRetryDelay(\n  retryConfig: RetryConfig,\n  retryCount: number,\n): number {\n  // exponential backoff capped to max retry delay\n  const backoffFactor =\n    retryConfig.backoff === 'linear' ? 1 : Math.pow(2, retryCount);\n  return Math.min(\n    backoffFactor * retryConfig.retryDelayMs,\n    retryConfig.maxRetryDelayMs,\n  );\n}\n\nexport function getLoaderConfigWithoutReties(\n  loderConfig: LoaderConfig,\n): LoaderConfig {\n  return {\n    ...loderConfig,\n    ...{\n      errorRetry: null,\n      timeoutRetry: null,\n    },\n  };\n}\n\nexport function shouldRetry(\n  retryConfig: RetryConfig | null | undefined,\n  retryCount: number,\n  isTimeout: boolean,\n  loaderResponse?: LoaderResponse | undefined,\n): retryConfig is RetryConfig & boolean {\n  if (!retryConfig) {\n    return false;\n  }\n  const httpStatus = loaderResponse?.code;\n  const retry =\n    retryCount < retryConfig.maxNumRetry &&\n    (retryForHttpStatus(httpStatus) || !!isTimeout);\n  return retryConfig.shouldRetry\n    ? retryConfig.shouldRetry(\n        retryConfig,\n        retryCount,\n        isTimeout,\n        loaderResponse,\n        retry,\n      )\n    : retry;\n}\n\nexport function retryForHttpStatus(httpStatus: number | undefined) {\n  // Do not retry on status 4xx, status 0 (CORS error), or undefined (decrypt/gap/parse error)\n  return (\n    (httpStatus === 0 && navigator.onLine === false) ||\n    (!!httpStatus && (httpStatus < 400 || httpStatus > 499))\n  );\n}\n", "type BinarySearchComparison<T> = (candidate: T) => -1 | 0 | 1;\n\nconst BinarySearch = {\n  /**\n   * Searches for an item in an array which matches a certain condition.\n   * This requires the condition to only match one item in the array,\n   * and for the array to be ordered.\n   *\n   * @param list The array to search.\n   * @param comparisonFn\n   *      Called and provided a candidate item as the first argument.\n   *      Should return:\n   *          > -1 if the item should be located at a lower index than the provided item.\n   *          > 1 if the item should be located at a higher index than the provided item.\n   *          > 0 if the item is the item you're looking for.\n   *\n   * @returns the object if found, otherwise returns null\n   */\n  search: function <T>(\n    list: T[],\n    comparisonFn: BinarySearchComparison<T>,\n  ): T | null {\n    let minIndex: number = 0;\n    let maxIndex: number = list.length - 1;\n    let currentIndex: number | null = null;\n    let currentElement: T | null = null;\n\n    while (minIndex <= maxIndex) {\n      currentIndex = ((minIndex + maxIndex) / 2) | 0;\n      currentElement = list[currentIndex];\n\n      const comparisonResult = comparisonFn(currentElement);\n      if (comparisonResult > 0) {\n        minIndex = currentIndex + 1;\n      } else if (comparisonResult < 0) {\n        maxIndex = currentIndex - 1;\n      } else {\n        return currentElement;\n      }\n    }\n\n    return null;\n  },\n};\n\nexport default BinarySearch;\n", "import BinarySearch from '../utils/binary-search';\nimport { Fragment } from '../loader/fragment';\n\n/**\n * Returns first fragment whose endPdt value exceeds the given PDT, or null.\n * @param fragments - The array of candidate fragments\n * @param PDTValue - The PDT value which must be exceeded\n * @param maxFragLookUpTolerance - The amount of time that a fragment's start/end can be within in order to be considered contiguous\n */\nexport function findFragmentByPDT(\n  fragments: Array<Fragment>,\n  PDTValue: number | null,\n  maxFragLookUpTolerance: number,\n): Fragment | null {\n  if (\n    PDTValue === null ||\n    !Array.isArray(fragments) ||\n    !fragments.length ||\n    !Number.isFinite(PDTValue)\n  ) {\n    return null;\n  }\n\n  // if less than start\n  const startPDT = fragments[0].programDateTime;\n  if (PDTValue < (startPDT || 0)) {\n    return null;\n  }\n\n  const endPDT = fragments[fragments.length - 1].endProgramDateTime;\n  if (PDTValue >= (endPDT || 0)) {\n    return null;\n  }\n\n  maxFragLookUpTolerance = maxFragLookUpTolerance || 0;\n  for (let seg = 0; seg < fragments.length; ++seg) {\n    const frag = fragments[seg];\n    if (pdtWithinToleranceTest(PDTValue, maxFragLookUpTolerance, frag)) {\n      return frag;\n    }\n  }\n\n  return null;\n}\n\n/**\n * Finds a fragment based on the SN of the previous fragment; or based on the needs of the current buffer.\n * This method compensates for small buffer gaps by applying a tolerance to the start of any candidate fragment, thus\n * breaking any traps which would cause the same fragment to be continuously selected within a small range.\n * @param fragPrevious - The last frag successfully appended\n * @param fragments - The array of candidate fragments\n * @param bufferEnd - The end of the contiguous buffered range the playhead is currently within\n * @param maxFragLookUpTolerance - The amount of time that a fragment's start/end can be within in order to be considered contiguous\n * @returns a matching fragment or null\n */\nexport function findFragmentByPTS(\n  fragPrevious: Fragment | null,\n  fragments: Array<Fragment>,\n  bufferEnd: number = 0,\n  maxFragLookUpTolerance: number = 0,\n  nextFragLookupTolerance: number = 0.005,\n): Fragment | null {\n  let fragNext: Fragment | null = null;\n  if (fragPrevious) {\n    fragNext =\n      fragments[\n        (fragPrevious.sn as number) - (fragments[0].sn as number) + 1\n      ] || null;\n    // check for buffer-end rounding error\n    const bufferEdgeError = fragPrevious.endDTS - bufferEnd;\n    if (bufferEdgeError > 0 && bufferEdgeError < 0.0000015) {\n      bufferEnd += 0.0000015;\n    }\n  } else if (bufferEnd === 0 && fragments[0].start === 0) {\n    fragNext = fragments[0];\n  }\n  // Prefer the next fragment if it's within tolerance\n  if (\n    fragNext &&\n    (((!fragPrevious || fragPrevious.level === fragNext.level) &&\n      fragmentWithinToleranceTest(\n        bufferEnd,\n        maxFragLookUpTolerance,\n        fragNext,\n      ) === 0) ||\n      fragmentWithinFastStartSwitch(\n        fragNext,\n        fragPrevious,\n        Math.min(nextFragLookupTolerance, maxFragLookUpTolerance),\n      ))\n  ) {\n    return fragNext;\n  }\n  // We might be seeking past the tolerance so find the best match\n  const foundFragment = BinarySearch.search(\n    fragments,\n    fragmentWithinToleranceTest.bind(null, bufferEnd, maxFragLookUpTolerance),\n  );\n  if (foundFragment && (foundFragment !== fragPrevious || !fragNext)) {\n    return foundFragment;\n  }\n  // If no match was found return the next fragment after fragPrevious, or null\n  return fragNext;\n}\n\nfunction fragmentWithinFastStartSwitch(\n  fragNext: Fragment,\n  fragPrevious: Fragment | null,\n  nextFragLookupTolerance: number,\n): boolean {\n  if (\n    fragPrevious &&\n    fragPrevious.start === 0 &&\n    fragPrevious.level < fragNext.level &&\n    (fragPrevious.endPTS || 0) > 0\n  ) {\n    const firstDuration = fragPrevious.tagList.reduce((duration, tag) => {\n      if (tag[0] === 'INF') {\n        duration += parseFloat(tag[1]);\n      }\n      return duration;\n    }, nextFragLookupTolerance);\n    return fragNext.start <= firstDuration;\n  }\n  return false;\n}\n\n/**\n * The test function used by the findFragmentBySn's BinarySearch to look for the best match to the current buffer conditions.\n * @param candidate - The fragment to test\n * @param bufferEnd - The end of the current buffered range the playhead is currently within\n * @param maxFragLookUpTolerance - The amount of time that a fragment's start can be within in order to be considered contiguous\n * @returns 0 if it matches, 1 if too low, -1 if too high\n */\nexport function fragmentWithinToleranceTest(\n  bufferEnd = 0,\n  maxFragLookUpTolerance = 0,\n  candidate: Fragment,\n) {\n  // eagerly accept an accurate match (no tolerance)\n  if (\n    candidate.start <= bufferEnd &&\n    candidate.start + candidate.duration > bufferEnd\n  ) {\n    return 0;\n  }\n  // offset should be within fragment boundary - config.maxFragLookUpTolerance\n  // this is to cope with situations like\n  // bufferEnd = 9.991\n  // frag[] : [0,10]\n  // frag[1] : [10,20]\n  // bufferEnd is within frag[0] range ... although what we are expecting is to return frag[1] here\n  //              frag start               frag start+duration\n  //                  |-----------------------------|\n  //              <--->                         <--->\n  //  ...--------><-----------------------------><---------....\n  // previous frag         matching fragment         next frag\n  //  return -1             return 0                 return 1\n  // logger.log(`level/sn/start/end/bufEnd:${level}/${candidate.sn}/${candidate.start}/${(candidate.start+candidate.duration)}/${bufferEnd}`);\n  // Set the lookup tolerance to be small enough to detect the current segment - ensures we don't skip over very small segments\n  const candidateLookupTolerance = Math.min(\n    maxFragLookUpTolerance,\n    candidate.duration + (candidate.deltaPTS ? candidate.deltaPTS : 0),\n  );\n  if (\n    candidate.start + candidate.duration - candidateLookupTolerance <=\n    bufferEnd\n  ) {\n    return 1;\n  } else if (\n    candidate.start - candidateLookupTolerance > bufferEnd &&\n    candidate.start\n  ) {\n    // if maxFragLookUpTolerance will have negative value then don't return -1 for first element\n    return -1;\n  }\n\n  return 0;\n}\n\n/**\n * The test function used by the findFragmentByPdt's BinarySearch to look for the best match to the current buffer conditions.\n * This function tests the candidate's program date time values, as represented in Unix time\n * @param candidate - The fragment to test\n * @param pdtBufferEnd - The Unix time representing the end of the current buffered range\n * @param maxFragLookUpTolerance - The amount of time that a fragment's start can be within in order to be considered contiguous\n * @returns true if contiguous, false otherwise\n */\nexport function pdtWithinToleranceTest(\n  pdtBufferEnd: number,\n  maxFragLookUpTolerance: number,\n  candidate: Fragment,\n): boolean {\n  const candidateLookupTolerance =\n    Math.min(\n      maxFragLookUpTolerance,\n      candidate.duration + (candidate.deltaPTS ? candidate.deltaPTS : 0),\n    ) * 1000;\n\n  // endProgramDateTime can be null, default to zero\n  const endProgramDateTime = candidate.endProgramDateTime || 0;\n  return endProgramDateTime - candidateLookupTolerance > pdtBufferEnd;\n}\n\nexport function findFragWithCC(\n  fragments: Fragment[],\n  cc: number,\n): Fragment | null {\n  return BinarySearch.search(fragments, (candidate) => {\n    if (candidate.cc < cc) {\n      return 1;\n    } else if (candidate.cc > cc) {\n      return -1;\n    } else {\n      return 0;\n    }\n  });\n}\n", "import { Events } from '../events';\nimport { ErrorDetails, ErrorTypes } from '../errors';\nimport { PlaylistContextType, PlaylistLevelType } from '../types/loader';\nimport {\n  getRetryConfig,\n  isTimeoutError,\n  shouldRetry,\n} from '../utils/error-helper';\nimport { findFragmentByPTS } from './fragment-finders';\nimport { HdcpLevel, HdcpLevels } from '../types/level';\nimport { logger } from '../utils/logger';\nimport type Hls from '../hls';\nimport type { RetryConfig } from '../config';\nimport type { NetworkComponentAPI } from '../types/component-api';\nimport type { ErrorData } from '../types/events';\nimport type { Fragment } from '../loader/fragment';\nimport type { LevelDetails } from '../loader/level-details';\n\nexport const enum NetworkErrorAction {\n  DoNothing = 0,\n  SendEndCallback = 1, // Reserved for future use\n  SendAlternateToPenaltyBox = 2,\n  RemoveAlternatePermanently = 3, // Reserved for future use\n  InsertDiscontinuity = 4, // Reserved for future use\n  RetryRequest = 5,\n}\n\nexport const enum ErrorActionFlags {\n  None = 0,\n  MoveAllAlternatesMatchingHost = 1,\n  MoveAllAlternatesMatchingHDCP = 1 << 1,\n  SwitchToSDR = 1 << 2, // Reserved for future use\n}\n\nexport type IErrorAction = {\n  action: NetworkErrorAction;\n  flags: ErrorActionFlags;\n  retryCount?: number;\n  retryConfig?: RetryConfig;\n  hdcpLevel?: HdcpLevel;\n  nextAutoLevel?: number;\n  resolved?: boolean;\n};\n\ntype PenalizedRendition = {\n  lastErrorPerfMs: number;\n  errors: ErrorData[];\n  details?: LevelDetails;\n};\n\ntype PenalizedRenditions = { [key: number]: PenalizedRendition };\n\nexport default class ErrorController implements NetworkComponentAPI {\n  private readonly hls: Hls;\n  private playlistError: number = 0;\n  private penalizedRenditions: PenalizedRenditions = {};\n  private log: (msg: any) => void;\n  private warn: (msg: any) => void;\n  private error: (msg: any) => void;\n\n  constructor(hls: Hls) {\n    this.hls = hls;\n    this.log = logger.log.bind(logger, `[info]:`);\n    this.warn = logger.warn.bind(logger, `[warning]:`);\n    this.error = logger.error.bind(logger, `[error]:`);\n    this.registerListeners();\n  }\n\n  private registerListeners() {\n    const hls = this.hls;\n    hls.on(Events.ERROR, this.onError, this);\n    hls.on(Events.MANIFEST_LOADING, this.onManifestLoading, this);\n    hls.on(Events.LEVEL_UPDATED, this.onLevelUpdated, this);\n  }\n\n  private unregisterListeners() {\n    const hls = this.hls;\n    if (!hls) {\n      return;\n    }\n    hls.off(Events.ERROR, this.onError, this);\n    hls.off(Events.ERROR, this.onErrorOut, this);\n    hls.off(Events.MANIFEST_LOADING, this.onManifestLoading, this);\n    hls.off(Events.LEVEL_UPDATED, this.onLevelUpdated, this);\n  }\n\n  destroy() {\n    this.unregisterListeners();\n    // @ts-ignore\n    this.hls = null;\n    this.penalizedRenditions = {};\n  }\n\n  startLoad(startPosition: number): void {}\n\n  stopLoad(): void {\n    this.playlistError = 0;\n  }\n\n  private getVariantLevelIndex(frag: Fragment | undefined): number {\n    return frag?.type === PlaylistLevelType.MAIN\n      ? frag.level\n      : this.hls.loadLevel;\n  }\n\n  private onManifestLoading() {\n    this.playlistError = 0;\n    this.penalizedRenditions = {};\n  }\n\n  private onLevelUpdated() {\n    this.playlistError = 0;\n  }\n\n  private onError(event: Events.ERROR, data: ErrorData) {\n    if (data.fatal) {\n      return;\n    }\n    const hls = this.hls;\n    const context = data.context;\n\n    switch (data.details) {\n      case ErrorDetails.FRAG_LOAD_ERROR:\n      case ErrorDetails.FRAG_LOAD_TIMEOUT:\n      case ErrorDetails.KEY_LOAD_ERROR:\n      case ErrorDetails.KEY_LOAD_TIMEOUT:\n        data.errorAction = this.getFragRetryOrSwitchAction(data);\n        return;\n      case ErrorDetails.FRAG_PARSING_ERROR:\n        // ignore empty segment errors marked as gap\n        if (data.frag?.gap) {\n          data.errorAction = {\n            action: NetworkErrorAction.DoNothing,\n            flags: ErrorActionFlags.None,\n          };\n          return;\n        }\n      // falls through\n      case ErrorDetails.FRAG_GAP:\n      case ErrorDetails.FRAG_DECRYPT_ERROR: {\n        // Switch level if possible, otherwise allow retry count to reach max error retries\n        data.errorAction = this.getFragRetryOrSwitchAction(data);\n        data.errorAction.action = NetworkErrorAction.SendAlternateToPenaltyBox;\n        return;\n      }\n      case ErrorDetails.LEVEL_EMPTY_ERROR:\n      case ErrorDetails.LEVEL_PARSING_ERROR:\n        {\n          // Only retry when empty and live\n          const levelIndex =\n            data.parent === PlaylistLevelType.MAIN\n              ? (data.level as number)\n              : hls.loadLevel;\n          if (\n            data.details === ErrorDetails.LEVEL_EMPTY_ERROR &&\n            !!data.context?.levelDetails?.live\n          ) {\n            data.errorAction = this.getPlaylistRetryOrSwitchAction(\n              data,\n              levelIndex,\n            );\n          } else {\n            // Escalate to fatal if not retrying or switching\n            data.levelRetry = false;\n            data.errorAction = this.getLevelSwitchAction(data, levelIndex);\n          }\n        }\n        return;\n      case ErrorDetails.LEVEL_LOAD_ERROR:\n      case ErrorDetails.LEVEL_LOAD_TIMEOUT:\n        if (typeof context?.level === 'number') {\n          data.errorAction = this.getPlaylistRetryOrSwitchAction(\n            data,\n            context.level,\n          );\n        }\n        return;\n      case ErrorDetails.AUDIO_TRACK_LOAD_ERROR:\n      case ErrorDetails.AUDIO_TRACK_LOAD_TIMEOUT:\n      case ErrorDetails.SUBTITLE_LOAD_ERROR:\n      case ErrorDetails.SUBTITLE_TRACK_LOAD_TIMEOUT:\n        if (context) {\n          const level = hls.levels[hls.loadLevel];\n          if (\n            level &&\n            ((context.type === PlaylistContextType.AUDIO_TRACK &&\n              level.hasAudioGroup(context.groupId)) ||\n              (context.type === PlaylistContextType.SUBTITLE_TRACK &&\n                level.hasSubtitleGroup(context.groupId)))\n          ) {\n            // Perform Pathway switch or Redundant failover if possible for fastest recovery\n            // otherwise allow playlist retry count to reach max error retries\n            data.errorAction = this.getPlaylistRetryOrSwitchAction(\n              data,\n              hls.loadLevel,\n            );\n            data.errorAction.action =\n              NetworkErrorAction.SendAlternateToPenaltyBox;\n            data.errorAction.flags =\n              ErrorActionFlags.MoveAllAlternatesMatchingHost;\n            return;\n          }\n        }\n        return;\n      case ErrorDetails.KEY_SYSTEM_STATUS_OUTPUT_RESTRICTED:\n        {\n          const level = hls.levels[hls.loadLevel];\n          const restrictedHdcpLevel = level?.attrs['HDCP-LEVEL'];\n          if (restrictedHdcpLevel) {\n            data.errorAction = {\n              action: NetworkErrorAction.SendAlternateToPenaltyBox,\n              flags: ErrorActionFlags.MoveAllAlternatesMatchingHDCP,\n              hdcpLevel: restrictedHdcpLevel,\n            };\n          } else {\n            this.keySystemError(data);\n          }\n        }\n        return;\n      case ErrorDetails.BUFFER_ADD_CODEC_ERROR:\n      case ErrorDetails.REMUX_ALLOC_ERROR:\n      case ErrorDetails.BUFFER_APPEND_ERROR:\n        data.errorAction = this.getLevelSwitchAction(\n          data,\n          data.level ?? hls.loadLevel,\n        );\n        return;\n      case ErrorDetails.INTERNAL_EXCEPTION:\n      case ErrorDetails.BUFFER_APPENDING_ERROR:\n      case ErrorDetails.BUFFER_FULL_ERROR:\n      case ErrorDetails.LEVEL_SWITCH_ERROR:\n      case ErrorDetails.BUFFER_STALLED_ERROR:\n      case ErrorDetails.BUFFER_SEEK_OVER_HOLE:\n      case ErrorDetails.BUFFER_NUDGE_ON_STALL:\n        data.errorAction = {\n          action: NetworkErrorAction.DoNothing,\n          flags: ErrorActionFlags.None,\n        };\n        return;\n    }\n\n    if (data.type === ErrorTypes.KEY_SYSTEM_ERROR) {\n      this.keySystemError(data);\n    }\n  }\n\n  private keySystemError(data: ErrorData) {\n    const levelIndex = this.getVariantLevelIndex(data.frag);\n    // Do not retry level. Escalate to fatal if switching levels fails.\n    data.levelRetry = false;\n    data.errorAction = this.getLevelSwitchAction(data, levelIndex);\n  }\n\n  private getPlaylistRetryOrSwitchAction(\n    data: ErrorData,\n    levelIndex: number | null | undefined,\n  ): IErrorAction {\n    const hls = this.hls;\n    const retryConfig = getRetryConfig(hls.config.playlistLoadPolicy, data);\n    const retryCount = this.playlistError++;\n    const retry = shouldRetry(\n      retryConfig,\n      retryCount,\n      isTimeoutError(data),\n      data.response,\n    );\n    if (retry) {\n      return {\n        action: NetworkErrorAction.RetryRequest,\n        flags: ErrorActionFlags.None,\n        retryConfig,\n        retryCount,\n      };\n    }\n    const errorAction = this.getLevelSwitchAction(data, levelIndex);\n    if (retryConfig) {\n      errorAction.retryConfig = retryConfig;\n      errorAction.retryCount = retryCount;\n    }\n    return errorAction;\n  }\n\n  private getFragRetryOrSwitchAction(data: ErrorData): IErrorAction {\n    const hls = this.hls;\n    // Share fragment error count accross media options (main, audio, subs)\n    // This allows for level based rendition switching when media option assets fail\n    const variantLevelIndex = this.getVariantLevelIndex(data.frag);\n    const level = hls.levels[variantLevelIndex];\n    const { fragLoadPolicy, keyLoadPolicy } = hls.config;\n    const retryConfig = getRetryConfig(\n      data.details.startsWith('key') ? keyLoadPolicy : fragLoadPolicy,\n      data,\n    );\n    const fragmentErrors = hls.levels.reduce(\n      (acc, level) => acc + level.fragmentError,\n      0,\n    );\n    // Switch levels when out of retried or level index out of bounds\n    if (level) {\n      if (data.details !== ErrorDetails.FRAG_GAP) {\n        level.fragmentError++;\n      }\n      const retry = shouldRetry(\n        retryConfig,\n        fragmentErrors,\n        isTimeoutError(data),\n        data.response,\n      );\n      if (retry) {\n        return {\n          action: NetworkErrorAction.RetryRequest,\n          flags: ErrorActionFlags.None,\n          retryConfig,\n          retryCount: fragmentErrors,\n        };\n      }\n    }\n    // Reach max retry count, or Missing level reference\n    // Switch to valid index\n    const errorAction = this.getLevelSwitchAction(data, variantLevelIndex);\n    // Add retry details to allow skipping of FRAG_PARSING_ERROR\n    if (retryConfig) {\n      errorAction.retryConfig = retryConfig;\n      errorAction.retryCount = fragmentErrors;\n    }\n    return errorAction;\n  }\n\n  private getLevelSwitchAction(\n    data: ErrorData,\n    levelIndex: number | null | undefined,\n  ): IErrorAction {\n    const hls = this.hls;\n    if (levelIndex === null || levelIndex === undefined) {\n      levelIndex = hls.loadLevel;\n    }\n    const level = this.hls.levels[levelIndex];\n    if (level) {\n      const errorDetails = data.details;\n      level.loadError++;\n      if (errorDetails === ErrorDetails.BUFFER_APPEND_ERROR) {\n        level.fragmentError++;\n      }\n      // Search for next level to retry\n      let nextLevel = -1;\n      const { levels, loadLevel, minAutoLevel, maxAutoLevel } = hls;\n      if (!hls.autoLevelEnabled) {\n        hls.loadLevel = -1;\n      }\n      const fragErrorType = data.frag?.type;\n      // Find alternate audio codec if available on audio codec error\n      const isAudioCodecError =\n        (fragErrorType === PlaylistLevelType.AUDIO &&\n          errorDetails === ErrorDetails.FRAG_PARSING_ERROR) ||\n        (data.sourceBufferName === 'audio' &&\n          (errorDetails === ErrorDetails.BUFFER_ADD_CODEC_ERROR ||\n            errorDetails === ErrorDetails.BUFFER_APPEND_ERROR));\n      const findAudioCodecAlternate =\n        isAudioCodecError &&\n        levels.some(({ audioCodec }) => level.audioCodec !== audioCodec);\n      // Find alternate video codec if available on video codec error\n      const isVideoCodecError =\n        data.sourceBufferName === 'video' &&\n        (errorDetails === ErrorDetails.BUFFER_ADD_CODEC_ERROR ||\n          errorDetails === ErrorDetails.BUFFER_APPEND_ERROR);\n      const findVideoCodecAlternate =\n        isVideoCodecError &&\n        levels.some(\n          ({ codecSet, audioCodec }) =>\n            level.codecSet !== codecSet && level.audioCodec === audioCodec,\n        );\n      const { type: playlistErrorType, groupId: playlistErrorGroupId } =\n        data.context ?? {};\n      for (let i = levels.length; i--; ) {\n        const candidate = (i + loadLevel) % levels.length;\n        if (\n          candidate !== loadLevel &&\n          candidate >= minAutoLevel &&\n          candidate <= maxAutoLevel &&\n          levels[candidate].loadError === 0\n        ) {\n          const levelCandidate = levels[candidate];\n          // Skip level switch if GAP tag is found in next level at same position\n          if (\n            errorDetails === ErrorDetails.FRAG_GAP &&\n            fragErrorType === PlaylistLevelType.MAIN &&\n            data.frag\n          ) {\n            const levelDetails = levels[candidate].details;\n            if (levelDetails) {\n              const fragCandidate = findFragmentByPTS(\n                data.frag,\n                levelDetails.fragments,\n                data.frag.start,\n              );\n              if (fragCandidate?.gap) {\n                continue;\n              }\n            }\n          } else if (\n            (playlistErrorType === PlaylistContextType.AUDIO_TRACK &&\n              levelCandidate.hasAudioGroup(playlistErrorGroupId)) ||\n            (playlistErrorType === PlaylistContextType.SUBTITLE_TRACK &&\n              levelCandidate.hasSubtitleGroup(playlistErrorGroupId))\n          ) {\n            // For audio/subs playlist errors find another group ID or fallthrough to redundant fail-over\n            continue;\n          } else if (\n            (fragErrorType === PlaylistLevelType.AUDIO &&\n              level.audioGroups?.some((groupId) =>\n                levelCandidate.hasAudioGroup(groupId),\n              )) ||\n            (fragErrorType === PlaylistLevelType.SUBTITLE &&\n              level.subtitleGroups?.some((groupId) =>\n                levelCandidate.hasSubtitleGroup(groupId),\n              )) ||\n            (findAudioCodecAlternate &&\n              level.audioCodec === levelCandidate.audioCodec) ||\n            (!findAudioCodecAlternate &&\n              level.audioCodec !== levelCandidate.audioCodec) ||\n            (findVideoCodecAlternate &&\n              level.codecSet === levelCandidate.codecSet)\n          ) {\n            // For video/audio/subs frag errors find another group ID or fallthrough to redundant fail-over\n            continue;\n          }\n          nextLevel = candidate;\n          break;\n        }\n      }\n      if (nextLevel > -1 && hls.loadLevel !== nextLevel) {\n        data.levelRetry = true;\n        this.playlistError = 0;\n        return {\n          action: NetworkErrorAction.SendAlternateToPenaltyBox,\n          flags: ErrorActionFlags.None,\n          nextAutoLevel: nextLevel,\n        };\n      }\n    }\n    // No levels to switch / Manual level selection / Level not found\n    // Resolve with Pathway switch, Redundant fail-over, or stay on lowest Level\n    return {\n      action: NetworkErrorAction.SendAlternateToPenaltyBox,\n      flags: ErrorActionFlags.MoveAllAlternatesMatchingHost,\n    };\n  }\n\n  public onErrorOut(event: Events.ERROR, data: ErrorData) {\n    switch (data.errorAction?.action) {\n      case NetworkErrorAction.DoNothing:\n        break;\n      case NetworkErrorAction.SendAlternateToPenaltyBox:\n        this.sendAlternateToPenaltyBox(data);\n        if (\n          !data.errorAction.resolved &&\n          data.details !== ErrorDetails.FRAG_GAP\n        ) {\n          data.fatal = true;\n        } else if (/MediaSource readyState: ended/.test(data.error.message)) {\n          this.warn(\n            `MediaSource ended after \"${data.sourceBufferName}\" sourceBuffer append error. Attempting to recover from media error.`,\n          );\n          this.hls.recoverMediaError();\n        }\n        break;\n      case NetworkErrorAction.RetryRequest:\n        // handled by stream and playlist/level controllers\n        break;\n    }\n\n    if (data.fatal) {\n      this.hls.stopLoad();\n      return;\n    }\n  }\n\n  private sendAlternateToPenaltyBox(data: ErrorData) {\n    const hls = this.hls;\n    const errorAction = data.errorAction;\n    if (!errorAction) {\n      return;\n    }\n    const { flags, hdcpLevel, nextAutoLevel } = errorAction;\n\n    switch (flags) {\n      case ErrorActionFlags.None:\n        this.switchLevel(data, nextAutoLevel);\n        break;\n      case ErrorActionFlags.MoveAllAlternatesMatchingHDCP:\n        if (hdcpLevel) {\n          hls.maxHdcpLevel = HdcpLevels[HdcpLevels.indexOf(hdcpLevel) - 1];\n          errorAction.resolved = true;\n        }\n        this.warn(\n          `Restricting playback to HDCP-LEVEL of \"${hls.maxHdcpLevel}\" or lower`,\n        );\n        break;\n    }\n    // If not resolved by previous actions try to switch to next level\n    if (!errorAction.resolved) {\n      this.switchLevel(data, nextAutoLevel);\n    }\n  }\n\n  private switchLevel(data: ErrorData, levelIndex: number | undefined) {\n    if (levelIndex !== undefined && data.errorAction) {\n      this.warn(`switching to level ${levelIndex} after ${data.details}`);\n      this.hls.nextAutoLevel = levelIndex;\n      data.errorAction.resolved = true;\n      // Stream controller is responsible for this but won't switch on false start\n      this.hls.nextLoadLevel = this.hls.nextAutoLevel;\n    }\n  }\n}\n", "import type Hls from '../hls';\nimport type { NetworkComponentAPI } from '../types/component-api';\nimport { getSkipValue, HlsSkip, HlsUrlParameters, Level } from '../types/level';\nimport { computeReloadInterval, mergeDetails } from '../utils/level-helper';\nimport { ErrorData } from '../types/events';\nimport { getRetryDelay, isTimeoutError } from '../utils/error-helper';\nimport { NetworkErrorAction } from './error-controller';\nimport { logger } from '../utils/logger';\nimport type { LevelDetails } from '../loader/level-details';\nimport type { MediaPlaylist } from '../types/media-playlist';\nimport type {\n  AudioTrackLoadedData,\n  LevelLoadedData,\n  TrackLoadedData,\n} from '../types/events';\n\nexport default class BasePlaylistController implements NetworkComponentAPI {\n  protected hls: Hls;\n  protected timer: number = -1;\n  protected requestScheduled: number = -1;\n  protected canLoad: boolean = false;\n  protected log: (msg: any) => void;\n  protected warn: (msg: any) => void;\n\n  constructor(hls: Hls, logPrefix: string) {\n    this.log = logger.log.bind(logger, `${logPrefix}:`);\n    this.warn = logger.warn.bind(logger, `${logPrefix}:`);\n    this.hls = hls;\n  }\n\n  public destroy(): void {\n    this.clearTimer();\n    // @ts-ignore\n    this.hls = this.log = this.warn = null;\n  }\n\n  protected clearTimer(): void {\n    if (this.timer !== -1) {\n      self.clearTimeout(this.timer);\n      this.timer = -1;\n    }\n  }\n\n  public startLoad(): void {\n    this.canLoad = true;\n    this.requestScheduled = -1;\n    this.loadPlaylist();\n  }\n\n  public stopLoad(): void {\n    this.canLoad = false;\n    this.clearTimer();\n  }\n\n  protected switchParams(\n    playlistUri: string,\n    previous: LevelDetails | undefined,\n    current: LevelDetails | undefined,\n  ): HlsUrlParameters | undefined {\n    const renditionReports = previous?.renditionReports;\n    if (renditionReports) {\n      let foundIndex = -1;\n      for (let i = 0; i < renditionReports.length; i++) {\n        const attr = renditionReports[i];\n        let uri: string;\n        try {\n          uri = new self.URL(attr.URI, previous.url).href;\n        } catch (error) {\n          logger.warn(\n            `Could not construct new URL for Rendition Report: ${error}`,\n          );\n          uri = attr.URI || '';\n        }\n        // Use exact match. Otherwise, the last partial match, if any, will be used\n        // (Playlist URI includes a query string that the Rendition Report does not)\n        if (uri === playlistUri) {\n          foundIndex = i;\n          break;\n        } else if (uri === playlistUri.substring(0, uri.length)) {\n          foundIndex = i;\n        }\n      }\n      if (foundIndex !== -1) {\n        const attr = renditionReports[foundIndex];\n        const msn = parseInt(attr['LAST-MSN']) || previous?.lastPartSn;\n        let part = parseInt(attr['LAST-PART']) || previous?.lastPartIndex;\n        if (this.hls.config.lowLatencyMode) {\n          const currentGoal = Math.min(\n            previous.age - previous.partTarget,\n            previous.targetduration,\n          );\n          if (part >= 0 && currentGoal > previous.partTarget) {\n            part += 1;\n          }\n        }\n        const skip = current && getSkipValue(current);\n        return new HlsUrlParameters(msn, part >= 0 ? part : undefined, skip);\n      }\n    }\n  }\n\n  protected loadPlaylist(hlsUrlParameters?: HlsUrlParameters): void {\n    if (this.requestScheduled === -1) {\n      this.requestScheduled = self.performance.now();\n    }\n    // Loading is handled by the subclasses\n  }\n\n  protected shouldLoadPlaylist(\n    playlist: Level | MediaPlaylist | null | undefined,\n  ): boolean {\n    return (\n      this.canLoad &&\n      !!playlist &&\n      !!playlist.url &&\n      (!playlist.details || playlist.details.live)\n    );\n  }\n\n  protected shouldReloadPlaylist(\n    playlist: Level | MediaPlaylist | null | undefined,\n  ): boolean {\n    return (\n      this.timer === -1 &&\n      this.requestScheduled === -1 &&\n      this.shouldLoadPlaylist(playlist)\n    );\n  }\n\n  protected playlistLoaded(\n    index: number,\n    data: LevelLoadedData | AudioTrackLoadedData | TrackLoadedData,\n    previousDetails?: LevelDetails,\n  ) {\n    const { details, stats } = data;\n\n    // Set last updated date-time\n    const now = self.performance.now();\n    const elapsed = stats.loading.first\n      ? Math.max(0, now - stats.loading.first)\n      : 0;\n    details.advancedDateTime = Date.now() - elapsed;\n\n    // if current playlist is a live playlist, arm a timer to reload it\n    if (details.live || previousDetails?.live) {\n      details.reloaded(previousDetails);\n      if (previousDetails) {\n        this.log(\n          `live playlist ${index} ${\n            details.advanced\n              ? 'REFRESHED ' + details.lastPartSn + '-' + details.lastPartIndex\n              : details.updated\n                ? 'UPDATED'\n                : 'MISSED'\n          }`,\n        );\n      }\n      // Merge live playlists to adjust fragment starts and fill in delta playlist skipped segments\n      if (previousDetails && details.fragments.length > 0) {\n        mergeDetails(previousDetails, details);\n      }\n      if (!this.canLoad || !details.live) {\n        return;\n      }\n      let deliveryDirectives: HlsUrlParameters | undefined;\n      let msn: number | undefined = undefined;\n      let part: number | undefined = undefined;\n      if (details.canBlockReload && details.endSN && details.advanced) {\n        // Load level with LL-HLS delivery directives\n        const lowLatencyMode = this.hls.config.lowLatencyMode;\n        const lastPartSn = details.lastPartSn;\n        const endSn = details.endSN;\n        const lastPartIndex = details.lastPartIndex;\n        const hasParts = lastPartIndex !== -1;\n        const lastPart = lastPartSn === endSn;\n        // When low latency mode is disabled, we'll skip part requests once the last part index is found\n        const nextSnStartIndex = lowLatencyMode ? 0 : lastPartIndex;\n        if (hasParts) {\n          msn = lastPart ? endSn + 1 : lastPartSn;\n          part = lastPart ? nextSnStartIndex : lastPartIndex + 1;\n        } else {\n          msn = endSn + 1;\n        }\n        // Low-Latency CDN Tune-in: \"age\" header and time since load indicates we're behind by more than one part\n        // Update directives to obtain the Playlist that has the estimated additional duration of media\n        const lastAdvanced = details.age;\n        const cdnAge = lastAdvanced + details.ageHeader;\n        let currentGoal = Math.min(\n          cdnAge - details.partTarget,\n          details.targetduration * 1.5,\n        );\n        if (currentGoal > 0) {\n          if (previousDetails && currentGoal > previousDetails.tuneInGoal) {\n            // If we attempted to get the next or latest playlist update, but currentGoal increased,\n            // then we either can't catchup, or the \"age\" header cannot be trusted.\n            this.warn(\n              `CDN Tune-in goal increased from: ${previousDetails.tuneInGoal} to: ${currentGoal} with playlist age: ${details.age}`,\n            );\n            currentGoal = 0;\n          } else {\n            const segments = Math.floor(currentGoal / details.targetduration);\n            msn += segments;\n            if (part !== undefined) {\n              const parts = Math.round(\n                (currentGoal % details.targetduration) / details.partTarget,\n              );\n              part += parts;\n            }\n            this.log(\n              `CDN Tune-in age: ${\n                details.ageHeader\n              }s last advanced ${lastAdvanced.toFixed(\n                2,\n              )}s goal: ${currentGoal} skip sn ${segments} to part ${part}`,\n            );\n          }\n          details.tuneInGoal = currentGoal;\n        }\n        deliveryDirectives = this.getDeliveryDirectives(\n          details,\n          data.deliveryDirectives,\n          msn,\n          part,\n        );\n        if (lowLatencyMode || !lastPart) {\n          this.loadPlaylist(deliveryDirectives);\n          return;\n        }\n      } else if (details.canBlockReload || details.canSkipUntil) {\n        deliveryDirectives = this.getDeliveryDirectives(\n          details,\n          data.deliveryDirectives,\n          msn,\n          part,\n        );\n      }\n      const bufferInfo = this.hls.mainForwardBufferInfo;\n      const position = bufferInfo ? bufferInfo.end - bufferInfo.len : 0;\n      const distanceToLiveEdgeMs = (details.edge - position) * 1000;\n      const reloadInterval = computeReloadInterval(\n        details,\n        distanceToLiveEdgeMs,\n      );\n      if (details.updated && now > this.requestScheduled + reloadInterval) {\n        this.requestScheduled = stats.loading.start;\n      }\n\n      if (msn !== undefined && details.canBlockReload) {\n        this.requestScheduled =\n          stats.loading.first +\n          reloadInterval -\n          (details.partTarget * 1000 || 1000);\n      } else if (\n        this.requestScheduled === -1 ||\n        this.requestScheduled + reloadInterval < now\n      ) {\n        this.requestScheduled = now;\n      } else if (this.requestScheduled - now <= 0) {\n        this.requestScheduled += reloadInterval;\n      }\n      let estimatedTimeUntilUpdate = this.requestScheduled - now;\n      estimatedTimeUntilUpdate = Math.max(0, estimatedTimeUntilUpdate);\n      this.log(\n        `reload live playlist ${index} in ${Math.round(\n          estimatedTimeUntilUpdate,\n        )} ms`,\n      );\n      // this.log(\n      //   `live reload ${details.updated ? 'REFRESHED' : 'MISSED'}\n      // reload in ${estimatedTimeUntilUpdate / 1000}\n      // round trip ${(stats.loading.end - stats.loading.start) / 1000}\n      // diff ${\n      //   (reloadInterval -\n      //     (estimatedTimeUntilUpdate +\n      //       stats.loading.end -\n      //       stats.loading.start)) /\n      //   1000\n      // }\n      // reload interval ${reloadInterval / 1000}\n      // target duration ${details.targetduration}\n      // distance to edge ${distanceToLiveEdgeMs / 1000}`\n      // );\n\n      this.timer = self.setTimeout(\n        () => this.loadPlaylist(deliveryDirectives),\n        estimatedTimeUntilUpdate,\n      );\n    } else {\n      this.clearTimer();\n    }\n  }\n\n  private getDeliveryDirectives(\n    details: LevelDetails,\n    previousDeliveryDirectives: HlsUrlParameters | null,\n    msn?: number,\n    part?: number,\n  ): HlsUrlParameters {\n    let skip = getSkipValue(details);\n    if (previousDeliveryDirectives?.skip && details.deltaUpdateFailed) {\n      msn = previousDeliveryDirectives.msn;\n      part = previousDeliveryDirectives.part;\n      skip = HlsSkip.No;\n    }\n    return new HlsUrlParameters(msn, part, skip);\n  }\n\n  protected checkRetry(errorEvent: ErrorData): boolean {\n    const errorDetails = errorEvent.details;\n    const isTimeout = isTimeoutError(errorEvent);\n    const errorAction = errorEvent.errorAction;\n    const { action, retryCount = 0, retryConfig } = errorAction || {};\n    const retry =\n      !!errorAction &&\n      !!retryConfig &&\n      (action === NetworkErrorAction.RetryRequest ||\n        (!errorAction.resolved &&\n          action === NetworkErrorAction.SendAlternateToPenaltyBox));\n    if (retry) {\n      this.requestScheduled = -1;\n      if (retryCount >= retryConfig.maxNumRetry) {\n        return false;\n      }\n      if (isTimeout && errorEvent.context?.deliveryDirectives) {\n        // The LL-HLS request already timed out so retry immediately\n        this.warn(\n          `Retrying playlist loading ${retryCount + 1}/${\n            retryConfig.maxNumRetry\n          } after \"${errorDetails}\" without delivery-directives`,\n        );\n        this.loadPlaylist();\n      } else {\n        const delay = getRetryDelay(retryConfig, retryCount);\n        // Schedule level/track reload\n        this.timer = self.setTimeout(() => this.loadPlaylist(), delay);\n        this.warn(\n          `Retrying playlist loading ${retryCount + 1}/${\n            retryConfig.maxNumRetry\n          } after \"${errorDetails}\" in ${delay}ms`,\n        );\n      }\n      // `levelRetry = true` used to inform other controllers that a retry is happening\n      errorEvent.levelRetry = true;\n      errorAction.resolved = true;\n    }\n    return retry;\n  }\n}\n", "/*\n * compute an Exponential Weighted moving average\n * - https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average\n *  - heavily inspired from shaka-player\n */\n\nclass EWMA {\n  public readonly halfLife: number;\n  private alpha_: number;\n  private estimate_: number;\n  private totalWeight_: number;\n\n  //  About half of the estimated value will be from the last |halfLife| samples by weight.\n  constructor(halfLife: number, estimate: number = 0, weight: number = 0) {\n    this.halfLife = halfLife;\n    // Larger values of alpha expire historical data more slowly.\n    this.alpha_ = halfLife ? Math.exp(Math.log(0.5) / halfLife) : 0;\n    this.estimate_ = estimate;\n    this.totalWeight_ = weight;\n  }\n\n  sample(weight: number, value: number) {\n    const adjAlpha = Math.pow(this.alpha_, weight);\n    this.estimate_ = value * (1 - adjAlpha) + adjAlpha * this.estimate_;\n    this.totalWeight_ += weight;\n  }\n\n  getTotalWeight(): number {\n    return this.totalWeight_;\n  }\n\n  getEstimate(): number {\n    if (this.alpha_) {\n      const zeroFactor = 1 - Math.pow(this.alpha_, this.totalWeight_);\n      if (zeroFactor) {\n        return this.estimate_ / zeroFactor;\n      }\n    }\n    return this.estimate_;\n  }\n}\n\nexport default EWMA;\n", "/*\n * EWMA Bandwidth Estimator\n *  - heavily inspired from shaka-player\n * Tracks bandwidth samples and estimates available bandwidth.\n * Based on the minimum of two exponentially-weighted moving averages with\n * different half-lives.\n */\n\nimport EWMA from '../utils/ewma';\n\nclass EwmaBandWidthEstimator {\n  private defaultEstimate_: number;\n  private minWeight_: number;\n  private minDelayMs_: number;\n  private slow_: EWMA;\n  private fast_: EWMA;\n  private defaultTTFB_: number;\n  private ttfb_: EWMA;\n\n  constructor(\n    slow: number,\n    fast: number,\n    defaultEstimate: number,\n    defaultTTFB: number = 100,\n  ) {\n    this.defaultEstimate_ = defaultEstimate;\n    this.minWeight_ = 0.001;\n    this.minDelayMs_ = 50;\n    this.slow_ = new EWMA(slow);\n    this.fast_ = new EWMA(fast);\n    this.defaultTTFB_ = defaultTTFB;\n    this.ttfb_ = new EWMA(slow);\n  }\n\n  update(slow: number, fast: number) {\n    const { slow_, fast_, ttfb_ } = this;\n    if (slow_.halfLife !== slow) {\n      this.slow_ = new EWMA(slow, slow_.getEstimate(), slow_.getTotalWeight());\n    }\n    if (fast_.halfLife !== fast) {\n      this.fast_ = new EWMA(fast, fast_.getEstimate(), fast_.getTotalWeight());\n    }\n    if (ttfb_.halfLife !== slow) {\n      this.ttfb_ = new EWMA(slow, ttfb_.getEstimate(), ttfb_.getTotalWeight());\n    }\n  }\n\n  sample(durationMs: number, numBytes: number) {\n    durationMs = Math.max(durationMs, this.minDelayMs_);\n    const numBits = 8 * numBytes;\n    // weight is duration in seconds\n    const durationS = durationMs / 1000;\n    // value is bandwidth in bits/s\n    const bandwidthInBps = numBits / durationS;\n    this.fast_.sample(durationS, bandwidthInBps);\n    this.slow_.sample(durationS, bandwidthInBps);\n  }\n\n  sampleTTFB(ttfb: number) {\n    // weight is frequency curve applied to TTFB in seconds\n    // (longer times have less weight with expected input under 1 second)\n    const seconds = ttfb / 1000;\n    const weight = Math.sqrt(2) * Math.exp(-Math.pow(seconds, 2) / 2);\n    this.ttfb_.sample(weight, Math.max(ttfb, 5));\n  }\n\n  canEstimate(): boolean {\n    return this.fast_.getTotalWeight() >= this.minWeight_;\n  }\n\n  getEstimate(): number {\n    if (this.canEstimate()) {\n      // console.log('slow estimate:'+ Math.round(this.slow_.getEstimate()));\n      // console.log('fast estimate:'+ Math.round(this.fast_.getEstimate()));\n      // Take the minimum of these two estimates.  This should have the effect of\n      // adapting down quickly, but up more slowly.\n      return Math.min(this.fast_.getEstimate(), this.slow_.getEstimate());\n    } else {\n      return this.defaultEstimate_;\n    }\n  }\n\n  getEstimateTTFB(): number {\n    if (this.ttfb_.getTotalWeight() >= this.minWeight_) {\n      return this.ttfb_.getEstimate();\n    } else {\n      return this.defaultTTFB_;\n    }\n  }\n\n  destroy() {}\n}\nexport default EwmaBandWidthEstimator;\n", "import { mimeTypeForCodec } from './codecs';\nimport type { Level, VideoRange } from '../types/level';\nimport type { AudioSelectionOption } from '../types/media-playlist';\nimport type { AudioTracksByGroup } from './rendition-helper';\n\nexport type MediaDecodingInfo = {\n  supported: boolean;\n  configurations: readonly MediaDecodingConfiguration[];\n  decodingInfoResults: readonly MediaCapabilitiesDecodingInfo[];\n  error?: Error;\n};\n\ntype BaseVideoConfiguration = Omit<VideoConfiguration, 'contentType'>;\n\nexport const SUPPORTED_INFO_DEFAULT: MediaDecodingInfo = {\n  supported: true,\n  configurations: [] as MediaDecodingConfiguration[],\n  decodingInfoResults: [\n    {\n      supported: true,\n      powerEfficient: true,\n      smooth: true,\n    },\n  ],\n} as const;\n\nexport const SUPPORTED_INFO_CACHE: Record<\n  string,\n  Promise<MediaCapabilitiesDecodingInfo>\n> = {};\n\nexport function requiresMediaCapabilitiesDecodingInfo(\n  level: Level,\n  audioTracksByGroup: AudioTracksByGroup,\n  currentVideoRange: VideoRange | undefined,\n  currentFrameRate: number,\n  currentBw: number,\n  audioPreference: AudioSelectionOption | undefined,\n): boolean {\n  // Only test support when configuration is exceeds minimum options\n  const audioGroups = level.audioCodec ? level.audioGroups : null;\n  const audioCodecPreference = audioPreference?.audioCodec;\n  const channelsPreference = audioPreference?.channels;\n  const maxChannels = channelsPreference\n    ? parseInt(channelsPreference)\n    : audioCodecPreference\n      ? Infinity\n      : 2;\n  let audioChannels: Record<string, number> | null = null;\n  if (audioGroups?.length) {\n    try {\n      if (audioGroups.length === 1 && audioGroups[0]) {\n        audioChannels = audioTracksByGroup.groups[audioGroups[0]].channels;\n      } else {\n        audioChannels = audioGroups.reduce(\n          (acc, groupId) => {\n            if (groupId) {\n              const audioTrackGroup = audioTracksByGroup.groups[groupId];\n              if (!audioTrackGroup) {\n                throw new Error(`Audio track group ${groupId} not found`);\n              }\n              // Sum all channel key values\n              Object.keys(audioTrackGroup.channels).forEach((key) => {\n                acc[key] = (acc[key] || 0) + audioTrackGroup.channels[key];\n              });\n            }\n            return acc;\n          },\n          { 2: 0 },\n        );\n      }\n    } catch (error) {\n      return true;\n    }\n  }\n  return (\n    (level.videoCodec !== undefined &&\n      ((level.width > 1920 && level.height > 1088) ||\n        (level.height > 1920 && level.width > 1088) ||\n        level.frameRate > Math.max(currentFrameRate, 30) ||\n        (level.videoRange !== 'SDR' &&\n          level.videoRange !== currentVideoRange) ||\n        level.bitrate > Math.max(currentBw, 8e6))) ||\n    (!!audioChannels &&\n      Number.isFinite(maxChannels) &&\n      Object.keys(audioChannels).some(\n        (channels) => parseInt(channels) > maxChannels,\n      ))\n  );\n}\n\nexport function getMediaDecodingInfoPromise(\n  level: Level,\n  audioTracksByGroup: AudioTracksByGroup,\n  mediaCapabilities: MediaCapabilities | undefined,\n): Promise<MediaDecodingInfo> {\n  const videoCodecs = level.videoCodec;\n  const audioCodecs = level.audioCodec;\n  if (!videoCodecs || !audioCodecs || !mediaCapabilities) {\n    return Promise.resolve(SUPPORTED_INFO_DEFAULT);\n  }\n\n  const baseVideoConfiguration: BaseVideoConfiguration = {\n    width: level.width,\n    height: level.height,\n    bitrate: Math.ceil(Math.max(level.bitrate * 0.9, level.averageBitrate)),\n    // Assume a framerate of 30fps since MediaCapabilities will not accept Level default of 0.\n    framerate: level.frameRate || 30,\n  };\n\n  const videoRange = level.videoRange;\n  if (videoRange !== 'SDR') {\n    baseVideoConfiguration.transferFunction =\n      videoRange.toLowerCase() as TransferFunction;\n  }\n\n  const configurations: MediaDecodingConfiguration[] = videoCodecs\n    .split(',')\n    .map((videoCodec) => ({\n      type: 'media-source',\n      video: {\n        ...baseVideoConfiguration,\n        contentType: mimeTypeForCodec(videoCodec, 'video'),\n      },\n    }));\n\n  if (audioCodecs && level.audioGroups) {\n    level.audioGroups.forEach((audioGroupId) => {\n      if (!audioGroupId) {\n        return;\n      }\n      audioTracksByGroup.groups[audioGroupId]?.tracks.forEach((audioTrack) => {\n        if (audioTrack.groupId === audioGroupId) {\n          const channels = audioTrack.channels || '';\n          const channelsNumber = parseFloat(channels);\n          if (Number.isFinite(channelsNumber) && channelsNumber > 2) {\n            configurations.push.apply(\n              configurations,\n              audioCodecs.split(',').map((audioCodec) => ({\n                type: 'media-source',\n                audio: {\n                  contentType: mimeTypeForCodec(audioCodec, 'audio'),\n                  channels: '' + channelsNumber,\n                  // spatialRendering:\n                  //   audioCodec === 'ec-3' && channels.indexOf('JOC'),\n                },\n              })),\n            );\n          }\n        }\n      });\n    });\n  }\n\n  return Promise.all(\n    configurations.map((configuration) => {\n      // Cache MediaCapabilities promises\n      const decodingInfoKey = getMediaDecodingInfoKey(configuration);\n      return (\n        SUPPORTED_INFO_CACHE[decodingInfoKey] ||\n        (SUPPORTED_INFO_CACHE[decodingInfoKey] =\n          mediaCapabilities.decodingInfo(configuration))\n      );\n    }),\n  )\n    .then((decodingInfoResults) => ({\n      supported: !decodingInfoResults.some((info) => !info.supported),\n      configurations,\n      decodingInfoResults,\n    }))\n    .catch((error) => ({\n      supported: false,\n      configurations,\n      decodingInfoResults: [] as MediaCapabilitiesDecodingInfo[],\n      error,\n    }));\n}\n\nfunction getMediaDecodingInfoKey(config: MediaDecodingConfiguration): string {\n  const { audio, video } = config;\n  const mediaConfig = video || audio;\n  if (mediaConfig) {\n    const codec = mediaConfig.contentType.split('\"')[1];\n    if (video) {\n      return `r${video.height}x${video.width}f${Math.ceil(video.framerate)}${\n        video.transferFunction || 'sd'\n      }_${codec}_${Math.ceil(video.bitrate / 1e5)}`;\n    }\n    if (audio) {\n      return `c${audio.channels}${audio.spatialRendering ? 's' : 'n'}_${codec}`;\n    }\n  }\n  return '';\n}\n", "import { type VideoRange, VideoRangeValues } from '../types/level';\nimport type { VideoSelectionOption } from '../types/media-playlist';\n\n/**\n * @returns Whether we can detect and validate HDR capability within the window context\n */\nexport function isHdrSupported() {\n  if (typeof matchMedia === 'function') {\n    const mediaQueryList = matchMedia('(dynamic-range: high)');\n    const badQuery = matchMedia('bad query');\n    if (mediaQueryList.media !== badQuery.media) {\n      return mediaQueryList.matches === true;\n    }\n  }\n  return false;\n}\n\n/**\n * Sanitizes inputs to return the active video selection options for HDR/SDR.\n * When both inputs are null:\n *\n *    `{ preferHDR: false, allowedVideoRanges: [] }`\n *\n * When `currentVideoRange` non-null, maintain the active range:\n *\n *    `{ preferHDR: currentVideoRange !== 'SDR', allowedVideoRanges: [currentVideoRange] }`\n *\n * When VideoSelectionOption non-null:\n *\n *  - Allow all video ranges if `allowedVideoRanges` unspecified.\n *  - If `preferHDR` is non-null use the value to filter `allowedVideoRanges`.\n *  - Else check window for HDR support and set `preferHDR` to the result.\n *\n * @param currentVideoRange\n * @param videoPreference\n */\nexport function getVideoSelectionOptions(\n  currentVideoRange: VideoRange | undefined,\n  videoPreference: VideoSelectionOption | undefined,\n) {\n  let preferHDR = false;\n  let allowedVideoRanges: Array<VideoRange> = [];\n\n  if (currentVideoRange) {\n    preferHDR = currentVideoRange !== 'SDR';\n    allowedVideoRanges = [currentVideoRange];\n  }\n\n  if (videoPreference) {\n    allowedVideoRanges =\n      videoPreference.allowedVideoRanges || VideoRangeValues.slice(0);\n    preferHDR =\n      videoPreference.preferHDR !== undefined\n        ? videoPreference.preferHDR\n        : isHdrSupported();\n\n    if (preferHDR) {\n      allowedVideoRanges = allowedVideoRanges.filter(\n        (range: VideoRange) => range !== 'SDR',\n      );\n    } else {\n      allowedVideoRanges = ['SDR'];\n    }\n  }\n\n  return {\n    preferHDR,\n    allowedVideoRanges,\n  };\n}\n", "import { codecsSetSelectionPreferenceValue } from './codecs';\nimport { getVideoSelectionOptions } from './hdr';\nimport { logger } from './logger';\nimport type { Level, VideoRange } from '../types/level';\nimport type {\n  AudioSelectionOption,\n  MediaPlaylist,\n  SubtitleSelectionOption,\n  VideoSelectionOption,\n} from '../types/media-playlist';\n\nexport type CodecSetTier = {\n  minBitrate: number;\n  minHeight: number;\n  minFramerate: number;\n  maxScore: number;\n  videoRanges: Record<string, number>;\n  channels: Record<string, number>;\n  hasDefaultAudio: boolean;\n  fragmentError: number;\n};\n\ntype AudioTrackGroup = {\n  tracks: MediaPlaylist[];\n  channels: Record<string, number>;\n  hasDefault: boolean;\n  hasAutoSelect: boolean;\n};\ntype StartParameters = {\n  codecSet: string | undefined;\n  videoRanges: Array<VideoRange>;\n  preferHDR: boolean;\n  minFramerate: number;\n  minBitrate: number;\n};\n\nexport function getStartCodecTier(\n  codecTiers: Record<string, CodecSetTier>,\n  currentVideoRange: VideoRange | undefined,\n  currentBw: number,\n  audioPreference: AudioSelectionOption | undefined,\n  videoPreference: VideoSelectionOption | undefined,\n): StartParameters {\n  const codecSets = Object.keys(codecTiers);\n  const channelsPreference = audioPreference?.channels;\n  const audioCodecPreference = audioPreference?.audioCodec;\n  const preferStereo = channelsPreference && parseInt(channelsPreference) === 2;\n  // Use first level set to determine stereo, and minimum resolution and framerate\n  let hasStereo = true;\n  let hasCurrentVideoRange = false;\n  let minHeight = Infinity;\n  let minFramerate = Infinity;\n  let minBitrate = Infinity;\n  let selectedScore = 0;\n  let videoRanges: Array<VideoRange> = [];\n\n  const { preferHDR, allowedVideoRanges } = getVideoSelectionOptions(\n    currentVideoRange,\n    videoPreference,\n  );\n\n  for (let i = codecSets.length; i--; ) {\n    const tier = codecTiers[codecSets[i]];\n    hasStereo = tier.channels[2] > 0;\n    minHeight = Math.min(minHeight, tier.minHeight);\n    minFramerate = Math.min(minFramerate, tier.minFramerate);\n    minBitrate = Math.min(minBitrate, tier.minBitrate);\n    const matchingVideoRanges = allowedVideoRanges.filter(\n      (range) => tier.videoRanges[range] > 0,\n    );\n    if (matchingVideoRanges.length > 0) {\n      hasCurrentVideoRange = true;\n      videoRanges = matchingVideoRanges;\n    }\n  }\n  minHeight = Number.isFinite(minHeight) ? minHeight : 0;\n  minFramerate = Number.isFinite(minFramerate) ? minFramerate : 0;\n  const maxHeight = Math.max(1080, minHeight);\n  const maxFramerate = Math.max(30, minFramerate);\n  minBitrate = Number.isFinite(minBitrate) ? minBitrate : currentBw;\n  currentBw = Math.max(minBitrate, currentBw);\n  // If there are no variants with matching preference, set currentVideoRange to undefined\n  if (!hasCurrentVideoRange) {\n    currentVideoRange = undefined;\n    videoRanges = [];\n  }\n  const codecSet = codecSets.reduce(\n    (selected: string | undefined, candidate: string) => {\n      // Remove candiates which do not meet bitrate, default audio, stereo or channels preference, 1080p or lower, 30fps or lower, or SDR/HDR selection if present\n      const candidateTier = codecTiers[candidate];\n      if (candidate === selected) {\n        return selected;\n      }\n      if (candidateTier.minBitrate > currentBw) {\n        logStartCodecCandidateIgnored(\n          candidate,\n          `min bitrate of ${candidateTier.minBitrate} > current estimate of ${currentBw}`,\n        );\n        return selected;\n      }\n      if (!candidateTier.hasDefaultAudio) {\n        logStartCodecCandidateIgnored(\n          candidate,\n          `no renditions with default or auto-select sound found`,\n        );\n        return selected;\n      }\n      if (\n        audioCodecPreference &&\n        candidate.indexOf(audioCodecPreference.substring(0, 4)) % 5 !== 0\n      ) {\n        logStartCodecCandidateIgnored(\n          candidate,\n          `audio codec preference \"${audioCodecPreference}\" not found`,\n        );\n        return selected;\n      }\n      if (channelsPreference && !preferStereo) {\n        if (!candidateTier.channels[channelsPreference]) {\n          logStartCodecCandidateIgnored(\n            candidate,\n            `no renditions with ${channelsPreference} channel sound found (channels options: ${Object.keys(\n              candidateTier.channels,\n            )})`,\n          );\n          return selected;\n        }\n      } else if (\n        (!audioCodecPreference || preferStereo) &&\n        hasStereo &&\n        candidateTier.channels['2'] === 0\n      ) {\n        logStartCodecCandidateIgnored(\n          candidate,\n          `no renditions with stereo sound found`,\n        );\n        return selected;\n      }\n      if (candidateTier.minHeight > maxHeight) {\n        logStartCodecCandidateIgnored(\n          candidate,\n          `min resolution of ${candidateTier.minHeight} > maximum of ${maxHeight}`,\n        );\n        return selected;\n      }\n      if (candidateTier.minFramerate > maxFramerate) {\n        logStartCodecCandidateIgnored(\n          candidate,\n          `min framerate of ${candidateTier.minFramerate} > maximum of ${maxFramerate}`,\n        );\n        return selected;\n      }\n      if (!videoRanges.some((range) => candidateTier.videoRanges[range] > 0)) {\n        logStartCodecCandidateIgnored(\n          candidate,\n          `no variants with VIDEO-RANGE of ${JSON.stringify(\n            videoRanges,\n          )} found`,\n        );\n        return selected;\n      }\n      if (candidateTier.maxScore < selectedScore) {\n        logStartCodecCandidateIgnored(\n          candidate,\n          `max score of ${candidateTier.maxScore} < selected max of ${selectedScore}`,\n        );\n        return selected;\n      }\n      // Remove candiates with less preferred codecs or more errors\n      if (\n        selected &&\n        (codecsSetSelectionPreferenceValue(candidate) >=\n          codecsSetSelectionPreferenceValue(selected) ||\n          candidateTier.fragmentError > codecTiers[selected].fragmentError)\n      ) {\n        return selected;\n      }\n      selectedScore = candidateTier.maxScore;\n      return candidate;\n    },\n    undefined,\n  );\n  return {\n    codecSet,\n    videoRanges,\n    preferHDR,\n    minFramerate,\n    minBitrate,\n  };\n}\n\nfunction logStartCodecCandidateIgnored(codeSet: string, reason: string) {\n  logger.log(\n    `[abr] start candidates with \"${codeSet}\" ignored because ${reason}`,\n  );\n}\n\nexport type AudioTracksByGroup = {\n  hasDefaultAudio: boolean;\n  hasAutoSelectAudio: boolean;\n  groups: Record<string, AudioTrackGroup>;\n};\n\nexport function getAudioTracksByGroup(allAudioTracks: MediaPlaylist[]) {\n  return allAudioTracks.reduce(\n    (audioTracksByGroup: AudioTracksByGroup, track) => {\n      let trackGroup = audioTracksByGroup.groups[track.groupId];\n      if (!trackGroup) {\n        trackGroup = audioTracksByGroup.groups[track.groupId] = {\n          tracks: [],\n          channels: { 2: 0 },\n          hasDefault: false,\n          hasAutoSelect: false,\n        };\n      }\n      trackGroup.tracks.push(track);\n      const channelsKey = track.channels || '2';\n      trackGroup.channels[channelsKey] =\n        (trackGroup.channels[channelsKey] || 0) + 1;\n      trackGroup.hasDefault = trackGroup.hasDefault || track.default;\n      trackGroup.hasAutoSelect = trackGroup.hasAutoSelect || track.autoselect;\n      if (trackGroup.hasDefault) {\n        audioTracksByGroup.hasDefaultAudio = true;\n      }\n      if (trackGroup.hasAutoSelect) {\n        audioTracksByGroup.hasAutoSelectAudio = true;\n      }\n      return audioTracksByGroup;\n    },\n    {\n      hasDefaultAudio: false,\n      hasAutoSelectAudio: false,\n      groups: {},\n    },\n  );\n}\n\nexport function getCodecTiers(\n  levels: Level[],\n  audioTracksByGroup: AudioTracksByGroup,\n  minAutoLevel: number,\n  maxAutoLevel: number,\n): Record<string, CodecSetTier> {\n  return levels\n    .slice(minAutoLevel, maxAutoLevel + 1)\n    .reduce((tiers: Record<string, CodecSetTier>, level) => {\n      if (!level.codecSet) {\n        return tiers;\n      }\n      const audioGroups = level.audioGroups;\n      let tier = tiers[level.codecSet];\n      if (!tier) {\n        tiers[level.codecSet] = tier = {\n          minBitrate: Infinity,\n          minHeight: Infinity,\n          minFramerate: Infinity,\n          maxScore: 0,\n          videoRanges: { SDR: 0 },\n          channels: { '2': 0 },\n          hasDefaultAudio: !audioGroups,\n          fragmentError: 0,\n        };\n      }\n      tier.minBitrate = Math.min(tier.minBitrate, level.bitrate);\n      const lesserWidthOrHeight = Math.min(level.height, level.width);\n      tier.minHeight = Math.min(tier.minHeight, lesserWidthOrHeight);\n      tier.minFramerate = Math.min(tier.minFramerate, level.frameRate);\n      tier.maxScore = Math.max(tier.maxScore, level.score);\n      tier.fragmentError += level.fragmentError;\n      tier.videoRanges[level.videoRange] =\n        (tier.videoRanges[level.videoRange] || 0) + 1;\n      if (__USE_ALT_AUDIO__ && audioGroups) {\n        audioGroups.forEach((audioGroupId) => {\n          if (!audioGroupId) {\n            return;\n          }\n          const audioGroup = audioTracksByGroup.groups[audioGroupId];\n          if (!audioGroup) {\n            return;\n          }\n          // Default audio is any group with DEFAULT=YES, or if missing then any group with AUTOSELECT=YES, or all variants\n          tier.hasDefaultAudio =\n            tier.hasDefaultAudio || audioTracksByGroup.hasDefaultAudio\n              ? audioGroup.hasDefault\n              : audioGroup.hasAutoSelect ||\n                (!audioTracksByGroup.hasDefaultAudio &&\n                  !audioTracksByGroup.hasAutoSelectAudio);\n          Object.keys(audioGroup.channels).forEach((channels) => {\n            tier.channels[channels] =\n              (tier.channels[channels] || 0) + audioGroup.channels[channels];\n          });\n        });\n      }\n      return tiers;\n    }, {});\n}\n\nexport function findMatchingOption(\n  option: MediaPlaylist | AudioSelectionOption | SubtitleSelectionOption,\n  tracks: MediaPlaylist[],\n  matchPredicate?: (\n    option: MediaPlaylist | AudioSelectionOption | SubtitleSelectionOption,\n    track: MediaPlaylist,\n  ) => boolean,\n): number {\n  if ('attrs' in option) {\n    const index = tracks.indexOf(option);\n    if (index !== -1) {\n      return index;\n    }\n  }\n  for (let i = 0; i < tracks.length; i++) {\n    const track = tracks[i];\n    if (matchesOption(option, track, matchPredicate)) {\n      return i;\n    }\n  }\n  return -1;\n}\n\nexport function matchesOption(\n  option: MediaPlaylist | AudioSelectionOption | SubtitleSelectionOption,\n  track: MediaPlaylist,\n  matchPredicate?: (\n    option: MediaPlaylist | AudioSelectionOption | SubtitleSelectionOption,\n    track: MediaPlaylist,\n  ) => boolean,\n): boolean {\n  const {\n    groupId,\n    name,\n    lang,\n    assocLang,\n    characteristics,\n    default: isDefault,\n  } = option;\n  const forced = (option as SubtitleSelectionOption).forced;\n  return (\n    (groupId === undefined || track.groupId === groupId) &&\n    (name === undefined || track.name === name) &&\n    (lang === undefined || track.lang === lang) &&\n    (lang === undefined || track.assocLang === assocLang) &&\n    (isDefault === undefined || track.default === isDefault) &&\n    (forced === undefined || track.forced === forced) &&\n    (characteristics === undefined ||\n      characteristicsMatch(characteristics, track.characteristics)) &&\n    (matchPredicate === undefined || matchPredicate(option, track))\n  );\n}\n\nfunction characteristicsMatch(\n  characteristicsA: string,\n  characteristicsB: string = '',\n): boolean {\n  const arrA = characteristicsA.split(',');\n  const arrB = characteristicsB.split(',');\n  // Expects each item to be unique:\n  return (\n    arrA.length === arrB.length && !arrA.some((el) => arrB.indexOf(el) === -1)\n  );\n}\n\nexport function audioMatchPredicate(\n  option: MediaPlaylist | AudioSelectionOption,\n  track: MediaPlaylist,\n) {\n  const { audioCodec, channels } = option;\n  return (\n    (audioCodec === undefined ||\n      (track.audioCodec || '').substring(0, 4) ===\n        audioCodec.substring(0, 4)) &&\n    (channels === undefined || channels === (track.channels || '2'))\n  );\n}\n\nexport function findClosestLevelWithAudioGroup(\n  option: MediaPlaylist | AudioSelectionOption,\n  levels: Level[],\n  allAudioTracks: MediaPlaylist[],\n  searchIndex: number,\n  matchPredicate: (\n    option: MediaPlaylist | AudioSelectionOption,\n    track: MediaPlaylist,\n  ) => boolean,\n): number {\n  const currentLevel = levels[searchIndex];\n  // Are there variants with same URI as current level?\n  // If so, find a match that does not require any level URI change\n  const variants = levels.reduce(\n    (variantMap: { [uri: string]: number[] }, level, index) => {\n      const uri = level.uri;\n      const renditions = variantMap[uri] || (variantMap[uri] = []);\n      renditions.push(index);\n      return variantMap;\n    },\n    {},\n  );\n  const renditions = variants[currentLevel.uri];\n  if (renditions.length > 1) {\n    searchIndex = Math.max.apply(Math, renditions);\n  }\n  // Find best match\n  const currentVideoRange = currentLevel.videoRange;\n  const currentFrameRate = currentLevel.frameRate;\n  const currentVideoCodec = currentLevel.codecSet.substring(0, 4);\n  const matchingVideo = searchDownAndUpList(\n    levels,\n    searchIndex,\n    (level: Level) => {\n      if (\n        level.videoRange !== currentVideoRange ||\n        level.frameRate !== currentFrameRate ||\n        level.codecSet.substring(0, 4) !== currentVideoCodec\n      ) {\n        return false;\n      }\n      const audioGroups = level.audioGroups;\n      const tracks = allAudioTracks.filter(\n        (track): boolean =>\n          !audioGroups || audioGroups.indexOf(track.groupId) !== -1,\n      );\n      return findMatchingOption(option, tracks, matchPredicate) > -1;\n    },\n  );\n  if (matchingVideo > -1) {\n    return matchingVideo;\n  }\n  return searchDownAndUpList(levels, searchIndex, (level: Level) => {\n    const audioGroups = level.audioGroups;\n    const tracks = allAudioTracks.filter(\n      (track): boolean =>\n        !audioGroups || audioGroups.indexOf(track.groupId) !== -1,\n    );\n    return findMatchingOption(option, tracks, matchPredicate) > -1;\n  });\n}\n\nfunction searchDownAndUpList(\n  arr: any[],\n  searchIndex: number,\n  predicate: (item: any) => boolean,\n): number {\n  for (let i = searchIndex; i; i--) {\n    if (predicate(arr[i])) {\n      return i;\n    }\n  }\n  for (let i = searchIndex + 1; i < arr.length; i++) {\n    if (predicate(arr[i])) {\n      return i;\n    }\n  }\n  return -1;\n}\n", "import EwmaBandWidthEstimator from '../utils/ewma-bandwidth-estimator';\nimport { Events } from '../events';\nimport { ErrorDetails } from '../errors';\nimport { PlaylistLevelType } from '../types/loader';\nimport { logger } from '../utils/logger';\nimport {\n  SUPPORTED_INFO_DEFAULT,\n  getMediaDecodingInfoPromise,\n  requiresMediaCapabilitiesDecodingInfo,\n} from '../utils/mediacapabilities-helper';\nimport {\n  getAudioTracksByGroup,\n  getCodecTiers,\n  getStartCodecTier,\n  type AudioTracksByGroup,\n  type CodecSetTier,\n} from '../utils/rendition-helper';\nimport type { Fragment } from '../loader/fragment';\nimport type { Part } from '../loader/fragment';\nimport type { Level, VideoRange } from '../types/level';\nimport type { LoaderStats } from '../types/loader';\nimport type Hls from '../hls';\nimport type {\n  FragLoadingData,\n  FragLoadedData,\n  FragBufferedData,\n  LevelLoadedData,\n  LevelSwitchingData,\n  ManifestLoadingData,\n  ErrorData,\n} from '../types/events';\nimport type { AbrComponentAPI } from '../types/component-api';\n\nclass AbrController implements AbrComponentAPI {\n  protected hls: Hls;\n  private lastLevelLoadSec: number = 0;\n  private lastLoadedFragLevel: number = -1;\n  private firstSelection: number = -1;\n  private _nextAutoLevel: number = -1;\n  private nextAutoLevelKey: string = '';\n  private audioTracksByGroup: AudioTracksByGroup | null = null;\n  private codecTiers: Record<string, CodecSetTier> | null = null;\n  private timer: number = -1;\n  private fragCurrent: Fragment | null = null;\n  private partCurrent: Part | null = null;\n  private bitrateTestDelay: number = 0;\n\n  public bwEstimator: EwmaBandWidthEstimator;\n\n  constructor(hls: Hls) {\n    this.hls = hls;\n    this.bwEstimator = this.initEstimator();\n    this.registerListeners();\n  }\n\n  public resetEstimator(abrEwmaDefaultEstimate?: number) {\n    if (abrEwmaDefaultEstimate) {\n      logger.log(`setting initial bwe to ${abrEwmaDefaultEstimate}`);\n      this.hls.config.abrEwmaDefaultEstimate = abrEwmaDefaultEstimate;\n    }\n    this.firstSelection = -1;\n    this.bwEstimator = this.initEstimator();\n  }\n\n  private initEstimator(): EwmaBandWidthEstimator {\n    const config = this.hls.config;\n    return new EwmaBandWidthEstimator(\n      config.abrEwmaSlowVoD,\n      config.abrEwmaFastVoD,\n      config.abrEwmaDefaultEstimate,\n    );\n  }\n\n  protected registerListeners() {\n    const { hls } = this;\n    hls.on(Events.MANIFEST_LOADING, this.onManifestLoading, this);\n    hls.on(Events.FRAG_LOADING, this.onFragLoading, this);\n    hls.on(Events.FRAG_LOADED, this.onFragLoaded, this);\n    hls.on(Events.FRAG_BUFFERED, this.onFragBuffered, this);\n    hls.on(Events.LEVEL_SWITCHING, this.onLevelSwitching, this);\n    hls.on(Events.LEVEL_LOADED, this.onLevelLoaded, this);\n    hls.on(Events.LEVELS_UPDATED, this.onLevelsUpdated, this);\n    hls.on(Events.MAX_AUTO_LEVEL_UPDATED, this.onMaxAutoLevelUpdated, this);\n    hls.on(Events.ERROR, this.onError, this);\n  }\n\n  protected unregisterListeners() {\n    const { hls } = this;\n    if (!hls) {\n      return;\n    }\n    hls.off(Events.MANIFEST_LOADING, this.onManifestLoading, this);\n    hls.off(Events.FRAG_LOADING, this.onFragLoading, this);\n    hls.off(Events.FRAG_LOADED, this.onFragLoaded, this);\n    hls.off(Events.FRAG_BUFFERED, this.onFragBuffered, this);\n    hls.off(Events.LEVEL_SWITCHING, this.onLevelSwitching, this);\n    hls.off(Events.LEVEL_LOADED, this.onLevelLoaded, this);\n    hls.off(Events.LEVELS_UPDATED, this.onLevelsUpdated, this);\n    hls.off(Events.MAX_AUTO_LEVEL_UPDATED, this.onMaxAutoLevelUpdated, this);\n    hls.off(Events.ERROR, this.onError, this);\n  }\n\n  public destroy() {\n    this.unregisterListeners();\n    this.clearTimer();\n    // @ts-ignore\n    this.hls = this._abandonRulesCheck = null;\n    this.fragCurrent = this.partCurrent = null;\n  }\n\n  protected onManifestLoading(\n    event: Events.MANIFEST_LOADING,\n    data: ManifestLoadingData,\n  ) {\n    this.lastLoadedFragLevel = -1;\n    this.firstSelection = -1;\n    this.lastLevelLoadSec = 0;\n    this.fragCurrent = this.partCurrent = null;\n    this.onLevelsUpdated();\n    this.clearTimer();\n  }\n\n  private onLevelsUpdated() {\n    if (this.lastLoadedFragLevel > -1 && this.fragCurrent) {\n      this.lastLoadedFragLevel = this.fragCurrent.level;\n    }\n    this._nextAutoLevel = -1;\n    this.onMaxAutoLevelUpdated();\n    this.codecTiers = null;\n    this.audioTracksByGroup = null;\n  }\n\n  private onMaxAutoLevelUpdated() {\n    this.firstSelection = -1;\n    this.nextAutoLevelKey = '';\n  }\n\n  protected onFragLoading(event: Events.FRAG_LOADING, data: FragLoadingData) {\n    const frag = data.frag;\n    if (this.ignoreFragment(frag)) {\n      return;\n    }\n    if (!frag.bitrateTest) {\n      this.fragCurrent = frag;\n      this.partCurrent = data.part ?? null;\n    }\n    this.clearTimer();\n    this.timer = self.setInterval(this._abandonRulesCheck, 100);\n  }\n\n  protected onLevelSwitching(\n    event: Events.LEVEL_SWITCHING,\n    data: LevelSwitchingData,\n  ): void {\n    this.clearTimer();\n  }\n\n  protected onError(event: Events.ERROR, data: ErrorData) {\n    if (data.fatal) {\n      return;\n    }\n    switch (data.details) {\n      case ErrorDetails.BUFFER_ADD_CODEC_ERROR:\n      case ErrorDetails.BUFFER_APPEND_ERROR:\n        // Reset last loaded level so that a new selection can be made after calling recoverMediaError\n        this.lastLoadedFragLevel = -1;\n        this.firstSelection = -1;\n        break;\n      case ErrorDetails.FRAG_LOAD_TIMEOUT: {\n        const frag = data.frag;\n        const { fragCurrent, partCurrent: part } = this;\n        if (\n          frag &&\n          fragCurrent &&\n          frag.sn === fragCurrent.sn &&\n          frag.level === fragCurrent.level\n        ) {\n          const now = performance.now();\n          const stats: LoaderStats = part ? part.stats : frag.stats;\n          const timeLoading = now - stats.loading.start;\n          const ttfb = stats.loading.first\n            ? stats.loading.first - stats.loading.start\n            : -1;\n          const loadedFirstByte = stats.loaded && ttfb > -1;\n          if (loadedFirstByte) {\n            const ttfbEstimate = this.bwEstimator.getEstimateTTFB();\n            this.bwEstimator.sample(\n              timeLoading - Math.min(ttfbEstimate, ttfb),\n              stats.loaded,\n            );\n          } else {\n            this.bwEstimator.sampleTTFB(timeLoading);\n          }\n        }\n        break;\n      }\n    }\n  }\n\n  private getTimeToLoadFrag(\n    timeToFirstByteSec: number,\n    bandwidth: number,\n    fragSizeBits: number,\n    isSwitch: boolean,\n  ): number {\n    const fragLoadSec = timeToFirstByteSec + fragSizeBits / bandwidth;\n    const playlistLoadSec = isSwitch ? this.lastLevelLoadSec : 0;\n    return fragLoadSec + playlistLoadSec;\n  }\n\n  protected onLevelLoaded(event: Events.LEVEL_LOADED, data: LevelLoadedData) {\n    const config = this.hls.config;\n    const { loading } = data.stats;\n    const timeLoadingMs = loading.end - loading.start;\n    if (Number.isFinite(timeLoadingMs)) {\n      this.lastLevelLoadSec = timeLoadingMs / 1000;\n    }\n    if (data.details.live) {\n      this.bwEstimator.update(config.abrEwmaSlowLive, config.abrEwmaFastLive);\n    } else {\n      this.bwEstimator.update(config.abrEwmaSlowVoD, config.abrEwmaFastVoD);\n    }\n  }\n\n  /*\n      This method monitors the download rate of the current fragment, and will downswitch if that fragment will not load\n      quickly enough to prevent underbuffering\n    */\n  private _abandonRulesCheck = () => {\n    const { fragCurrent: frag, partCurrent: part, hls } = this;\n    const { autoLevelEnabled, media } = hls;\n    if (!frag || !media) {\n      return;\n    }\n\n    const now = performance.now();\n    const stats: LoaderStats = part ? part.stats : frag.stats;\n    const duration = part ? part.duration : frag.duration;\n    const timeLoading = now - stats.loading.start;\n    const minAutoLevel = hls.minAutoLevel;\n    // If frag loading is aborted, complete, or from lowest level, stop timer and return\n    if (\n      stats.aborted ||\n      (stats.loaded && stats.loaded === stats.total) ||\n      frag.level <= minAutoLevel\n    ) {\n      this.clearTimer();\n      // reset forced auto level value so that next level will be selected\n      this._nextAutoLevel = -1;\n      return;\n    }\n\n    // This check only runs if we're in ABR mode and actually playing\n    if (\n      !autoLevelEnabled ||\n      media.paused ||\n      !media.playbackRate ||\n      !media.readyState\n    ) {\n      return;\n    }\n\n    const bufferInfo = hls.mainForwardBufferInfo;\n    if (bufferInfo === null) {\n      return;\n    }\n\n    const ttfbEstimate = this.bwEstimator.getEstimateTTFB();\n    const playbackRate = Math.abs(media.playbackRate);\n    // To maintain stable adaptive playback, only begin monitoring frag loading after half or more of its playback duration has passed\n    if (\n      timeLoading <=\n      Math.max(ttfbEstimate, 1000 * (duration / (playbackRate * 2)))\n    ) {\n      return;\n    }\n\n    // bufferStarvationDelay is an estimate of the amount time (in seconds) it will take to exhaust the buffer\n    const bufferStarvationDelay = bufferInfo.len / playbackRate;\n    const ttfb = stats.loading.first\n      ? stats.loading.first - stats.loading.start\n      : -1;\n    const loadedFirstByte = stats.loaded && ttfb > -1;\n    const bwEstimate: number = this.getBwEstimate();\n    const levels = hls.levels;\n    const level = levels[frag.level];\n    const expectedLen =\n      stats.total ||\n      Math.max(stats.loaded, Math.round((duration * level.averageBitrate) / 8));\n    let timeStreaming = loadedFirstByte ? timeLoading - ttfb : timeLoading;\n    if (timeStreaming < 1 && loadedFirstByte) {\n      timeStreaming = Math.min(timeLoading, (stats.loaded * 8) / bwEstimate);\n    }\n    const loadRate = loadedFirstByte\n      ? (stats.loaded * 1000) / timeStreaming\n      : 0;\n    // fragLoadDelay is an estimate of the time (in seconds) it will take to buffer the remainder of the fragment\n    const fragLoadedDelay = loadRate\n      ? (expectedLen - stats.loaded) / loadRate\n      : (expectedLen * 8) / bwEstimate + ttfbEstimate / 1000;\n    // Only downswitch if the time to finish loading the current fragment is greater than the amount of buffer left\n    if (fragLoadedDelay <= bufferStarvationDelay) {\n      return;\n    }\n\n    const bwe = loadRate ? loadRate * 8 : bwEstimate;\n    let fragLevelNextLoadedDelay: number = Number.POSITIVE_INFINITY;\n    let nextLoadLevel: number;\n    // Iterate through lower level and try to find the largest one that avoids rebuffering\n    for (\n      nextLoadLevel = frag.level - 1;\n      nextLoadLevel > minAutoLevel;\n      nextLoadLevel--\n    ) {\n      // compute time to load next fragment at lower level\n      // 8 = bits per byte (bps/Bps)\n      const levelNextBitrate = levels[nextLoadLevel].maxBitrate;\n      fragLevelNextLoadedDelay = this.getTimeToLoadFrag(\n        ttfbEstimate / 1000,\n        bwe,\n        duration * levelNextBitrate,\n        !levels[nextLoadLevel].details,\n      );\n      if (fragLevelNextLoadedDelay < bufferStarvationDelay) {\n        break;\n      }\n    }\n    // Only emergency switch down if it takes less time to load a new fragment at lowest level instead of continuing\n    // to load the current one\n    if (fragLevelNextLoadedDelay >= fragLoadedDelay) {\n      return;\n    }\n\n    // if estimated load time of new segment is completely unreasonable, ignore and do not emergency switch down\n    if (fragLevelNextLoadedDelay > duration * 10) {\n      return;\n    }\n    hls.nextLoadLevel = hls.nextAutoLevel = nextLoadLevel;\n    if (loadedFirstByte) {\n      // If there has been loading progress, sample bandwidth using loading time offset by minimum TTFB time\n      this.bwEstimator.sample(\n        timeLoading - Math.min(ttfbEstimate, ttfb),\n        stats.loaded,\n      );\n    } else {\n      // If there has been no loading progress, sample TTFB\n      this.bwEstimator.sampleTTFB(timeLoading);\n    }\n    const nextLoadLevelBitrate = levels[nextLoadLevel].maxBitrate;\n    if (\n      this.getBwEstimate() * this.hls.config.abrBandWidthUpFactor >\n      nextLoadLevelBitrate\n    ) {\n      this.resetEstimator(nextLoadLevelBitrate);\n    }\n\n    this.clearTimer();\n    logger.warn(`[abr] Fragment ${frag.sn}${\n      part ? ' part ' + part.index : ''\n    } of level ${frag.level} is loading too slowly;\n      Time to underbuffer: ${bufferStarvationDelay.toFixed(3)} s\n      Estimated load time for current fragment: ${fragLoadedDelay.toFixed(3)} s\n      Estimated load time for down switch fragment: ${fragLevelNextLoadedDelay.toFixed(\n        3,\n      )} s\n      TTFB estimate: ${ttfb | 0} ms\n      Current BW estimate: ${\n        Number.isFinite(bwEstimate) ? bwEstimate | 0 : 'Unknown'\n      } bps\n      New BW estimate: ${this.getBwEstimate() | 0} bps\n      Switching to level ${nextLoadLevel} @ ${nextLoadLevelBitrate | 0} bps`);\n    hls.trigger(Events.FRAG_LOAD_EMERGENCY_ABORTED, { frag, part, stats });\n  };\n\n  protected onFragLoaded(\n    event: Events.FRAG_LOADED,\n    { frag, part }: FragLoadedData,\n  ) {\n    const stats = part ? part.stats : frag.stats;\n    if (frag.type === PlaylistLevelType.MAIN) {\n      this.bwEstimator.sampleTTFB(stats.loading.first - stats.loading.start);\n    }\n    if (this.ignoreFragment(frag)) {\n      return;\n    }\n    // stop monitoring bw once frag loaded\n    this.clearTimer();\n    // reset forced auto level value so that next level will be selected\n    if (frag.level === this._nextAutoLevel) {\n      this._nextAutoLevel = -1;\n    }\n    this.firstSelection = -1;\n\n    // compute level average bitrate\n    if (this.hls.config.abrMaxWithRealBitrate) {\n      const duration = part ? part.duration : frag.duration;\n      const level = this.hls.levels[frag.level];\n      const loadedBytes =\n        (level.loaded ? level.loaded.bytes : 0) + stats.loaded;\n      const loadedDuration =\n        (level.loaded ? level.loaded.duration : 0) + duration;\n      level.loaded = { bytes: loadedBytes, duration: loadedDuration };\n      level.realBitrate = Math.round((8 * loadedBytes) / loadedDuration);\n    }\n    if (frag.bitrateTest) {\n      const fragBufferedData: FragBufferedData = {\n        stats,\n        frag,\n        part,\n        id: frag.type,\n      };\n      this.onFragBuffered(Events.FRAG_BUFFERED, fragBufferedData);\n      frag.bitrateTest = false;\n    } else {\n      // store level id after successful fragment load for playback\n      this.lastLoadedFragLevel = frag.level;\n    }\n  }\n\n  protected onFragBuffered(\n    event: Events.FRAG_BUFFERED,\n    data: FragBufferedData,\n  ) {\n    const { frag, part } = data;\n    const stats = part?.stats.loaded ? part.stats : frag.stats;\n\n    if (stats.aborted) {\n      return;\n    }\n    if (this.ignoreFragment(frag)) {\n      return;\n    }\n    // Use the difference between parsing and request instead of buffering and request to compute fragLoadingProcessing;\n    // rationale is that buffer appending only happens once media is attached. This can happen when config.startFragPrefetch\n    // is used. If we used buffering in that case, our BW estimate sample will be very large.\n    const processingMs =\n      stats.parsing.end -\n      stats.loading.start -\n      Math.min(\n        stats.loading.first - stats.loading.start,\n        this.bwEstimator.getEstimateTTFB(),\n      );\n    this.bwEstimator.sample(processingMs, stats.loaded);\n    stats.bwEstimate = this.getBwEstimate();\n    if (frag.bitrateTest) {\n      this.bitrateTestDelay = processingMs / 1000;\n    } else {\n      this.bitrateTestDelay = 0;\n    }\n  }\n\n  private ignoreFragment(frag: Fragment): boolean {\n    // Only count non-alt-audio frags which were actually buffered in our BW calculations\n    return frag.type !== PlaylistLevelType.MAIN || frag.sn === 'initSegment';\n  }\n\n  public clearTimer() {\n    if (this.timer > -1) {\n      self.clearInterval(this.timer);\n      this.timer = -1;\n    }\n  }\n\n  public get firstAutoLevel(): number {\n    const { maxAutoLevel, minAutoLevel } = this.hls;\n    const bwEstimate = this.getBwEstimate();\n    const maxStartDelay = this.hls.config.maxStarvationDelay;\n    const abrAutoLevel = this.findBestLevel(\n      bwEstimate,\n      minAutoLevel,\n      maxAutoLevel,\n      0,\n      maxStartDelay,\n      1,\n      1,\n    );\n    if (abrAutoLevel > -1) {\n      return abrAutoLevel;\n    }\n    const firstLevel = this.hls.firstLevel;\n    const clamped = Math.min(Math.max(firstLevel, minAutoLevel), maxAutoLevel);\n    logger.warn(\n      `[abr] Could not find best starting auto level. Defaulting to first in playlist ${firstLevel} clamped to ${clamped}`,\n    );\n    return clamped;\n  }\n\n  public get forcedAutoLevel(): number {\n    if (this.nextAutoLevelKey) {\n      return -1;\n    }\n    return this._nextAutoLevel;\n  }\n\n  // return next auto level\n  public get nextAutoLevel(): number {\n    const forcedAutoLevel = this.forcedAutoLevel;\n    const bwEstimator = this.bwEstimator;\n    const useEstimate = bwEstimator.canEstimate();\n    const loadedFirstFrag = this.lastLoadedFragLevel > -1;\n    // in case next auto level has been forced, and bw not available or not reliable, return forced value\n    if (\n      forcedAutoLevel !== -1 &&\n      (!useEstimate ||\n        !loadedFirstFrag ||\n        this.nextAutoLevelKey === this.getAutoLevelKey())\n    ) {\n      return forcedAutoLevel;\n    }\n\n    // compute next level using ABR logic\n    const nextABRAutoLevel =\n      useEstimate && loadedFirstFrag\n        ? this.getNextABRAutoLevel()\n        : this.firstAutoLevel;\n\n    // use forced auto level while it hasn't errored more than ABR selection\n    if (forcedAutoLevel !== -1) {\n      const levels = this.hls.levels;\n      if (\n        levels.length > Math.max(forcedAutoLevel, nextABRAutoLevel) &&\n        levels[forcedAutoLevel].loadError <= levels[nextABRAutoLevel].loadError\n      ) {\n        return forcedAutoLevel;\n      }\n    }\n\n    // save result until state has changed\n    this._nextAutoLevel = nextABRAutoLevel;\n    this.nextAutoLevelKey = this.getAutoLevelKey();\n\n    return nextABRAutoLevel;\n  }\n\n  private getAutoLevelKey(): string {\n    return `${this.getBwEstimate()}_${this.getStarvationDelay().toFixed(2)}`;\n  }\n\n  private getNextABRAutoLevel(): number {\n    const { fragCurrent, partCurrent, hls } = this;\n    const { maxAutoLevel, config, minAutoLevel } = hls;\n    const currentFragDuration = partCurrent\n      ? partCurrent.duration\n      : fragCurrent\n        ? fragCurrent.duration\n        : 0;\n    const avgbw = this.getBwEstimate();\n    // bufferStarvationDelay is the wall-clock time left until the playback buffer is exhausted.\n    const bufferStarvationDelay = this.getStarvationDelay();\n\n    let bwFactor = config.abrBandWidthFactor;\n    let bwUpFactor = config.abrBandWidthUpFactor;\n\n    // First, look to see if we can find a level matching with our avg bandwidth AND that could also guarantee no rebuffering at all\n    if (bufferStarvationDelay) {\n      const bestLevel = this.findBestLevel(\n        avgbw,\n        minAutoLevel,\n        maxAutoLevel,\n        bufferStarvationDelay,\n        0,\n        bwFactor,\n        bwUpFactor,\n      );\n      if (bestLevel >= 0) {\n        return bestLevel;\n      }\n    }\n    // not possible to get rid of rebuffering... try to find level that will guarantee less than maxStarvationDelay of rebuffering\n    let maxStarvationDelay = currentFragDuration\n      ? Math.min(currentFragDuration, config.maxStarvationDelay)\n      : config.maxStarvationDelay;\n\n    if (!bufferStarvationDelay) {\n      // in case buffer is empty, let's check if previous fragment was loaded to perform a bitrate test\n      const bitrateTestDelay = this.bitrateTestDelay;\n      if (bitrateTestDelay) {\n        // if it is the case, then we need to adjust our max starvation delay using maxLoadingDelay config value\n        // max video loading delay used in  automatic start level selection :\n        // in that mode ABR controller will ensure that video loading time (ie the time to fetch the first fragment at lowest quality level +\n        // the time to fetch the fragment at the appropriate quality level is less than ```maxLoadingDelay``` )\n        // cap maxLoadingDelay and ensure it is not bigger 'than bitrate test' frag duration\n        const maxLoadingDelay = currentFragDuration\n          ? Math.min(currentFragDuration, config.maxLoadingDelay)\n          : config.maxLoadingDelay;\n        maxStarvationDelay = maxLoadingDelay - bitrateTestDelay;\n        logger.info(\n          `[abr] bitrate test took ${Math.round(\n            1000 * bitrateTestDelay,\n          )}ms, set first fragment max fetchDuration to ${Math.round(\n            1000 * maxStarvationDelay,\n          )} ms`,\n        );\n        // don't use conservative factor on bitrate test\n        bwFactor = bwUpFactor = 1;\n      }\n    }\n    const bestLevel = this.findBestLevel(\n      avgbw,\n      minAutoLevel,\n      maxAutoLevel,\n      bufferStarvationDelay,\n      maxStarvationDelay,\n      bwFactor,\n      bwUpFactor,\n    );\n    logger.info(\n      `[abr] ${\n        bufferStarvationDelay ? 'rebuffering expected' : 'buffer is empty'\n      }, optimal quality level ${bestLevel}`,\n    );\n    if (bestLevel > -1) {\n      return bestLevel;\n    }\n    // If no matching level found, see if min auto level would be a better option\n    const minLevel = hls.levels[minAutoLevel];\n    const autoLevel = hls.levels[hls.loadLevel];\n    if (minLevel?.bitrate < autoLevel?.bitrate) {\n      return minAutoLevel;\n    }\n    // or if bitrate is not lower, continue to use loadLevel\n    return hls.loadLevel;\n  }\n\n  private getStarvationDelay(): number {\n    const hls = this.hls;\n    const media = hls.media;\n    if (!media) {\n      return Infinity;\n    }\n    // playbackRate is the absolute value of the playback rate; if media.playbackRate is 0, we use 1 to load as\n    // if we're playing back at the normal rate.\n    const playbackRate =\n      media && media.playbackRate !== 0 ? Math.abs(media.playbackRate) : 1.0;\n    const bufferInfo = hls.mainForwardBufferInfo;\n    return (bufferInfo ? bufferInfo.len : 0) / playbackRate;\n  }\n\n  private getBwEstimate(): number {\n    return this.bwEstimator.canEstimate()\n      ? this.bwEstimator.getEstimate()\n      : this.hls.config.abrEwmaDefaultEstimate;\n  }\n\n  private findBestLevel(\n    currentBw: number,\n    minAutoLevel: number,\n    maxAutoLevel: number,\n    bufferStarvationDelay: number,\n    maxStarvationDelay: number,\n    bwFactor: number,\n    bwUpFactor: number,\n  ): number {\n    const maxFetchDuration: number = bufferStarvationDelay + maxStarvationDelay;\n    const lastLoadedFragLevel = this.lastLoadedFragLevel;\n    const selectionBaseLevel =\n      lastLoadedFragLevel === -1 ? this.hls.firstLevel : lastLoadedFragLevel;\n    const { fragCurrent, partCurrent } = this;\n    const { levels, allAudioTracks, loadLevel, config } = this.hls;\n    if (levels.length === 1) {\n      return 0;\n    }\n    const level: Level | undefined = levels[selectionBaseLevel];\n    const live = !!level?.details?.live;\n    const firstSelection = loadLevel === -1 || lastLoadedFragLevel === -1;\n    let currentCodecSet: string | undefined;\n    let currentVideoRange: VideoRange | undefined = 'SDR';\n    let currentFrameRate = level?.frameRate || 0;\n\n    const { audioPreference, videoPreference } = config;\n    const audioTracksByGroup =\n      this.audioTracksByGroup ||\n      (this.audioTracksByGroup = getAudioTracksByGroup(allAudioTracks));\n    if (firstSelection) {\n      if (this.firstSelection !== -1) {\n        return this.firstSelection;\n      }\n      const codecTiers =\n        this.codecTiers ||\n        (this.codecTiers = getCodecTiers(\n          levels,\n          audioTracksByGroup,\n          minAutoLevel,\n          maxAutoLevel,\n        ));\n      const startTier = getStartCodecTier(\n        codecTiers,\n        currentVideoRange,\n        currentBw,\n        audioPreference,\n        videoPreference,\n      );\n      const { codecSet, videoRanges, minFramerate, minBitrate, preferHDR } =\n        startTier;\n      currentCodecSet = codecSet;\n      currentVideoRange = preferHDR\n        ? videoRanges[videoRanges.length - 1]\n        : videoRanges[0];\n      currentFrameRate = minFramerate;\n      currentBw = Math.max(currentBw, minBitrate);\n      logger.log(`[abr] picked start tier ${JSON.stringify(startTier)}`);\n    } else {\n      currentCodecSet = level?.codecSet;\n      currentVideoRange = level?.videoRange;\n    }\n\n    const currentFragDuration = partCurrent\n      ? partCurrent.duration\n      : fragCurrent\n        ? fragCurrent.duration\n        : 0;\n\n    const ttfbEstimateSec = this.bwEstimator.getEstimateTTFB() / 1000;\n    const levelsSkipped: number[] = [];\n    for (let i = maxAutoLevel; i >= minAutoLevel; i--) {\n      const levelInfo = levels[i];\n      const upSwitch = i > selectionBaseLevel;\n      if (!levelInfo) {\n        continue;\n      }\n      if (\n        __USE_MEDIA_CAPABILITIES__ &&\n        config.useMediaCapabilities &&\n        !levelInfo.supportedResult &&\n        !levelInfo.supportedPromise\n      ) {\n        const mediaCapabilities = navigator.mediaCapabilities as\n          | MediaCapabilities\n          | undefined;\n        if (\n          typeof mediaCapabilities?.decodingInfo === 'function' &&\n          requiresMediaCapabilitiesDecodingInfo(\n            levelInfo,\n            audioTracksByGroup,\n            currentVideoRange,\n            currentFrameRate,\n            currentBw,\n            audioPreference,\n          )\n        ) {\n          levelInfo.supportedPromise = getMediaDecodingInfoPromise(\n            levelInfo,\n            audioTracksByGroup,\n            mediaCapabilities,\n          );\n          levelInfo.supportedPromise.then((decodingInfo) => {\n            if (!this.hls) {\n              return;\n            }\n            levelInfo.supportedResult = decodingInfo;\n            const levels = this.hls.levels;\n            const index = levels.indexOf(levelInfo);\n            if (decodingInfo.error) {\n              logger.warn(\n                `[abr] MediaCapabilities decodingInfo error: \"${\n                  decodingInfo.error\n                }\" for level ${index} ${JSON.stringify(decodingInfo)}`,\n              );\n            } else if (!decodingInfo.supported) {\n              logger.warn(\n                `[abr] Unsupported MediaCapabilities decodingInfo result for level ${index} ${JSON.stringify(\n                  decodingInfo,\n                )}`,\n              );\n              if (index > -1 && levels.length > 1) {\n                logger.log(`[abr] Removing unsupported level ${index}`);\n                this.hls.removeLevel(index);\n              }\n            }\n          });\n        } else {\n          levelInfo.supportedResult = SUPPORTED_INFO_DEFAULT;\n        }\n      }\n\n      // skip candidates which change codec-family or video-range,\n      // and which decrease or increase frame-rate for up and down-switch respectfully\n      if (\n        (currentCodecSet && levelInfo.codecSet !== currentCodecSet) ||\n        (currentVideoRange && levelInfo.videoRange !== currentVideoRange) ||\n        (upSwitch && currentFrameRate > levelInfo.frameRate) ||\n        (!upSwitch &&\n          currentFrameRate > 0 &&\n          currentFrameRate < levelInfo.frameRate) ||\n        (levelInfo.supportedResult &&\n          !levelInfo.supportedResult.decodingInfoResults?.[0].smooth)\n      ) {\n        levelsSkipped.push(i);\n        continue;\n      }\n\n      const levelDetails = levelInfo.details;\n      const avgDuration =\n        (partCurrent\n          ? levelDetails?.partTarget\n          : levelDetails?.averagetargetduration) || currentFragDuration;\n\n      let adjustedbw: number;\n      // follow algorithm captured from stagefright :\n      // https://android.googlesource.com/platform/frameworks/av/+/master/media/libstagefright/httplive/LiveSession.cpp\n      // Pick the highest bandwidth stream below or equal to estimated bandwidth.\n      // consider only 80% of the available bandwidth, but if we are switching up,\n      // be even more conservative (70%) to avoid overestimating and immediately\n      // switching back.\n      if (!upSwitch) {\n        adjustedbw = bwFactor * currentBw;\n      } else {\n        adjustedbw = bwUpFactor * currentBw;\n      }\n\n      // Use average bitrate when starvation delay (buffer length) is gt or eq two segment durations and rebuffering is not expected (maxStarvationDelay > 0)\n      const bitrate: number =\n        currentFragDuration &&\n        bufferStarvationDelay >= currentFragDuration * 2 &&\n        maxStarvationDelay === 0\n          ? levels[i].averageBitrate\n          : levels[i].maxBitrate;\n      const fetchDuration: number = this.getTimeToLoadFrag(\n        ttfbEstimateSec,\n        adjustedbw,\n        bitrate * avgDuration,\n        levelDetails === undefined,\n      );\n\n      const canSwitchWithinTolerance =\n        // if adjusted bw is greater than level bitrate AND\n        adjustedbw >= bitrate &&\n        // no level change, or new level has no error history\n        (i === lastLoadedFragLevel ||\n          (levelInfo.loadError === 0 && levelInfo.fragmentError === 0)) &&\n        // fragment fetchDuration unknown OR live stream OR fragment fetchDuration less than max allowed fetch duration, then this level matches\n        // we don't account for max Fetch Duration for live streams, this is to avoid switching down when near the edge of live sliding window ...\n        // special case to support startLevel = -1 (bitrateTest) on live streams : in that case we should not exit loop so that findBestLevel will return -1\n        (fetchDuration <= ttfbEstimateSec ||\n          !Number.isFinite(fetchDuration) ||\n          (live && !this.bitrateTestDelay) ||\n          fetchDuration < maxFetchDuration);\n      if (canSwitchWithinTolerance) {\n        const forcedAutoLevel = this.forcedAutoLevel;\n        if (\n          i !== loadLevel &&\n          (forcedAutoLevel === -1 || forcedAutoLevel !== loadLevel)\n        ) {\n          if (levelsSkipped.length) {\n            logger.trace(\n              `[abr] Skipped level(s) ${levelsSkipped.join(\n                ',',\n              )} of ${maxAutoLevel} max with CODECS and VIDEO-RANGE:\"${\n                levels[levelsSkipped[0]].codecs\n              }\" ${levels[levelsSkipped[0]].videoRange}; not compatible with \"${\n                level.codecs\n              }\" ${currentVideoRange}`,\n            );\n          }\n          logger.info(\n            `[abr] switch candidate:${selectionBaseLevel}->${i} adjustedbw(${Math.round(\n              adjustedbw,\n            )})-bitrate=${Math.round(\n              adjustedbw - bitrate,\n            )} ttfb:${ttfbEstimateSec.toFixed(\n              1,\n            )} avgDuration:${avgDuration.toFixed(\n              1,\n            )} maxFetchDuration:${maxFetchDuration.toFixed(\n              1,\n            )} fetchDuration:${fetchDuration.toFixed(\n              1,\n            )} firstSelection:${firstSelection} codecSet:${currentCodecSet} videoRange:${currentVideoRange} hls.loadLevel:${loadLevel}`,\n          );\n        }\n        if (firstSelection) {\n          this.firstSelection = i;\n        }\n        // as we are looping from highest to lowest, this will return the best achievable quality level\n        return i;\n      }\n    }\n    // not enough time budget even with quality level 0 ... rebuffering might happen\n    return -1;\n  }\n\n  public set nextAutoLevel(nextLevel: number) {\n    const { maxAutoLevel, minAutoLevel } = this.hls;\n    const value = Math.min(Math.max(nextLevel, minAutoLevel), maxAutoLevel);\n    if (this._nextAutoLevel !== value) {\n      this.nextAutoLevelKey = '';\n      this._nextAutoLevel = value;\n    }\n  }\n}\n\nexport default AbrController;\n", "/**\n * @ignore\n * Sub-class specialization of EventHandler base class.\n *\n * TaskLoop allows to schedule a task function being called (optionnaly repeatedly) on the main loop,\n * scheduled asynchroneously, avoiding recursive calls in the same tick.\n *\n * The task itself is implemented in `doTick`. It can be requested and called for single execution\n * using the `tick` method.\n *\n * It will be assured that the task execution method (`tick`) only gets called once per main loop \"tick\",\n * no matter how often it gets requested for execution. Execution in further ticks will be scheduled accordingly.\n *\n * If further execution requests have already been scheduled on the next tick, it can be checked with `hasNextTick`,\n * and cancelled with `clearNextTick`.\n *\n * The task can be scheduled as an interval repeatedly with a period as parameter (see `setInterval`, `clearInterval`).\n *\n * Sub-classes need to implement the `doTick` method which will effectively have the task execution routine.\n *\n * Further explanations:\n *\n * The baseclass has a `tick` method that will schedule the doTick call. It may be called synchroneously\n * only for a stack-depth of one. On re-entrant calls, sub-sequent calls are scheduled for next main loop ticks.\n *\n * When the task execution (`tick` method) is called in re-entrant way this is detected and\n * we are limiting the task execution per call stack to exactly one, but scheduling/post-poning further\n * task processing on the next main loop iteration (also known as \"next tick\" in the Node/JS runtime lingo).\n */\nexport default class TaskLoop {\n  private readonly _boundTick: () => void;\n  private _tickTimer: number | null = null;\n  private _tickInterval: number | null = null;\n  private _tickCallCount = 0;\n\n  constructor() {\n    this._boundTick = this.tick.bind(this);\n  }\n\n  public destroy() {\n    this.onHandlerDestroying();\n    this.onHandlerDestroyed();\n  }\n\n  protected onHandlerDestroying() {\n    // clear all timers before unregistering from event bus\n    this.clearNextTick();\n    this.clearInterval();\n  }\n\n  protected onHandlerDestroyed() {}\n\n  public hasInterval(): boolean {\n    return !!this._tickInterval;\n  }\n\n  public hasNextTick(): boolean {\n    return !!this._tickTimer;\n  }\n\n  /**\n   * @param millis - Interval time (ms)\n   * @eturns True when interval has been scheduled, false when already scheduled (no effect)\n   */\n  public setInterval(millis: number): boolean {\n    if (!this._tickInterval) {\n      this._tickCallCount = 0;\n      this._tickInterval = self.setInterval(this._boundTick, millis);\n      return true;\n    }\n    return false;\n  }\n\n  /**\n   * @returns True when interval was cleared, false when none was set (no effect)\n   */\n  public clearInterval(): boolean {\n    if (this._tickInterval) {\n      self.clearInterval(this._tickInterval);\n      this._tickInterval = null;\n      return true;\n    }\n    return false;\n  }\n\n  /**\n   * @returns True when timeout was cleared, false when none was set (no effect)\n   */\n  public clearNextTick(): boolean {\n    if (this._tickTimer) {\n      self.clearTimeout(this._tickTimer);\n      this._tickTimer = null;\n      return true;\n    }\n    return false;\n  }\n\n  /**\n   * Will call the subclass doTick implementation in this main loop tick\n   * or in the next one (via setTimeout(,0)) in case it has already been called\n   * in this tick (in case this is a re-entrant call).\n   */\n  public tick(): void {\n    this._tickCallCount++;\n    if (this._tickCallCount === 1) {\n      this.doTick();\n      // re-entrant call to tick from previous doTick call stack\n      // -> schedule a call on the next main loop iteration to process this task processing request\n      if (this._tickCallCount > 1) {\n        // make sure only one timer exists at any time at max\n        this.tickImmediate();\n      }\n      this._tickCallCount = 0;\n    }\n  }\n\n  public tickImmediate(): void {\n    this.clearNextTick();\n    this._tickTimer = self.setTimeout(this._boundTick, 0);\n  }\n\n  /**\n   * For subclass to implement task logic\n   * @abstract\n   */\n  protected doTick(): void {}\n}\n", "import { Events } from '../events';\nimport { Fragment, Part } from '../loader/fragment';\nimport { PlaylistLevelType } from '../types/loader';\nimport type { SourceBufferName } from '../types/buffer';\nimport type {\n  FragmentBufferedRange,\n  FragmentEntity,\n  FragmentTimeRange,\n} from '../types/fragment-tracker';\nimport type { ComponentAPI } from '../types/component-api';\nimport type {\n  BufferAppendedData,\n  FragBufferedData,\n  FragLoadedData,\n} from '../types/events';\nimport type Hls from '../hls';\n\nexport const enum FragmentState {\n  NOT_LOADED = 'NOT_LOADED',\n  APPENDING = 'APPENDING',\n  PARTIAL = 'PARTIAL',\n  OK = 'OK',\n}\n\nexport class FragmentTracker implements ComponentAPI {\n  private activePartLists: { [key in PlaylistLevelType]?: Part[] } =\n    Object.create(null);\n  private endListFragments: { [key in PlaylistLevelType]?: FragmentEntity } =\n    Object.create(null);\n  private fragments: Partial<Record<string, FragmentEntity>> =\n    Object.create(null);\n  private timeRanges:\n    | {\n        [key in SourceBufferName]?: TimeRanges;\n      }\n    | null = Object.create(null);\n\n  private bufferPadding: number = 0.2;\n  private hls: Hls;\n  private hasGaps: boolean = false;\n\n  constructor(hls: Hls) {\n    this.hls = hls;\n\n    this._registerListeners();\n  }\n\n  private _registerListeners() {\n    const { hls } = this;\n    hls.on(Events.BUFFER_APPENDED, this.onBufferAppended, this);\n    hls.on(Events.FRAG_BUFFERED, this.onFragBuffered, this);\n    hls.on(Events.FRAG_LOADED, this.onFragLoaded, this);\n  }\n\n  private _unregisterListeners() {\n    const { hls } = this;\n    hls.off(Events.BUFFER_APPENDED, this.onBufferAppended, this);\n    hls.off(Events.FRAG_BUFFERED, this.onFragBuffered, this);\n    hls.off(Events.FRAG_LOADED, this.onFragLoaded, this);\n  }\n\n  public destroy() {\n    this._unregisterListeners();\n    // @ts-ignore\n    this.fragments =\n      // @ts-ignore\n      this.activePartLists =\n      // @ts-ignore\n      this.endListFragments =\n      this.timeRanges =\n        null;\n  }\n\n  /**\n   * Return a Fragment or Part with an appended range that matches the position and levelType\n   * Otherwise, return null\n   */\n  public getAppendedFrag(\n    position: number,\n    levelType: PlaylistLevelType,\n  ): Fragment | Part | null {\n    const activeParts = this.activePartLists[levelType];\n    if (activeParts) {\n      for (let i = activeParts.length; i--; ) {\n        const activePart = activeParts[i];\n        if (!activePart) {\n          break;\n        }\n        const appendedPTS = activePart.end;\n        if (\n          activePart.start <= position &&\n          appendedPTS !== null &&\n          position <= appendedPTS\n        ) {\n          return activePart;\n        }\n      }\n    }\n    return this.getBufferedFrag(position, levelType);\n  }\n\n  /**\n   * Return a buffered Fragment that matches the position and levelType.\n   * A buffered Fragment is one whose loading, parsing and appending is done (completed or \"partial\" meaning aborted).\n   * If not found any Fragment, return null\n   */\n  public getBufferedFrag(\n    position: number,\n    levelType: PlaylistLevelType,\n  ): Fragment | null {\n    const { fragments } = this;\n    const keys = Object.keys(fragments);\n    for (let i = keys.length; i--; ) {\n      const fragmentEntity = fragments[keys[i]];\n      if (fragmentEntity?.body.type === levelType && fragmentEntity.buffered) {\n        const frag = fragmentEntity.body;\n        if (frag.start <= position && position <= frag.end) {\n          return frag;\n        }\n      }\n    }\n    return null;\n  }\n\n  /**\n   * Partial fragments effected by coded frame eviction will be removed\n   * The browser will unload parts of the buffer to free up memory for new buffer data\n   * Fragments will need to be reloaded when the buffer is freed up, removing partial fragments will allow them to reload(since there might be parts that are still playable)\n   */\n  public detectEvictedFragments(\n    elementaryStream: SourceBufferName,\n    timeRange: TimeRanges,\n    playlistType: PlaylistLevelType,\n    appendedPart?: Part | null,\n  ) {\n    if (this.timeRanges) {\n      this.timeRanges[elementaryStream] = timeRange;\n    }\n    // Check if any flagged fragments have been unloaded\n    // excluding anything newer than appendedPartSn\n    const appendedPartSn = (appendedPart?.fragment.sn || -1) as number;\n    Object.keys(this.fragments).forEach((key) => {\n      const fragmentEntity = this.fragments[key];\n      if (!fragmentEntity) {\n        return;\n      }\n      if (appendedPartSn >= (fragmentEntity.body.sn as number)) {\n        return;\n      }\n      if (!fragmentEntity.buffered && !fragmentEntity.loaded) {\n        if (fragmentEntity.body.type === playlistType) {\n          this.removeFragment(fragmentEntity.body);\n        }\n        return;\n      }\n      const esData = fragmentEntity.range[elementaryStream];\n      if (!esData) {\n        return;\n      }\n      esData.time.some((time: FragmentTimeRange) => {\n        const isNotBuffered = !this.isTimeBuffered(\n          time.startPTS,\n          time.endPTS,\n          timeRange,\n        );\n        if (isNotBuffered) {\n          // Unregister partial fragment as it needs to load again to be reused\n          this.removeFragment(fragmentEntity.body);\n        }\n        return isNotBuffered;\n      });\n    });\n  }\n\n  /**\n   * Checks if the fragment passed in is loaded in the buffer properly\n   * Partially loaded fragments will be registered as a partial fragment\n   */\n  public detectPartialFragments(data: FragBufferedData) {\n    const timeRanges = this.timeRanges;\n    const { frag, part } = data;\n    if (!timeRanges || frag.sn === 'initSegment') {\n      return;\n    }\n\n    const fragKey = getFragmentKey(frag);\n    const fragmentEntity = this.fragments[fragKey];\n    if (!fragmentEntity || (fragmentEntity.buffered && frag.gap)) {\n      return;\n    }\n    const isFragHint = !frag.relurl;\n    Object.keys(timeRanges).forEach((elementaryStream: SourceBufferName) => {\n      const streamInfo = frag.elementaryStreams[elementaryStream];\n      if (!streamInfo) {\n        return;\n      }\n      const timeRange = timeRanges[elementaryStream] as TimeRanges;\n      const partial = isFragHint || streamInfo.partial === true;\n      fragmentEntity.range[elementaryStream] = this.getBufferedTimes(\n        frag,\n        part,\n        partial,\n        timeRange,\n      );\n    });\n    fragmentEntity.loaded = null;\n    if (Object.keys(fragmentEntity.range).length) {\n      fragmentEntity.buffered = true;\n      const endList = (fragmentEntity.body.endList =\n        frag.endList || fragmentEntity.body.endList);\n      if (endList) {\n        this.endListFragments[fragmentEntity.body.type] = fragmentEntity;\n      }\n      if (!isPartial(fragmentEntity)) {\n        // Remove older fragment parts from lookup after frag is tracked as buffered\n        this.removeParts((frag.sn as number) - 1, frag.type);\n      }\n    } else {\n      // remove fragment if nothing was appended\n      this.removeFragment(fragmentEntity.body);\n    }\n  }\n\n  private removeParts(snToKeep: number, levelType: PlaylistLevelType) {\n    const activeParts = this.activePartLists[levelType];\n    if (!activeParts) {\n      return;\n    }\n    this.activePartLists[levelType] = activeParts.filter(\n      (part) => (part.fragment.sn as number) >= snToKeep,\n    );\n  }\n\n  public fragBuffered(frag: Fragment, force?: true) {\n    const fragKey = getFragmentKey(frag);\n    let fragmentEntity = this.fragments[fragKey];\n    if (!fragmentEntity && force) {\n      fragmentEntity = this.fragments[fragKey] = {\n        body: frag,\n        appendedPTS: null,\n        loaded: null,\n        buffered: false,\n        range: Object.create(null),\n      };\n      if (frag.gap) {\n        this.hasGaps = true;\n      }\n    }\n    if (fragmentEntity) {\n      fragmentEntity.loaded = null;\n      fragmentEntity.buffered = true;\n    }\n  }\n\n  private getBufferedTimes(\n    fragment: Fragment,\n    part: Part | null,\n    partial: boolean,\n    timeRange: TimeRanges,\n  ): FragmentBufferedRange {\n    const buffered: FragmentBufferedRange = {\n      time: [],\n      partial,\n    };\n    const startPTS = fragment.start;\n    const endPTS = fragment.end;\n    const minEndPTS = fragment.minEndPTS || endPTS;\n    const maxStartPTS = fragment.maxStartPTS || startPTS;\n    for (let i = 0; i < timeRange.length; i++) {\n      const startTime = timeRange.start(i) - this.bufferPadding;\n      const endTime = timeRange.end(i) + this.bufferPadding;\n      if (maxStartPTS >= startTime && minEndPTS <= endTime) {\n        // Fragment is entirely contained in buffer\n        // No need to check the other timeRange times since it's completely playable\n        buffered.time.push({\n          startPTS: Math.max(startPTS, timeRange.start(i)),\n          endPTS: Math.min(endPTS, timeRange.end(i)),\n        });\n        break;\n      } else if (startPTS < endTime && endPTS > startTime) {\n        const start = Math.max(startPTS, timeRange.start(i));\n        const end = Math.min(endPTS, timeRange.end(i));\n        if (end > start) {\n          buffered.partial = true;\n          // Check for intersection with buffer\n          // Get playable sections of the fragment\n          buffered.time.push({\n            startPTS: start,\n            endPTS: end,\n          });\n        }\n      } else if (endPTS <= startTime) {\n        // No need to check the rest of the timeRange as it is in order\n        break;\n      }\n    }\n    return buffered;\n  }\n\n  /**\n   * Gets the partial fragment for a certain time\n   */\n  public getPartialFragment(time: number): Fragment | null {\n    let bestFragment: Fragment | null = null;\n    let timePadding: number;\n    let startTime: number;\n    let endTime: number;\n    let bestOverlap: number = 0;\n    const { bufferPadding, fragments } = this;\n    Object.keys(fragments).forEach((key) => {\n      const fragmentEntity = fragments[key];\n      if (!fragmentEntity) {\n        return;\n      }\n      if (isPartial(fragmentEntity)) {\n        startTime = fragmentEntity.body.start - bufferPadding;\n        endTime = fragmentEntity.body.end + bufferPadding;\n        if (time >= startTime && time <= endTime) {\n          // Use the fragment that has the most padding from start and end time\n          timePadding = Math.min(time - startTime, endTime - time);\n          if (bestOverlap <= timePadding) {\n            bestFragment = fragmentEntity.body;\n            bestOverlap = timePadding;\n          }\n        }\n      }\n    });\n    return bestFragment;\n  }\n\n  public isEndListAppended(type: PlaylistLevelType): boolean {\n    const lastFragmentEntity = this.endListFragments[type];\n    return (\n      lastFragmentEntity !== undefined &&\n      (lastFragmentEntity.buffered || isPartial(lastFragmentEntity))\n    );\n  }\n\n  public getState(fragment: Fragment): FragmentState {\n    const fragKey = getFragmentKey(fragment);\n    const fragmentEntity = this.fragments[fragKey];\n\n    if (fragmentEntity) {\n      if (!fragmentEntity.buffered) {\n        return FragmentState.APPENDING;\n      } else if (isPartial(fragmentEntity)) {\n        return FragmentState.PARTIAL;\n      } else {\n        return FragmentState.OK;\n      }\n    }\n\n    return FragmentState.NOT_LOADED;\n  }\n\n  private isTimeBuffered(\n    startPTS: number,\n    endPTS: number,\n    timeRange: TimeRanges,\n  ): boolean {\n    let startTime;\n    let endTime;\n    for (let i = 0; i < timeRange.length; i++) {\n      startTime = timeRange.start(i) - this.bufferPadding;\n      endTime = timeRange.end(i) + this.bufferPadding;\n      if (startPTS >= startTime && endPTS <= endTime) {\n        return true;\n      }\n\n      if (endPTS <= startTime) {\n        // No need to check the rest of the timeRange as it is in order\n        return false;\n      }\n    }\n\n    return false;\n  }\n\n  private onFragLoaded(event: Events.FRAG_LOADED, data: FragLoadedData) {\n    const { frag, part } = data;\n    // don't track initsegment (for which sn is not a number)\n    // don't track frags used for bitrateTest, they're irrelevant.\n    if (frag.sn === 'initSegment' || frag.bitrateTest) {\n      return;\n    }\n\n    // Fragment entity `loaded` FragLoadedData is null when loading parts\n    const loaded = part ? null : data;\n\n    const fragKey = getFragmentKey(frag);\n    this.fragments[fragKey] = {\n      body: frag,\n      appendedPTS: null,\n      loaded,\n      buffered: false,\n      range: Object.create(null),\n    };\n  }\n\n  private onBufferAppended(\n    event: Events.BUFFER_APPENDED,\n    data: BufferAppendedData,\n  ) {\n    const { frag, part, timeRanges } = data;\n    if (frag.sn === 'initSegment') {\n      return;\n    }\n    const playlistType = frag.type;\n    if (part) {\n      let activeParts = this.activePartLists[playlistType];\n      if (!activeParts) {\n        this.activePartLists[playlistType] = activeParts = [];\n      }\n      activeParts.push(part);\n    }\n    // Store the latest timeRanges loaded in the buffer\n    this.timeRanges = timeRanges;\n    Object.keys(timeRanges).forEach((elementaryStream: SourceBufferName) => {\n      const timeRange = timeRanges[elementaryStream] as TimeRanges;\n      this.detectEvictedFragments(\n        elementaryStream,\n        timeRange,\n        playlistType,\n        part,\n      );\n    });\n  }\n\n  private onFragBuffered(event: Events.FRAG_BUFFERED, data: FragBufferedData) {\n    this.detectPartialFragments(data);\n  }\n\n  private hasFragment(fragment: Fragment): boolean {\n    const fragKey = getFragmentKey(fragment);\n    return !!this.fragments[fragKey];\n  }\n\n  public hasParts(type: PlaylistLevelType): boolean {\n    return !!this.activePartLists[type]?.length;\n  }\n\n  public removeFragmentsInRange(\n    start: number,\n    end: number,\n    playlistType: PlaylistLevelType,\n    withGapOnly?: boolean,\n    unbufferedOnly?: boolean,\n  ) {\n    if (withGapOnly && !this.hasGaps) {\n      return;\n    }\n    Object.keys(this.fragments).forEach((key) => {\n      const fragmentEntity = this.fragments[key];\n      if (!fragmentEntity) {\n        return;\n      }\n      const frag = fragmentEntity.body;\n      if (frag.type !== playlistType || (withGapOnly && !frag.gap)) {\n        return;\n      }\n      if (\n        frag.start < end &&\n        frag.end > start &&\n        (fragmentEntity.buffered || unbufferedOnly)\n      ) {\n        this.removeFragment(frag);\n      }\n    });\n  }\n\n  public removeFragment(fragment: Fragment) {\n    const fragKey = getFragmentKey(fragment);\n    fragment.stats.loaded = 0;\n    fragment.clearElementaryStreamInfo();\n    const activeParts = this.activePartLists[fragment.type];\n    if (activeParts) {\n      const snToRemove = fragment.sn;\n      this.activePartLists[fragment.type] = activeParts.filter(\n        (part) => part.fragment.sn !== snToRemove,\n      );\n    }\n    delete this.fragments[fragKey];\n    if (fragment.endList) {\n      delete this.endListFragments[fragment.type];\n    }\n  }\n\n  public removeAllFragments() {\n    this.fragments = Object.create(null);\n    this.endListFragments = Object.create(null);\n    this.activePartLists = Object.create(null);\n    this.hasGaps = false;\n  }\n}\n\nfunction isPartial(fragmentEntity: FragmentEntity): boolean {\n  return (\n    fragmentEntity.buffered &&\n    (fragmentEntity.body.gap ||\n      fragmentEntity.range.video?.partial ||\n      fragmentEntity.range.audio?.partial ||\n      fragmentEntity.range.audiovideo?.partial)\n  );\n}\n\nfunction getFragmentKey(fragment: Fragment): string {\n  return `${fragment.type}_${fragment.level}_${fragment.sn}`;\n}\n", "/**\n * Provides methods dealing with buffer length retrieval for example.\n *\n * In general, a helper around HTML5 MediaElement TimeRanges gathered from `buffered` property.\n *\n * Also @see https://developer.mozilla.org/en-US/docs/Web/API/HTMLMediaElement/buffered\n */\n\nimport { logger } from './logger';\n\ntype BufferTimeRange = {\n  start: number;\n  end: number;\n};\n\nexport type Bufferable = {\n  buffered: TimeRanges;\n};\n\nexport type BufferInfo = {\n  len: number;\n  start: number;\n  end: number;\n  nextStart?: number;\n};\n\nconst noopBuffered: TimeRanges = {\n  length: 0,\n  start: () => 0,\n  end: () => 0,\n};\n\nexport class BufferHelper {\n  /**\n   * Return true if `media`'s buffered include `position`\n   */\n  static isBuffered(media: Bufferable, position: number): boolean {\n    try {\n      if (media) {\n        const buffered = BufferHelper.getBuffered(media);\n        for (let i = 0; i < buffered.length; i++) {\n          if (position >= buffered.start(i) && position <= buffered.end(i)) {\n            return true;\n          }\n        }\n      }\n    } catch (error) {\n      // this is to catch\n      // InvalidStateError: Failed to read the 'buffered' property from 'SourceBuffer':\n      // This SourceBuffer has been removed from the parent media source\n    }\n    return false;\n  }\n\n  static bufferInfo(\n    media: Bufferable | null,\n    pos: number,\n    maxHoleDuration: number,\n  ): BufferInfo {\n    try {\n      if (media) {\n        const vbuffered = BufferHelper.getBuffered(media);\n        const buffered: BufferTimeRange[] = [];\n        let i: number;\n        for (i = 0; i < vbuffered.length; i++) {\n          buffered.push({ start: vbuffered.start(i), end: vbuffered.end(i) });\n        }\n\n        return this.bufferedInfo(buffered, pos, maxHoleDuration);\n      }\n    } catch (error) {\n      // this is to catch\n      // InvalidStateError: Failed to read the 'buffered' property from 'SourceBuffer':\n      // This SourceBuffer has been removed from the parent media source\n    }\n    return { len: 0, start: pos, end: pos, nextStart: undefined };\n  }\n\n  static bufferedInfo(\n    buffered: BufferTimeRange[],\n    pos: number,\n    maxHoleDuration: number,\n  ): {\n    len: number;\n    start: number;\n    end: number;\n    nextStart?: number;\n  } {\n    pos = Math.max(0, pos);\n    // sort on buffer.start/smaller end (IE does not always return sorted buffered range)\n    buffered.sort(function (a, b) {\n      const diff = a.start - b.start;\n      if (diff) {\n        return diff;\n      } else {\n        return b.end - a.end;\n      }\n    });\n\n    let buffered2: BufferTimeRange[] = [];\n    if (maxHoleDuration) {\n      // there might be some small holes between buffer time range\n      // consider that holes smaller than maxHoleDuration are irrelevant and build another\n      // buffer time range representations that discards those holes\n      for (let i = 0; i < buffered.length; i++) {\n        const buf2len = buffered2.length;\n        if (buf2len) {\n          const buf2end = buffered2[buf2len - 1].end;\n          // if small hole (value between 0 or maxHoleDuration ) or overlapping (negative)\n          if (buffered[i].start - buf2end < maxHoleDuration) {\n            // merge overlapping time ranges\n            // update lastRange.end only if smaller than item.end\n            // e.g.  [ 1, 15] with  [ 2,8] => [ 1,15] (no need to modify lastRange.end)\n            // whereas [ 1, 8] with  [ 2,15] => [ 1,15] ( lastRange should switch from [1,8] to [1,15])\n            if (buffered[i].end > buf2end) {\n              buffered2[buf2len - 1].end = buffered[i].end;\n            }\n          } else {\n            // big hole\n            buffered2.push(buffered[i]);\n          }\n        } else {\n          // first value\n          buffered2.push(buffered[i]);\n        }\n      }\n    } else {\n      buffered2 = buffered;\n    }\n\n    let bufferLen = 0;\n\n    // bufferStartNext can possibly be undefined based on the conditional logic below\n    let bufferStartNext: number | undefined;\n\n    // bufferStart and bufferEnd are buffer boundaries around current video position\n    let bufferStart: number = pos;\n    let bufferEnd: number = pos;\n    for (let i = 0; i < buffered2.length; i++) {\n      const start = buffered2[i].start;\n      const end = buffered2[i].end;\n      // logger.log('buf start/end:' + buffered.start(i) + '/' + buffered.end(i));\n      if (pos + maxHoleDuration >= start && pos < end) {\n        // play position is inside this buffer TimeRange, retrieve end of buffer position and buffer length\n        bufferStart = start;\n        bufferEnd = end;\n        bufferLen = bufferEnd - pos;\n      } else if (pos + maxHoleDuration < start) {\n        bufferStartNext = start;\n        break;\n      }\n    }\n    return {\n      len: bufferLen,\n      start: bufferStart || 0,\n      end: bufferEnd || 0,\n      nextStart: bufferStartNext,\n    };\n  }\n\n  /**\n   * Safe method to get buffered property.\n   * SourceBuffer.buffered may throw if SourceBuffer is removed from it's MediaSource\n   */\n  static getBuffered(media: Bufferable): TimeRanges {\n    try {\n      return media.buffered;\n    } catch (e) {\n      logger.log('failed to get media.buffered', e);\n      return noopBuffered;\n    }\n  }\n}\n", "import type { RemuxerResult } from './remuxer';\nimport type { HlsChunkPerformanceTiming } from './loader';\nimport type { SourceBufferName } from './buffer';\n\nexport interface TransmuxerResult {\n  remuxResult: RemuxerResult;\n  chunkMeta: ChunkMetadata;\n}\n\nexport class ChunkMetadata {\n  public readonly level: number;\n  public readonly sn: number;\n  public readonly part: number;\n  public readonly id: number;\n  public readonly size: number;\n  public readonly partial: boolean;\n  public readonly transmuxing: HlsChunkPerformanceTiming =\n    getNewPerformanceTiming();\n  public readonly buffering: {\n    [key in SourceBufferName]: HlsChunkPerformanceTiming;\n  } = {\n    audio: getNewPerformanceTiming(),\n    video: getNewPerformanceTiming(),\n    audiovideo: getNewPerformanceTiming(),\n  };\n\n  constructor(\n    level: number,\n    sn: number,\n    id: number,\n    size = 0,\n    part = -1,\n    partial = false,\n  ) {\n    this.level = level;\n    this.sn = sn;\n    this.id = id;\n    this.size = size;\n    this.part = part;\n    this.partial = partial;\n  }\n}\n\nfunction getNewPerformanceTiming(): HlsChunkPerformanceTiming {\n  return { start: 0, executeStart: 0, executeEnd: 0, end: 0 };\n}\n", "import { logger } from './logger';\nimport { adjustSliding } from './level-helper';\n\nimport type { Fragment } from '../loader/fragment';\nimport type { LevelDetails } from '../loader/level-details';\n\nexport function findFirstFragWithCC(\n  fragments: Fragment[],\n  cc: number,\n): Fragment | null {\n  for (let i = 0, len = fragments.length; i < len; i++) {\n    if (fragments[i]?.cc === cc) {\n      return fragments[i];\n    }\n  }\n  return null;\n}\n\nexport function shouldAlignOnDiscontinuities(\n  lastFrag: Fragment | null,\n  switchDetails: LevelDetails | undefined,\n  details: LevelDetails,\n): switchDetails is LevelDetails & boolean {\n  if (switchDetails) {\n    if (\n      details.endCC > details.startCC ||\n      (lastFrag && lastFrag.cc < details.startCC)\n    ) {\n      return true;\n    }\n  }\n  return false;\n}\n\n// Find the first frag in the previous level which matches the CC of the first frag of the new level\nexport function findDiscontinuousReferenceFrag(\n  prevDetails: LevelDetails,\n  curDetails: LevelDetails,\n) {\n  const prevFrags = prevDetails.fragments;\n  const curFrags = curDetails.fragments;\n\n  if (!curFrags.length || !prevFrags.length) {\n    logger.log('No fragments to align');\n    return;\n  }\n\n  const prevStartFrag = findFirstFragWithCC(prevFrags, curFrags[0].cc);\n\n  if (!prevStartFrag || (prevStartFrag && !prevStartFrag.startPTS)) {\n    logger.log('No frag in previous level to align on');\n    return;\n  }\n\n  return prevStartFrag;\n}\n\nfunction adjustFragmentStart(frag: Fragment, sliding: number) {\n  if (frag) {\n    const start = frag.start + sliding;\n    frag.start = frag.startPTS = start;\n    frag.endPTS = start + frag.duration;\n  }\n}\n\nexport function adjustSlidingStart(sliding: number, details: LevelDetails) {\n  // Update segments\n  const fragments = details.fragments;\n  for (let i = 0, len = fragments.length; i < len; i++) {\n    adjustFragmentStart(fragments[i], sliding);\n  }\n  // Update LL-HLS parts at the end of the playlist\n  if (details.fragmentHint) {\n    adjustFragmentStart(details.fragmentHint, sliding);\n  }\n  details.alignedSliding = true;\n}\n\n/**\n * Using the parameters of the last level, this function computes PTS' of the new fragments so that they form a\n * contiguous stream with the last fragments.\n * The PTS of a fragment lets Hls.js know where it fits into a stream - by knowing every PTS, we know which fragment to\n * download at any given time. PTS is normally computed when the fragment is demuxed, so taking this step saves us time\n * and an extra download.\n * @param lastFrag\n * @param lastLevel\n * @param details\n */\nexport function alignStream(\n  lastFrag: Fragment | null,\n  switchDetails: LevelDetails | undefined,\n  details: LevelDetails,\n) {\n  if (!switchDetails) {\n    return;\n  }\n  alignDiscontinuities(lastFrag, details, switchDetails);\n  if (!details.alignedSliding && switchDetails) {\n    // If the PTS wasn't figured out via discontinuity sequence that means there was no CC increase within the level.\n    // Aligning via Program Date Time should therefore be reliable, since PDT should be the same within the same\n    // discontinuity sequence.\n    alignMediaPlaylistByPDT(details, switchDetails);\n  }\n  if (!details.alignedSliding && switchDetails && !details.skippedSegments) {\n    // Try to align on sn so that we pick a better start fragment.\n    // Do not perform this on playlists with delta updates as this is only to align levels on switch\n    // and adjustSliding only adjusts fragments after skippedSegments.\n    adjustSliding(switchDetails, details);\n  }\n}\n\n/**\n * Computes the PTS if a new level's fragments using the PTS of a fragment in the last level which shares the same\n * discontinuity sequence.\n * @param lastFrag - The last Fragment which shares the same discontinuity sequence\n * @param lastLevel - The details of the last loaded level\n * @param details - The details of the new level\n */\nfunction alignDiscontinuities(\n  lastFrag: Fragment | null,\n  details: LevelDetails,\n  switchDetails: LevelDetails | undefined,\n) {\n  if (shouldAlignOnDiscontinuities(lastFrag, switchDetails, details)) {\n    const referenceFrag = findDiscontinuousReferenceFrag(\n      switchDetails,\n      details,\n    );\n    if (referenceFrag && Number.isFinite(referenceFrag.start)) {\n      logger.log(\n        `Adjusting PTS using last level due to CC increase within current level ${details.url}`,\n      );\n      adjustSlidingStart(referenceFrag.start, details);\n    }\n  }\n}\n\n/**\n * Ensures appropriate time-alignment between renditions based on PDT.\n * This function assumes the timelines represented in `refDetails` are accurate, including the PDTs\n * for the last discontinuity sequence number shared by both playlists when present,\n * and uses the \"wallclock\"/PDT timeline as a cross-reference to `details`, adjusting the presentation\n * times/timelines of `details` accordingly.\n * Given the asynchronous nature of fetches and initial loads of live `main` and audio/subtitle tracks,\n * the primary purpose of this function is to ensure the \"local timelines\" of audio/subtitle tracks\n * are aligned to the main/video timeline, using PDT as the cross-reference/\"anchor\" that should\n * be consistent across playlists, per the HLS spec.\n * @param details - The details of the rendition you'd like to time-align (e.g. an audio rendition).\n * @param refDetails - The details of the reference rendition with start and PDT times for alignment.\n */\nexport function alignMediaPlaylistByPDT(\n  details: LevelDetails,\n  refDetails: LevelDetails,\n) {\n  if (!details.hasProgramDateTime || !refDetails.hasProgramDateTime) {\n    return;\n  }\n\n  const fragments = details.fragments;\n  const refFragments = refDetails.fragments;\n  if (!fragments.length || !refFragments.length) {\n    return;\n  }\n\n  // Calculate a delta to apply to all fragments according to the delta in PDT times and start times\n  // of a fragment in the reference details, and a fragment in the target details of the same discontinuity.\n  // If a fragment of the same discontinuity was not found use the middle fragment of both.\n  let refFrag: Fragment | null | undefined;\n  let frag: Fragment | null | undefined;\n  const targetCC = Math.min(refDetails.endCC, details.endCC);\n  if (refDetails.startCC < targetCC && details.startCC < targetCC) {\n    refFrag = findFirstFragWithCC(refFragments, targetCC);\n    frag = findFirstFragWithCC(fragments, targetCC);\n  }\n  if (!refFrag || !frag) {\n    refFrag = refFragments[Math.floor(refFragments.length / 2)];\n    frag =\n      findFirstFragWithCC(fragments, refFrag.cc) ||\n      fragments[Math.floor(fragments.length / 2)];\n  }\n  const refPDT = refFrag.programDateTime;\n  const targetPDT = frag.programDateTime;\n  if (!refPDT || !targetPDT) {\n    return;\n  }\n\n  const delta = (targetPDT - refPDT) / 1000 - (frag.start - refFrag.start);\n  adjustSlidingStart(delta, details);\n}\n", "import { ErrorTypes, ErrorDetails } from '../errors';\nimport { Fragment } from './fragment';\nimport {\n  Loader,\n  LoaderConfiguration,\n  FragmentLoaderContext,\n} from '../types/loader';\nimport { getLoaderConfigWithoutReties } from '../utils/error-helper';\nimport type { HlsConfig } from '../config';\nimport type { BaseSegment, Part } from './fragment';\nimport type {\n  ErrorData,\n  FragLoadedData,\n  PartsLoadedData,\n} from '../types/events';\n\nconst MIN_CHUNK_SIZE = Math.pow(2, 17); // 128kb\n\nexport default class FragmentLoader {\n  private readonly config: HlsConfig;\n  private loader: Loader<FragmentLoaderContext> | null = null;\n  private partLoadTimeout: number = -1;\n\n  constructor(config: HlsConfig) {\n    this.config = config;\n  }\n\n  destroy() {\n    if (this.loader) {\n      this.loader.destroy();\n      this.loader = null;\n    }\n  }\n\n  abort() {\n    if (this.loader) {\n      // Abort the loader for current fragment. Only one may load at any given time\n      this.loader.abort();\n    }\n  }\n\n  load(\n    frag: Fragment,\n    onProgress?: FragmentLoadProgressCallback,\n  ): Promise<FragLoadedData> {\n    const url = frag.url;\n    if (!url) {\n      return Promise.reject(\n        new LoadError({\n          type: ErrorTypes.NETWORK_ERROR,\n          details: ErrorDetails.FRAG_LOAD_ERROR,\n          fatal: false,\n          frag,\n          error: new Error(\n            `Fragment does not have a ${url ? 'part list' : 'url'}`,\n          ),\n          networkDetails: null,\n        }),\n      );\n    }\n    this.abort();\n\n    const config = this.config;\n    const FragmentILoader = config.fLoader;\n    const DefaultILoader = config.loader;\n\n    return new Promise((resolve, reject) => {\n      if (this.loader) {\n        this.loader.destroy();\n      }\n      if (frag.gap) {\n        if (frag.tagList.some((tags) => tags[0] === 'GAP')) {\n          reject(createGapLoadError(frag));\n          return;\n        } else {\n          // Reset temporary treatment as GAP tag\n          frag.gap = false;\n        }\n      }\n      const loader =\n        (this.loader =\n        frag.loader =\n          FragmentILoader\n            ? new FragmentILoader(config)\n            : (new DefaultILoader(config) as Loader<FragmentLoaderContext>));\n      const loaderContext = createLoaderContext(frag);\n      const loadPolicy = getLoaderConfigWithoutReties(\n        config.fragLoadPolicy.default,\n      );\n      const loaderConfig: LoaderConfiguration = {\n        loadPolicy,\n        timeout: loadPolicy.maxLoadTimeMs,\n        maxRetry: 0,\n        retryDelay: 0,\n        maxRetryDelay: 0,\n        highWaterMark: frag.sn === 'initSegment' ? Infinity : MIN_CHUNK_SIZE,\n      };\n      // Assign frag stats to the loader's stats reference\n      frag.stats = loader.stats;\n      loader.load(loaderContext, loaderConfig, {\n        onSuccess: (response, stats, context, networkDetails) => {\n          this.resetLoader(frag, loader);\n          let payload = response.data as ArrayBuffer;\n          if (context.resetIV && frag.decryptdata) {\n            frag.decryptdata.iv = new Uint8Array(payload.slice(0, 16));\n            payload = payload.slice(16);\n          }\n          resolve({\n            frag,\n            part: null,\n            payload,\n            networkDetails,\n          });\n        },\n        onError: (response, context, networkDetails, stats) => {\n          this.resetLoader(frag, loader);\n          reject(\n            new LoadError({\n              type: ErrorTypes.NETWORK_ERROR,\n              details: ErrorDetails.FRAG_LOAD_ERROR,\n              fatal: false,\n              frag,\n              response: { url, data: undefined, ...response },\n              error: new Error(`HTTP Error ${response.code} ${response.text}`),\n              networkDetails,\n              stats,\n            }),\n          );\n        },\n        onAbort: (stats, context, networkDetails) => {\n          this.resetLoader(frag, loader);\n          reject(\n            new LoadError({\n              type: ErrorTypes.NETWORK_ERROR,\n              details: ErrorDetails.INTERNAL_ABORTED,\n              fatal: false,\n              frag,\n              error: new Error('Aborted'),\n              networkDetails,\n              stats,\n            }),\n          );\n        },\n        onTimeout: (stats, context, networkDetails) => {\n          this.resetLoader(frag, loader);\n          reject(\n            new LoadError({\n              type: ErrorTypes.NETWORK_ERROR,\n              details: ErrorDetails.FRAG_LOAD_TIMEOUT,\n              fatal: false,\n              frag,\n              error: new Error(`Timeout after ${loaderConfig.timeout}ms`),\n              networkDetails,\n              stats,\n            }),\n          );\n        },\n        onProgress: (stats, context, data, networkDetails) => {\n          if (onProgress) {\n            onProgress({\n              frag,\n              part: null,\n              payload: data as ArrayBuffer,\n              networkDetails,\n            });\n          }\n        },\n      });\n    });\n  }\n\n  public loadPart(\n    frag: Fragment,\n    part: Part,\n    onProgress: FragmentLoadProgressCallback,\n  ): Promise<FragLoadedData> {\n    this.abort();\n\n    const config = this.config;\n    const FragmentILoader = config.fLoader;\n    const DefaultILoader = config.loader;\n\n    return new Promise((resolve, reject) => {\n      if (this.loader) {\n        this.loader.destroy();\n      }\n      if (frag.gap || part.gap) {\n        reject(createGapLoadError(frag, part));\n        return;\n      }\n      const loader =\n        (this.loader =\n        frag.loader =\n          FragmentILoader\n            ? new FragmentILoader(config)\n            : (new DefaultILoader(config) as Loader<FragmentLoaderContext>));\n      const loaderContext = createLoaderContext(frag, part);\n      // Should we define another load policy for parts?\n      const loadPolicy = getLoaderConfigWithoutReties(\n        config.fragLoadPolicy.default,\n      );\n      const loaderConfig: LoaderConfiguration = {\n        loadPolicy,\n        timeout: loadPolicy.maxLoadTimeMs,\n        maxRetry: 0,\n        retryDelay: 0,\n        maxRetryDelay: 0,\n        highWaterMark: MIN_CHUNK_SIZE,\n      };\n      // Assign part stats to the loader's stats reference\n      part.stats = loader.stats;\n      loader.load(loaderContext, loaderConfig, {\n        onSuccess: (response, stats, context, networkDetails) => {\n          this.resetLoader(frag, loader);\n          this.updateStatsFromPart(frag, part);\n          const partLoadedData: FragLoadedData = {\n            frag,\n            part,\n            payload: response.data as ArrayBuffer,\n            networkDetails,\n          };\n          onProgress(partLoadedData);\n          resolve(partLoadedData);\n        },\n        onError: (response, context, networkDetails, stats) => {\n          this.resetLoader(frag, loader);\n          reject(\n            new LoadError({\n              type: ErrorTypes.NETWORK_ERROR,\n              details: ErrorDetails.FRAG_LOAD_ERROR,\n              fatal: false,\n              frag,\n              part,\n              response: {\n                url: loaderContext.url,\n                data: undefined,\n                ...response,\n              },\n              error: new Error(`HTTP Error ${response.code} ${response.text}`),\n              networkDetails,\n              stats,\n            }),\n          );\n        },\n        onAbort: (stats, context, networkDetails) => {\n          frag.stats.aborted = part.stats.aborted;\n          this.resetLoader(frag, loader);\n          reject(\n            new LoadError({\n              type: ErrorTypes.NETWORK_ERROR,\n              details: ErrorDetails.INTERNAL_ABORTED,\n              fatal: false,\n              frag,\n              part,\n              error: new Error('Aborted'),\n              networkDetails,\n              stats,\n            }),\n          );\n        },\n        onTimeout: (stats, context, networkDetails) => {\n          this.resetLoader(frag, loader);\n          reject(\n            new LoadError({\n              type: ErrorTypes.NETWORK_ERROR,\n              details: ErrorDetails.FRAG_LOAD_TIMEOUT,\n              fatal: false,\n              frag,\n              part,\n              error: new Error(`Timeout after ${loaderConfig.timeout}ms`),\n              networkDetails,\n              stats,\n            }),\n          );\n        },\n      });\n    });\n  }\n\n  private updateStatsFromPart(frag: Fragment, part: Part) {\n    const fragStats = frag.stats;\n    const partStats = part.stats;\n    const partTotal = partStats.total;\n    fragStats.loaded += partStats.loaded;\n    if (partTotal) {\n      const estTotalParts = Math.round(frag.duration / part.duration);\n      const estLoadedParts = Math.min(\n        Math.round(fragStats.loaded / partTotal),\n        estTotalParts,\n      );\n      const estRemainingParts = estTotalParts - estLoadedParts;\n      const estRemainingBytes =\n        estRemainingParts * Math.round(fragStats.loaded / estLoadedParts);\n      fragStats.total = fragStats.loaded + estRemainingBytes;\n    } else {\n      fragStats.total = Math.max(fragStats.loaded, fragStats.total);\n    }\n    const fragLoading = fragStats.loading;\n    const partLoading = partStats.loading;\n    if (fragLoading.start) {\n      // add to fragment loader latency\n      fragLoading.first += partLoading.first - partLoading.start;\n    } else {\n      fragLoading.start = partLoading.start;\n      fragLoading.first = partLoading.first;\n    }\n    fragLoading.end = partLoading.end;\n  }\n\n  private resetLoader(frag: Fragment, loader: Loader<FragmentLoaderContext>) {\n    frag.loader = null;\n    if (this.loader === loader) {\n      self.clearTimeout(this.partLoadTimeout);\n      this.loader = null;\n    }\n    loader.destroy();\n  }\n}\n\nfunction createLoaderContext(\n  frag: Fragment,\n  part: Part | null = null,\n): FragmentLoaderContext {\n  const segment: BaseSegment = part || frag;\n  const loaderContext: FragmentLoaderContext = {\n    frag,\n    part,\n    responseType: 'arraybuffer',\n    url: segment.url,\n    headers: {},\n    rangeStart: 0,\n    rangeEnd: 0,\n  };\n  const start = segment.byteRangeStartOffset as number;\n  const end = segment.byteRangeEndOffset as number;\n  if (Number.isFinite(start) && Number.isFinite(end)) {\n    let byteRangeStart = start;\n    let byteRangeEnd = end;\n    if (frag.sn === 'initSegment' && frag.decryptdata?.method === 'AES-128') {\n      // MAP segment encrypted with method 'AES-128', when served with HTTP Range,\n      // has the unencrypted size specified in the range.\n      // Ref: https://tools.ietf.org/html/draft-pantos-hls-rfc8216bis-08#section-6.3.6\n      const fragmentLen = end - start;\n      if (fragmentLen % 16) {\n        byteRangeEnd = end + (16 - (fragmentLen % 16));\n      }\n      if (start !== 0) {\n        loaderContext.resetIV = true;\n        byteRangeStart = start - 16;\n      }\n    }\n    loaderContext.rangeStart = byteRangeStart;\n    loaderContext.rangeEnd = byteRangeEnd;\n  }\n  return loaderContext;\n}\n\nfunction createGapLoadError(frag: Fragment, part?: Part): LoadError {\n  const error = new Error(`GAP ${frag.gap ? 'tag' : 'attribute'} found`);\n  const errorData: FragLoadFailResult = {\n    type: ErrorTypes.MEDIA_ERROR,\n    details: ErrorDetails.FRAG_GAP,\n    fatal: false,\n    frag,\n    error,\n    networkDetails: null,\n  };\n  if (part) {\n    errorData.part = part;\n  }\n  (part ? part : frag).stats.aborted = true;\n  return new LoadError(errorData);\n}\n\nexport class LoadError extends Error {\n  public readonly data: FragLoadFailResult;\n  constructor(data: FragLoadFailResult) {\n    super(data.error.message);\n    this.data = data;\n  }\n}\n\nexport interface FragLoadFailResult extends ErrorData {\n  frag: Fragment;\n  part?: Part;\n  response?: {\n    data: any;\n    // error status code\n    code: number;\n    // error description\n    text: string;\n    url: string;\n  };\n  networkDetails: any;\n}\n\nexport type FragmentLoadProgressCallback = (\n  result: FragLoadedData | PartsLoadedData,\n) => void;\n", "export default class AESCrypto {\n  private subtle: SubtleCrypto;\n  private aesIV: Uint8Array;\n\n  constructor(subtle: SubtleCrypto, iv: Uint8Array) {\n    this.subtle = subtle;\n    this.aesIV = iv;\n  }\n\n  decrypt(data: ArrayBuffer, key: CryptoKey) {\n    return this.subtle.decrypt({ name: 'AES-CBC', iv: this.aesIV }, key, data);\n  }\n}\n", "export default class FastAESKey {\n  private subtle: SubtleCrypto;\n  private key: ArrayBuffer;\n\n  constructor(subtle: SubtleCrypto, key: ArrayBuffer) {\n    this.subtle = subtle;\n    this.key = key;\n  }\n\n  expandKey() {\n    return this.subtle.importKey('raw', this.key, { name: 'AES-CBC' }, false, [\n      'encrypt',\n      'decrypt',\n    ]);\n  }\n}\n", "import { sliceUint8 } from '../utils/typed-array';\n\n// PKCS7\nexport function removePadding(array: Uint8Array): Uint8Array {\n  const outputBytes = array.byteLength;\n  const paddingBytes =\n    outputBytes && new DataView(array.buffer).getUint8(outputBytes - 1);\n  if (paddingBytes) {\n    return sliceUint8(array, 0, outputBytes - paddingBytes);\n  }\n  return array;\n}\n\nexport default class AESDecryptor {\n  private rcon: Array<number> = [\n    0x0, 0x1, 0x2, 0x4, 0x8, 0x10, 0x20, 0x40, 0x80, 0x1b, 0x36,\n  ];\n  private subMix: Array<Uint32Array> = [\n    new Uint32Array(256),\n    new Uint32Array(256),\n    new Uint32Array(256),\n    new Uint32Array(256),\n  ];\n  private invSubMix: Array<Uint32Array> = [\n    new Uint32Array(256),\n    new Uint32Array(256),\n    new Uint32Array(256),\n    new Uint32Array(256),\n  ];\n  private sBox: Uint32Array = new Uint32Array(256);\n  private invSBox: Uint32Array = new Uint32Array(256);\n  private key: Uint32Array = new Uint32Array(0);\n\n  private ksRows: number = 0;\n  private keySize: number = 0;\n  private keySchedule!: Uint32Array;\n  private invKeySchedule!: Uint32Array;\n\n  constructor() {\n    this.initTable();\n  }\n\n  // Using view.getUint32() also swaps the byte order.\n  uint8ArrayToUint32Array_(arrayBuffer) {\n    const view = new DataView(arrayBuffer);\n    const newArray = new Uint32Array(4);\n    for (let i = 0; i < 4; i++) {\n      newArray[i] = view.getUint32(i * 4);\n    }\n\n    return newArray;\n  }\n\n  initTable() {\n    const sBox = this.sBox;\n    const invSBox = this.invSBox;\n    const subMix = this.subMix;\n    const subMix0 = subMix[0];\n    const subMix1 = subMix[1];\n    const subMix2 = subMix[2];\n    const subMix3 = subMix[3];\n    const invSubMix = this.invSubMix;\n    const invSubMix0 = invSubMix[0];\n    const invSubMix1 = invSubMix[1];\n    const invSubMix2 = invSubMix[2];\n    const invSubMix3 = invSubMix[3];\n\n    const d = new Uint32Array(256);\n    let x = 0;\n    let xi = 0;\n    let i = 0;\n    for (i = 0; i < 256; i++) {\n      if (i < 128) {\n        d[i] = i << 1;\n      } else {\n        d[i] = (i << 1) ^ 0x11b;\n      }\n    }\n\n    for (i = 0; i < 256; i++) {\n      let sx = xi ^ (xi << 1) ^ (xi << 2) ^ (xi << 3) ^ (xi << 4);\n      sx = (sx >>> 8) ^ (sx & 0xff) ^ 0x63;\n      sBox[x] = sx;\n      invSBox[sx] = x;\n\n      // Compute multiplication\n      const x2 = d[x];\n      const x4 = d[x2];\n      const x8 = d[x4];\n\n      // Compute sub/invSub bytes, mix columns tables\n      let t = (d[sx] * 0x101) ^ (sx * 0x1010100);\n      subMix0[x] = (t << 24) | (t >>> 8);\n      subMix1[x] = (t << 16) | (t >>> 16);\n      subMix2[x] = (t << 8) | (t >>> 24);\n      subMix3[x] = t;\n\n      // Compute inv sub bytes, inv mix columns tables\n      t = (x8 * 0x1010101) ^ (x4 * 0x10001) ^ (x2 * 0x101) ^ (x * 0x1010100);\n      invSubMix0[sx] = (t << 24) | (t >>> 8);\n      invSubMix1[sx] = (t << 16) | (t >>> 16);\n      invSubMix2[sx] = (t << 8) | (t >>> 24);\n      invSubMix3[sx] = t;\n\n      // Compute next counter\n      if (!x) {\n        x = xi = 1;\n      } else {\n        x = x2 ^ d[d[d[x8 ^ x2]]];\n        xi ^= d[d[xi]];\n      }\n    }\n  }\n\n  expandKey(keyBuffer: ArrayBuffer) {\n    // convert keyBuffer to Uint32Array\n    const key = this.uint8ArrayToUint32Array_(keyBuffer);\n    let sameKey = true;\n    let offset = 0;\n\n    while (offset < key.length && sameKey) {\n      sameKey = key[offset] === this.key[offset];\n      offset++;\n    }\n\n    if (sameKey) {\n      return;\n    }\n\n    this.key = key;\n    const keySize = (this.keySize = key.length);\n\n    if (keySize !== 4 && keySize !== 6 && keySize !== 8) {\n      throw new Error('Invalid aes key size=' + keySize);\n    }\n\n    const ksRows = (this.ksRows = (keySize + 6 + 1) * 4);\n    let ksRow;\n    let invKsRow;\n\n    const keySchedule = (this.keySchedule = new Uint32Array(ksRows));\n    const invKeySchedule = (this.invKeySchedule = new Uint32Array(ksRows));\n    const sbox = this.sBox;\n    const rcon = this.rcon;\n\n    const invSubMix = this.invSubMix;\n    const invSubMix0 = invSubMix[0];\n    const invSubMix1 = invSubMix[1];\n    const invSubMix2 = invSubMix[2];\n    const invSubMix3 = invSubMix[3];\n\n    let prev;\n    let t;\n\n    for (ksRow = 0; ksRow < ksRows; ksRow++) {\n      if (ksRow < keySize) {\n        prev = keySchedule[ksRow] = key[ksRow];\n        continue;\n      }\n      t = prev;\n\n      if (ksRow % keySize === 0) {\n        // Rot word\n        t = (t << 8) | (t >>> 24);\n\n        // Sub word\n        t =\n          (sbox[t >>> 24] << 24) |\n          (sbox[(t >>> 16) & 0xff] << 16) |\n          (sbox[(t >>> 8) & 0xff] << 8) |\n          sbox[t & 0xff];\n\n        // Mix Rcon\n        t ^= rcon[(ksRow / keySize) | 0] << 24;\n      } else if (keySize > 6 && ksRow % keySize === 4) {\n        // Sub word\n        t =\n          (sbox[t >>> 24] << 24) |\n          (sbox[(t >>> 16) & 0xff] << 16) |\n          (sbox[(t >>> 8) & 0xff] << 8) |\n          sbox[t & 0xff];\n      }\n\n      keySchedule[ksRow] = prev = (keySchedule[ksRow - keySize] ^ t) >>> 0;\n    }\n\n    for (invKsRow = 0; invKsRow < ksRows; invKsRow++) {\n      ksRow = ksRows - invKsRow;\n      if (invKsRow & 3) {\n        t = keySchedule[ksRow];\n      } else {\n        t = keySchedule[ksRow - 4];\n      }\n\n      if (invKsRow < 4 || ksRow <= 4) {\n        invKeySchedule[invKsRow] = t;\n      } else {\n        invKeySchedule[invKsRow] =\n          invSubMix0[sbox[t >>> 24]] ^\n          invSubMix1[sbox[(t >>> 16) & 0xff]] ^\n          invSubMix2[sbox[(t >>> 8) & 0xff]] ^\n          invSubMix3[sbox[t & 0xff]];\n      }\n\n      invKeySchedule[invKsRow] = invKeySchedule[invKsRow] >>> 0;\n    }\n  }\n\n  // Adding this as a method greatly improves performance.\n  networkToHostOrderSwap(word) {\n    return (\n      (word << 24) |\n      ((word & 0xff00) << 8) |\n      ((word & 0xff0000) >> 8) |\n      (word >>> 24)\n    );\n  }\n\n  decrypt(inputArrayBuffer: ArrayBuffer, offset: number, aesIV: ArrayBuffer) {\n    const nRounds = this.keySize + 6;\n    const invKeySchedule = this.invKeySchedule;\n    const invSBOX = this.invSBox;\n\n    const invSubMix = this.invSubMix;\n    const invSubMix0 = invSubMix[0];\n    const invSubMix1 = invSubMix[1];\n    const invSubMix2 = invSubMix[2];\n    const invSubMix3 = invSubMix[3];\n\n    const initVector = this.uint8ArrayToUint32Array_(aesIV);\n    let initVector0 = initVector[0];\n    let initVector1 = initVector[1];\n    let initVector2 = initVector[2];\n    let initVector3 = initVector[3];\n\n    const inputInt32 = new Int32Array(inputArrayBuffer);\n    const outputInt32 = new Int32Array(inputInt32.length);\n\n    let t0, t1, t2, t3;\n    let s0, s1, s2, s3;\n    let inputWords0, inputWords1, inputWords2, inputWords3;\n\n    let ksRow, i;\n    const swapWord = this.networkToHostOrderSwap;\n\n    while (offset < inputInt32.length) {\n      inputWords0 = swapWord(inputInt32[offset]);\n      inputWords1 = swapWord(inputInt32[offset + 1]);\n      inputWords2 = swapWord(inputInt32[offset + 2]);\n      inputWords3 = swapWord(inputInt32[offset + 3]);\n\n      s0 = inputWords0 ^ invKeySchedule[0];\n      s1 = inputWords3 ^ invKeySchedule[1];\n      s2 = inputWords2 ^ invKeySchedule[2];\n      s3 = inputWords1 ^ invKeySchedule[3];\n\n      ksRow = 4;\n\n      // Iterate through the rounds of decryption\n      for (i = 1; i < nRounds; i++) {\n        t0 =\n          invSubMix0[s0 >>> 24] ^\n          invSubMix1[(s1 >> 16) & 0xff] ^\n          invSubMix2[(s2 >> 8) & 0xff] ^\n          invSubMix3[s3 & 0xff] ^\n          invKeySchedule[ksRow];\n        t1 =\n          invSubMix0[s1 >>> 24] ^\n          invSubMix1[(s2 >> 16) & 0xff] ^\n          invSubMix2[(s3 >> 8) & 0xff] ^\n          invSubMix3[s0 & 0xff] ^\n          invKeySchedule[ksRow + 1];\n        t2 =\n          invSubMix0[s2 >>> 24] ^\n          invSubMix1[(s3 >> 16) & 0xff] ^\n          invSubMix2[(s0 >> 8) & 0xff] ^\n          invSubMix3[s1 & 0xff] ^\n          invKeySchedule[ksRow + 2];\n        t3 =\n          invSubMix0[s3 >>> 24] ^\n          invSubMix1[(s0 >> 16) & 0xff] ^\n          invSubMix2[(s1 >> 8) & 0xff] ^\n          invSubMix3[s2 & 0xff] ^\n          invKeySchedule[ksRow + 3];\n        // Update state\n        s0 = t0;\n        s1 = t1;\n        s2 = t2;\n        s3 = t3;\n\n        ksRow = ksRow + 4;\n      }\n\n      // Shift rows, sub bytes, add round key\n      t0 =\n        (invSBOX[s0 >>> 24] << 24) ^\n        (invSBOX[(s1 >> 16) & 0xff] << 16) ^\n        (invSBOX[(s2 >> 8) & 0xff] << 8) ^\n        invSBOX[s3 & 0xff] ^\n        invKeySchedule[ksRow];\n      t1 =\n        (invSBOX[s1 >>> 24] << 24) ^\n        (invSBOX[(s2 >> 16) & 0xff] << 16) ^\n        (invSBOX[(s3 >> 8) & 0xff] << 8) ^\n        invSBOX[s0 & 0xff] ^\n        invKeySchedule[ksRow + 1];\n      t2 =\n        (invSBOX[s2 >>> 24] << 24) ^\n        (invSBOX[(s3 >> 16) & 0xff] << 16) ^\n        (invSBOX[(s0 >> 8) & 0xff] << 8) ^\n        invSBOX[s1 & 0xff] ^\n        invKeySchedule[ksRow + 2];\n      t3 =\n        (invSBOX[s3 >>> 24] << 24) ^\n        (invSBOX[(s0 >> 16) & 0xff] << 16) ^\n        (invSBOX[(s1 >> 8) & 0xff] << 8) ^\n        invSBOX[s2 & 0xff] ^\n        invKeySchedule[ksRow + 3];\n\n      // Write\n      outputInt32[offset] = swapWord(t0 ^ initVector0);\n      outputInt32[offset + 1] = swapWord(t3 ^ initVector1);\n      outputInt32[offset + 2] = swapWord(t2 ^ initVector2);\n      outputInt32[offset + 3] = swapWord(t1 ^ initVector3);\n\n      // reset initVector to last 4 unsigned int\n      initVector0 = inputWords0;\n      initVector1 = inputWords1;\n      initVector2 = inputWords2;\n      initVector3 = inputWords3;\n\n      offset = offset + 4;\n    }\n\n    return outputInt32.buffer;\n  }\n}\n", "import AESCrypto from './aes-crypto';\nimport FastAESKey from './fast-aes-key';\nimport AESDecryptor, { removePadding } from './aes-decryptor';\nimport { logger } from '../utils/logger';\nimport { appendUint8Array } from '../utils/mp4-tools';\nimport { sliceUint8 } from '../utils/typed-array';\nimport type { HlsConfig } from '../config';\n\nconst CHUNK_SIZE = 16; // 16 bytes, 128 bits\n\nexport default class Decrypter {\n  private logEnabled: boolean = true;\n  private removePKCS7Padding: boolean;\n  private subtle: SubtleCrypto | null = null;\n  private softwareDecrypter: AESDecryptor | null = null;\n  private key: ArrayBuffer | null = null;\n  private fastAesKey: FastAESKey | null = null;\n  private remainderData: Uint8Array | null = null;\n  private currentIV: ArrayBuffer | null = null;\n  private currentResult: ArrayBuffer | null = null;\n  private useSoftware: boolean;\n\n  constructor(config: HlsConfig, { removePKCS7Padding = true } = {}) {\n    this.useSoftware = config.enableSoftwareAES;\n    this.removePKCS7Padding = removePKCS7Padding;\n    // built in decryptor expects PKCS7 padding\n    if (removePKCS7Padding) {\n      try {\n        const browserCrypto = self.crypto;\n        if (browserCrypto) {\n          this.subtle =\n            browserCrypto.subtle ||\n            ((browserCrypto as any).webkitSubtle as SubtleCrypto);\n        }\n      } catch (e) {\n        /* no-op */\n      }\n    }\n\n    this.useSoftware = !this.subtle;\n  }\n\n  destroy() {\n    this.subtle = null;\n    this.softwareDecrypter = null;\n    this.key = null;\n    this.fastAesKey = null;\n    this.remainderData = null;\n    this.currentIV = null;\n    this.currentResult = null;\n  }\n\n  public isSync() {\n    return this.useSoftware;\n  }\n\n  public flush(): Uint8Array | null {\n    const { currentResult, remainderData } = this;\n    if (!currentResult || remainderData) {\n      this.reset();\n      return null;\n    }\n    const data = new Uint8Array(currentResult);\n    this.reset();\n    if (this.removePKCS7Padding) {\n      return removePadding(data);\n    }\n    return data;\n  }\n\n  public reset() {\n    this.currentResult = null;\n    this.currentIV = null;\n    this.remainderData = null;\n    if (this.softwareDecrypter) {\n      this.softwareDecrypter = null;\n    }\n  }\n\n  public decrypt(\n    data: Uint8Array | ArrayBuffer,\n    key: ArrayBuffer,\n    iv: ArrayBuffer,\n  ): Promise<ArrayBuffer> {\n    if (this.useSoftware) {\n      return new Promise((resolve, reject) => {\n        this.softwareDecrypt(new Uint8Array(data), key, iv);\n        const decryptResult = this.flush();\n        if (decryptResult) {\n          resolve(decryptResult.buffer);\n        } else {\n          reject(new Error('[softwareDecrypt] Failed to decrypt data'));\n        }\n      });\n    }\n    return this.webCryptoDecrypt(new Uint8Array(data), key, iv);\n  }\n\n  // Software decryption is progressive. Progressive decryption may not return a result on each call. Any cached\n  // data is handled in the flush() call\n  public softwareDecrypt(\n    data: Uint8Array,\n    key: ArrayBuffer,\n    iv: ArrayBuffer,\n  ): ArrayBuffer | null {\n    const { currentIV, currentResult, remainderData } = this;\n    this.logOnce('JS AES decrypt');\n    // The output is staggered during progressive parsing - the current result is cached, and emitted on the next call\n    // This is done in order to strip PKCS7 padding, which is found at the end of each segment. We only know we've reached\n    // the end on flush(), but by that time we have already received all bytes for the segment.\n    // Progressive decryption does not work with WebCrypto\n\n    if (remainderData) {\n      data = appendUint8Array(remainderData, data);\n      this.remainderData = null;\n    }\n\n    // Byte length must be a multiple of 16 (AES-128 = 128 bit blocks = 16 bytes)\n    const currentChunk = this.getValidChunk(data);\n    if (!currentChunk.length) {\n      return null;\n    }\n\n    if (currentIV) {\n      iv = currentIV;\n    }\n\n    let softwareDecrypter = this.softwareDecrypter;\n    if (!softwareDecrypter) {\n      softwareDecrypter = this.softwareDecrypter = new AESDecryptor();\n    }\n    softwareDecrypter.expandKey(key);\n\n    const result = currentResult;\n\n    this.currentResult = softwareDecrypter.decrypt(currentChunk.buffer, 0, iv);\n    this.currentIV = sliceUint8(currentChunk, -16).buffer;\n\n    if (!result) {\n      return null;\n    }\n    return result;\n  }\n\n  public webCryptoDecrypt(\n    data: Uint8Array,\n    key: ArrayBuffer,\n    iv: ArrayBuffer,\n  ): Promise<ArrayBuffer> {\n    if (this.key !== key || !this.fastAesKey) {\n      if (!this.subtle) {\n        return Promise.resolve(this.onWebCryptoError(data, key, iv));\n      }\n      this.key = key;\n      this.fastAesKey = new FastAESKey(this.subtle, key);\n    }\n    return this.fastAesKey\n      .expandKey()\n      .then((aesKey) => {\n        // decrypt using web crypto\n        if (!this.subtle) {\n          return Promise.reject(new Error('web crypto not initialized'));\n        }\n        this.logOnce('WebCrypto AES decrypt');\n        const crypto = new AESCrypto(this.subtle, new Uint8Array(iv));\n        return crypto.decrypt(data.buffer, aesKey);\n      })\n      .catch((err) => {\n        logger.warn(\n          `[decrypter]: WebCrypto Error, disable WebCrypto API, ${err.name}: ${err.message}`,\n        );\n\n        return this.onWebCryptoError(data, key, iv);\n      });\n  }\n\n  private onWebCryptoError(\n    data: Uint8Array,\n    key: ArrayBuffer,\n    iv: ArrayBuffer,\n  ): ArrayBuffer | never {\n    this.useSoftware = true;\n    this.logEnabled = true;\n    this.softwareDecrypt(data, key, iv);\n    const decryptResult = this.flush();\n    if (decryptResult) {\n      return decryptResult.buffer;\n    }\n    throw new Error('WebCrypto and softwareDecrypt: failed to decrypt data');\n  }\n\n  private getValidChunk(data: Uint8Array): Uint8Array {\n    let currentChunk = data;\n    const splitPoint = data.length - (data.length % CHUNK_SIZE);\n    if (splitPoint !== data.length) {\n      currentChunk = sliceUint8(data, 0, splitPoint);\n      this.remainderData = sliceUint8(data, splitPoint);\n    }\n    return currentChunk;\n  }\n\n  private logOnce(msg: string) {\n    if (!this.logEnabled) {\n      return;\n    }\n    logger.log(`[decrypter]: ${msg}`);\n    this.logEnabled = false;\n  }\n}\n", "/**\n *  TimeRanges to string helper\n */\n\nconst TimeRanges = {\n  toString: function (r: TimeRanges) {\n    let log = '';\n    const len = r.length;\n    for (let i = 0; i < len; i++) {\n      log += `[${r.start(i).toFixed(3)}-${r.end(i).toFixed(3)}]`;\n    }\n\n    return log;\n  },\n};\n\nexport default TimeRanges;\n", "import TaskLoop from '../task-loop';\nimport { FragmentState } from './fragment-tracker';\nimport { Bufferable, BufferHelper, BufferInfo } from '../utils/buffer-helper';\nimport { logger } from '../utils/logger';\nimport { Events } from '../events';\nimport { ErrorDetails, ErrorTypes } from '../errors';\nimport { ChunkMetadata } from '../types/transmuxer';\nimport { appendUint8Array } from '../utils/mp4-tools';\nimport { alignStream } from '../utils/discontinuities';\nimport {\n  findFragmentByPDT,\n  findFragmentByPTS,\n  findFragWithCC,\n} from './fragment-finders';\nimport {\n  findPart,\n  getFragmentWithSN,\n  getPartWith,\n  updateFragPTSDTS,\n} from '../utils/level-helper';\nimport TransmuxerInterface from '../demux/transmuxer-interface';\nimport { Fragment, Part } from '../loader/fragment';\nimport FragmentLoader, {\n  FragmentLoadProgressCallback,\n  LoadError,\n} from '../loader/fragment-loader';\nimport KeyLoader from '../loader/key-loader';\nimport { LevelDetails } from '../loader/level-details';\nimport Decrypter from '../crypt/decrypter';\nimport TimeRanges from '../utils/time-ranges';\nimport { PlaylistLevelType } from '../types/loader';\nimport { getRetryDelay } from '../utils/error-helper';\nimport { NetworkErrorAction } from './error-controller';\nimport type {\n  BufferAppendingData,\n  ErrorData,\n  FragLoadedData,\n  PartsLoadedData,\n  KeyLoadedData,\n  MediaAttachedData,\n  BufferFlushingData,\n  ManifestLoadedData,\n} from '../types/events';\nimport type { FragmentTracker } from './fragment-tracker';\nimport type { Level } from '../types/level';\nimport type { RemuxedTrack } from '../types/remuxer';\nimport type Hls from '../hls';\nimport type { HlsConfig } from '../config';\nimport type { NetworkComponentAPI } from '../types/component-api';\nimport type { SourceBufferName } from '../types/buffer';\nimport type { RationalTimestamp } from '../utils/timescale-conversion';\n\ntype ResolveFragLoaded = (FragLoadedEndData) => void;\ntype RejectFragLoaded = (LoadError) => void;\n\nexport const State = {\n  STOPPED: 'STOPPED',\n  IDLE: 'IDLE',\n  KEY_LOADING: 'KEY_LOADING',\n  FRAG_LOADING: 'FRAG_LOADING',\n  FRAG_LOADING_WAITING_RETRY: 'FRAG_LOADING_WAITING_RETRY',\n  WAITING_TRACK: 'WAITING_TRACK',\n  PARSING: 'PARSING',\n  PARSED: 'PARSED',\n  ENDED: 'ENDED',\n  ERROR: 'ERROR',\n  WAITING_INIT_PTS: 'WAITING_INIT_PTS',\n  WAITING_LEVEL: 'WAITING_LEVEL',\n};\n\nexport default class BaseStreamController\n  extends TaskLoop\n  implements NetworkComponentAPI\n{\n  protected hls: Hls;\n\n  protected fragPrevious: Fragment | null = null;\n  protected fragCurrent: Fragment | null = null;\n  protected fragmentTracker: FragmentTracker;\n  protected transmuxer: TransmuxerInterface | null = null;\n  protected _state: string = State.STOPPED;\n  protected playlistType: PlaylistLevelType;\n  protected media: HTMLMediaElement | null = null;\n  protected mediaBuffer: Bufferable | null = null;\n  protected config: HlsConfig;\n  protected bitrateTest: boolean = false;\n  protected lastCurrentTime: number = 0;\n  protected nextLoadPosition: number = 0;\n  protected startPosition: number = 0;\n  protected startTimeOffset: number | null = null;\n  protected loadedmetadata: boolean = false;\n  protected retryDate: number = 0;\n  protected levels: Array<Level> | null = null;\n  protected fragmentLoader: FragmentLoader;\n  protected keyLoader: KeyLoader;\n  protected levelLastLoaded: Level | null = null;\n  protected startFragRequested: boolean = false;\n  protected decrypter: Decrypter;\n  protected initPTS: RationalTimestamp[] = [];\n  protected onvseeking: EventListener | null = null;\n  protected onvended: EventListener | null = null;\n\n  private readonly logPrefix: string = '';\n  protected log: (msg: any) => void;\n  protected warn: (msg: any) => void;\n\n  constructor(\n    hls: Hls,\n    fragmentTracker: FragmentTracker,\n    keyLoader: KeyLoader,\n    logPrefix: string,\n    playlistType: PlaylistLevelType,\n  ) {\n    super();\n    this.playlistType = playlistType;\n    this.logPrefix = logPrefix;\n    this.log = logger.log.bind(logger, `${logPrefix}:`);\n    this.warn = logger.warn.bind(logger, `${logPrefix}:`);\n    this.hls = hls;\n    this.fragmentLoader = new FragmentLoader(hls.config);\n    this.keyLoader = keyLoader;\n    this.fragmentTracker = fragmentTracker;\n    this.config = hls.config;\n    this.decrypter = new Decrypter(hls.config);\n    hls.on(Events.MANIFEST_LOADED, this.onManifestLoaded, this);\n  }\n\n  protected doTick() {\n    this.onTickEnd();\n  }\n\n  protected onTickEnd() {}\n\n  // eslint-disable-next-line @typescript-eslint/no-unused-vars\n  public startLoad(startPosition: number): void {}\n\n  public stopLoad() {\n    this.fragmentLoader.abort();\n    this.keyLoader.abort(this.playlistType);\n    const frag = this.fragCurrent;\n    if (frag?.loader) {\n      frag.abortRequests();\n      this.fragmentTracker.removeFragment(frag);\n    }\n    this.resetTransmuxer();\n    this.fragCurrent = null;\n    this.fragPrevious = null;\n    this.clearInterval();\n    this.clearNextTick();\n    this.state = State.STOPPED;\n  }\n\n  protected _streamEnded(\n    bufferInfo: BufferInfo,\n    levelDetails: LevelDetails,\n  ): boolean {\n    // If playlist is live, there is another buffered range after the current range, nothing buffered, media is detached,\n    // of nothing loading/loaded return false\n    if (\n      levelDetails.live ||\n      bufferInfo.nextStart ||\n      !bufferInfo.end ||\n      !this.media\n    ) {\n      return false;\n    }\n    const partList = levelDetails.partList;\n    // Since the last part isn't guaranteed to correspond to the last playlist segment for Low-Latency HLS,\n    // check instead if the last part is buffered.\n    if (partList?.length) {\n      const lastPart = partList[partList.length - 1];\n\n      // Checking the midpoint of the part for potential margin of error and related issues.\n      // NOTE: Technically I believe parts could yield content that is < the computed duration (including potential a duration of 0)\n      // and still be spec-compliant, so there may still be edge cases here. Likewise, there could be issues in end of stream\n      // part mismatches for independent audio and video playlists/segments.\n      const lastPartBuffered = BufferHelper.isBuffered(\n        this.media,\n        lastPart.start + lastPart.duration / 2,\n      );\n      return lastPartBuffered;\n    }\n\n    const playlistType =\n      levelDetails.fragments[levelDetails.fragments.length - 1].type;\n    return this.fragmentTracker.isEndListAppended(playlistType);\n  }\n\n  protected getLevelDetails(): LevelDetails | undefined {\n    if (this.levels && this.levelLastLoaded !== null) {\n      return this.levelLastLoaded?.details;\n    }\n  }\n\n  protected onMediaAttached(\n    event: Events.MEDIA_ATTACHED,\n    data: MediaAttachedData,\n  ) {\n    const media = (this.media = this.mediaBuffer = data.media);\n    this.onvseeking = this.onMediaSeeking.bind(this) as EventListener;\n    this.onvended = this.onMediaEnded.bind(this) as EventListener;\n    media.addEventListener('seeking', this.onvseeking);\n    media.addEventListener('ended', this.onvended);\n    const config = this.config;\n    if (this.levels && config.autoStartLoad && this.state === State.STOPPED) {\n      this.startLoad(config.startPosition);\n    }\n  }\n\n  protected onMediaDetaching() {\n    const media = this.media;\n    if (media?.ended) {\n      this.log('MSE detaching and video ended, reset startPosition');\n      this.startPosition = this.lastCurrentTime = 0;\n    }\n\n    // remove video listeners\n    if (media && this.onvseeking && this.onvended) {\n      media.removeEventListener('seeking', this.onvseeking);\n      media.removeEventListener('ended', this.onvended);\n      this.onvseeking = this.onvended = null;\n    }\n    if (this.keyLoader) {\n      this.keyLoader.detach();\n    }\n    this.media = this.mediaBuffer = null;\n    this.loadedmetadata = false;\n    this.fragmentTracker.removeAllFragments();\n    this.stopLoad();\n  }\n\n  protected onMediaSeeking() {\n    const { config, fragCurrent, media, mediaBuffer, state } = this;\n    const currentTime: number = media ? media.currentTime : 0;\n    const bufferInfo = BufferHelper.bufferInfo(\n      mediaBuffer ? mediaBuffer : media,\n      currentTime,\n      config.maxBufferHole,\n    );\n\n    this.log(\n      `media seeking to ${\n        Number.isFinite(currentTime) ? currentTime.toFixed(3) : currentTime\n      }, state: ${state}`,\n    );\n\n    if (this.state === State.ENDED) {\n      this.resetLoadingState();\n    } else if (fragCurrent) {\n      // Seeking while frag load is in progress\n      const tolerance = config.maxFragLookUpTolerance;\n      const fragStartOffset = fragCurrent.start - tolerance;\n      const fragEndOffset =\n        fragCurrent.start + fragCurrent.duration + tolerance;\n      // if seeking out of buffered range or into new one\n      if (\n        !bufferInfo.len ||\n        fragEndOffset < bufferInfo.start ||\n        fragStartOffset > bufferInfo.end\n      ) {\n        const pastFragment = currentTime > fragEndOffset;\n        // if the seek position is outside the current fragment range\n        if (currentTime < fragStartOffset || pastFragment) {\n          if (pastFragment && fragCurrent.loader) {\n            this.log(\n              'seeking outside of buffer while fragment load in progress, cancel fragment load',\n            );\n            fragCurrent.abortRequests();\n            this.resetLoadingState();\n          }\n          this.fragPrevious = null;\n        }\n      }\n    }\n\n    if (media) {\n      // Remove gap fragments\n      this.fragmentTracker.removeFragmentsInRange(\n        currentTime,\n        Infinity,\n        this.playlistType,\n        true,\n      );\n\n      this.lastCurrentTime = currentTime;\n    }\n\n    // in case seeking occurs although no media buffered, adjust startPosition and nextLoadPosition to seek target\n    if (!this.loadedmetadata && !bufferInfo.len) {\n      this.nextLoadPosition = this.startPosition = currentTime;\n    }\n\n    // Async tick to speed up processing\n    this.tickImmediate();\n  }\n\n  protected onMediaEnded() {\n    // reset startPosition and lastCurrentTime to restart playback @ stream beginning\n    this.startPosition = this.lastCurrentTime = 0;\n  }\n\n  protected onManifestLoaded(\n    event: Events.MANIFEST_LOADED,\n    data: ManifestLoadedData,\n  ): void {\n    this.startTimeOffset = data.startTimeOffset;\n    this.initPTS = [];\n  }\n\n  protected onHandlerDestroying() {\n    this.hls.off(Events.MANIFEST_LOADED, this.onManifestLoaded, this);\n    this.stopLoad();\n    super.onHandlerDestroying();\n    // @ts-ignore\n    this.hls = null;\n  }\n\n  protected onHandlerDestroyed() {\n    this.state = State.STOPPED;\n    if (this.fragmentLoader) {\n      this.fragmentLoader.destroy();\n    }\n    if (this.keyLoader) {\n      this.keyLoader.destroy();\n    }\n    if (this.decrypter) {\n      this.decrypter.destroy();\n    }\n\n    this.hls =\n      this.log =\n      this.warn =\n      this.decrypter =\n      this.keyLoader =\n      this.fragmentLoader =\n      this.fragmentTracker =\n        null as any;\n    super.onHandlerDestroyed();\n  }\n\n  protected loadFragment(\n    frag: Fragment,\n    level: Level,\n    targetBufferTime: number,\n  ) {\n    this._loadFragForPlayback(frag, level, targetBufferTime);\n  }\n\n  private _loadFragForPlayback(\n    frag: Fragment,\n    level: Level,\n    targetBufferTime: number,\n  ) {\n    const progressCallback: FragmentLoadProgressCallback = (\n      data: FragLoadedData,\n    ) => {\n      if (this.fragContextChanged(frag)) {\n        this.warn(\n          `Fragment ${frag.sn}${\n            data.part ? ' p: ' + data.part.index : ''\n          } of level ${frag.level} was dropped during download.`,\n        );\n        this.fragmentTracker.removeFragment(frag);\n        return;\n      }\n      frag.stats.chunkCount++;\n      this._handleFragmentLoadProgress(data);\n    };\n\n    this._doFragLoad(frag, level, targetBufferTime, progressCallback)\n      .then((data) => {\n        if (!data) {\n          // if we're here we probably needed to backtrack or are waiting for more parts\n          return;\n        }\n        const state = this.state;\n        if (this.fragContextChanged(frag)) {\n          if (\n            state === State.FRAG_LOADING ||\n            (!this.fragCurrent && state === State.PARSING)\n          ) {\n            this.fragmentTracker.removeFragment(frag);\n            this.state = State.IDLE;\n          }\n          return;\n        }\n\n        if ('payload' in data) {\n          this.log(`Loaded fragment ${frag.sn} of level ${frag.level}`);\n          this.hls.trigger(Events.FRAG_LOADED, data);\n        }\n\n        // Pass through the whole payload; controllers not implementing progressive loading receive data from this callback\n        this._handleFragmentLoadComplete(data);\n      })\n      .catch((reason) => {\n        if (this.state === State.STOPPED || this.state === State.ERROR) {\n          return;\n        }\n        this.warn(`Frag error: ${reason?.message || reason}`);\n        this.resetFragmentLoading(frag);\n      });\n  }\n\n  protected clearTrackerIfNeeded(frag: Fragment) {\n    const { fragmentTracker } = this;\n    const fragState = fragmentTracker.getState(frag);\n    if (fragState === FragmentState.APPENDING) {\n      // Lower the max buffer length and try again\n      const playlistType = frag.type as PlaylistLevelType;\n      const bufferedInfo = this.getFwdBufferInfo(\n        this.mediaBuffer,\n        playlistType,\n      );\n      const minForwardBufferLength = Math.max(\n        frag.duration,\n        bufferedInfo ? bufferedInfo.len : this.config.maxBufferLength,\n      );\n      // If backtracking, always remove from the tracker without reducing max buffer length\n      const backtrackFragment = (this as any).backtrackFragment as\n        | Fragment\n        | undefined;\n      const backtracked = backtrackFragment\n        ? (frag.sn as number) - (backtrackFragment.sn as number)\n        : 0;\n      if (\n        backtracked === 1 ||\n        this.reduceMaxBufferLength(minForwardBufferLength, frag.duration)\n      ) {\n        fragmentTracker.removeFragment(frag);\n      }\n    } else if (this.mediaBuffer?.buffered.length === 0) {\n      // Stop gap for bad tracker / buffer flush behavior\n      fragmentTracker.removeAllFragments();\n    } else if (fragmentTracker.hasParts(frag.type)) {\n      // In low latency mode, remove fragments for which only some parts were buffered\n      fragmentTracker.detectPartialFragments({\n        frag,\n        part: null,\n        stats: frag.stats,\n        id: frag.type,\n      });\n      if (fragmentTracker.getState(frag) === FragmentState.PARTIAL) {\n        fragmentTracker.removeFragment(frag);\n      }\n    }\n  }\n\n  protected checkLiveUpdate(details: LevelDetails) {\n    if (details.updated && !details.live) {\n      // Live stream ended, update fragment tracker\n      const lastFragment = details.fragments[details.fragments.length - 1];\n      this.fragmentTracker.detectPartialFragments({\n        frag: lastFragment,\n        part: null,\n        stats: lastFragment.stats,\n        id: lastFragment.type,\n      });\n    }\n    if (!details.fragments[0]) {\n      details.deltaUpdateFailed = true;\n    }\n  }\n\n  protected flushMainBuffer(\n    startOffset: number,\n    endOffset: number,\n    type: SourceBufferName | null = null,\n  ) {\n    if (!(startOffset - endOffset)) {\n      return;\n    }\n    // When alternate audio is playing, the audio-stream-controller is responsible for the audio buffer. Otherwise,\n    // passing a null type flushes both buffers\n    const flushScope: BufferFlushingData = { startOffset, endOffset, type };\n    this.hls.trigger(Events.BUFFER_FLUSHING, flushScope);\n  }\n\n  protected _loadInitSegment(frag: Fragment, level: Level) {\n    this._doFragLoad(frag, level)\n      .then((data) => {\n        if (!data || this.fragContextChanged(frag) || !this.levels) {\n          throw new Error('init load aborted');\n        }\n\n        return data;\n      })\n      .then((data: FragLoadedData) => {\n        const { hls } = this;\n        const { payload } = data;\n        const decryptData = frag.decryptdata;\n\n        // check to see if the payload needs to be decrypted\n        if (\n          payload &&\n          payload.byteLength > 0 &&\n          decryptData?.key &&\n          decryptData.iv &&\n          decryptData.method === 'AES-128'\n        ) {\n          const startTime = self.performance.now();\n          // decrypt init segment data\n          return this.decrypter\n            .decrypt(\n              new Uint8Array(payload),\n              decryptData.key.buffer,\n              decryptData.iv.buffer,\n            )\n            .catch((err) => {\n              hls.trigger(Events.ERROR, {\n                type: ErrorTypes.MEDIA_ERROR,\n                details: ErrorDetails.FRAG_DECRYPT_ERROR,\n                fatal: false,\n                error: err,\n                reason: err.message,\n                frag,\n              });\n              throw err;\n            })\n            .then((decryptedData) => {\n              const endTime = self.performance.now();\n              hls.trigger(Events.FRAG_DECRYPTED, {\n                frag,\n                payload: decryptedData,\n                stats: {\n                  tstart: startTime,\n                  tdecrypt: endTime,\n                },\n              });\n              data.payload = decryptedData;\n              return this.completeInitSegmentLoad(data);\n            });\n        }\n        return this.completeInitSegmentLoad(data);\n      })\n      .catch((reason) => {\n        if (this.state === State.STOPPED || this.state === State.ERROR) {\n          return;\n        }\n        this.warn(reason);\n        this.resetFragmentLoading(frag);\n      });\n  }\n\n  private completeInitSegmentLoad(data: FragLoadedData) {\n    const { levels } = this;\n    if (!levels) {\n      throw new Error('init load aborted, missing levels');\n    }\n    const stats = data.frag.stats;\n    this.state = State.IDLE;\n    data.frag.data = new Uint8Array(data.payload);\n    stats.parsing.start = stats.buffering.start = self.performance.now();\n    stats.parsing.end = stats.buffering.end = self.performance.now();\n    this.tick();\n  }\n\n  protected fragContextChanged(frag: Fragment | null) {\n    const { fragCurrent } = this;\n    return (\n      !frag ||\n      !fragCurrent ||\n      frag.sn !== fragCurrent.sn ||\n      frag.level !== fragCurrent.level\n    );\n  }\n\n  protected fragBufferedComplete(frag: Fragment, part: Part | null) {\n    const media = this.mediaBuffer ? this.mediaBuffer : this.media;\n    this.log(\n      `Buffered ${frag.type} sn: ${frag.sn}${\n        part ? ' part: ' + part.index : ''\n      } of ${\n        this.playlistType === PlaylistLevelType.MAIN ? 'level' : 'track'\n      } ${frag.level} (frag:[${(frag.startPTS ?? NaN).toFixed(3)}-${(\n        frag.endPTS ?? NaN\n      ).toFixed(3)}] > buffer:${\n        media\n          ? TimeRanges.toString(BufferHelper.getBuffered(media))\n          : '(detached)'\n      })`,\n    );\n    if (frag.sn !== 'initSegment') {\n      if (frag.type !== PlaylistLevelType.SUBTITLE) {\n        const el = frag.elementaryStreams;\n        if (!Object.keys(el).some((type) => !!el[type])) {\n          // empty segment\n          this.state = State.IDLE;\n          return;\n        }\n      }\n      const level = this.levels?.[frag.level];\n      if (level?.fragmentError) {\n        this.log(\n          `Resetting level fragment error count of ${level.fragmentError} on frag buffered`,\n        );\n        level.fragmentError = 0;\n      }\n    }\n    this.state = State.IDLE;\n    if (!media) {\n      return;\n    }\n    if (\n      !this.loadedmetadata &&\n      frag.type == PlaylistLevelType.MAIN &&\n      media.buffered.length &&\n      this.fragCurrent?.sn === this.fragPrevious?.sn\n    ) {\n      this.loadedmetadata = true;\n      this.seekToStartPos();\n    }\n    this.tick();\n  }\n\n  protected seekToStartPos() {}\n\n  protected _handleFragmentLoadComplete(fragLoadedEndData: PartsLoadedData) {\n    const { transmuxer } = this;\n    if (!transmuxer) {\n      return;\n    }\n    const { frag, part, partsLoaded } = fragLoadedEndData;\n    // If we did not load parts, or loaded all parts, we have complete (not partial) fragment data\n    const complete =\n      !partsLoaded ||\n      partsLoaded.length === 0 ||\n      partsLoaded.some((fragLoaded) => !fragLoaded);\n    const chunkMeta = new ChunkMetadata(\n      frag.level,\n      frag.sn as number,\n      frag.stats.chunkCount + 1,\n      0,\n      part ? part.index : -1,\n      !complete,\n    );\n    transmuxer.flush(chunkMeta);\n  }\n\n  // eslint-disable-next-line @typescript-eslint/no-unused-vars\n  protected _handleFragmentLoadProgress(\n    frag: PartsLoadedData | FragLoadedData,\n  ) {}\n\n  protected _doFragLoad(\n    frag: Fragment,\n    level: Level,\n    targetBufferTime: number | null = null,\n    progressCallback?: FragmentLoadProgressCallback,\n  ): Promise<PartsLoadedData | FragLoadedData | null> {\n    const details = level?.details;\n    if (!this.levels || !details) {\n      throw new Error(\n        `frag load aborted, missing level${details ? '' : ' detail'}s`,\n      );\n    }\n\n    let keyLoadingPromise: Promise<KeyLoadedData | void> | null = null;\n    if (frag.encrypted && !frag.decryptdata?.key) {\n      this.log(\n        `Loading key for ${frag.sn} of [${details.startSN}-${details.endSN}], ${\n          this.logPrefix === '[stream-controller]' ? 'level' : 'track'\n        } ${frag.level}`,\n      );\n      this.state = State.KEY_LOADING;\n      this.fragCurrent = frag;\n      keyLoadingPromise = this.keyLoader.load(frag).then((keyLoadedData) => {\n        if (!this.fragContextChanged(keyLoadedData.frag)) {\n          this.hls.trigger(Events.KEY_LOADED, keyLoadedData);\n          if (this.state === State.KEY_LOADING) {\n            this.state = State.IDLE;\n          }\n          return keyLoadedData;\n        }\n      });\n      this.hls.trigger(Events.KEY_LOADING, { frag });\n      if (this.fragCurrent === null) {\n        keyLoadingPromise = Promise.reject(\n          new Error(`frag load aborted, context changed in KEY_LOADING`),\n        );\n      }\n    } else if (!frag.encrypted && details.encryptedFragments.length) {\n      this.keyLoader.loadClear(frag, details.encryptedFragments);\n    }\n\n    targetBufferTime = Math.max(frag.start, targetBufferTime || 0);\n    if (this.config.lowLatencyMode && frag.sn !== 'initSegment') {\n      const partList = details.partList;\n      if (partList && progressCallback) {\n        if (targetBufferTime > frag.end && details.fragmentHint) {\n          frag = details.fragmentHint;\n        }\n        const partIndex = this.getNextPart(partList, frag, targetBufferTime);\n        if (partIndex > -1) {\n          const part = partList[partIndex];\n          this.log(\n            `Loading part sn: ${frag.sn} p: ${part.index} cc: ${\n              frag.cc\n            } of playlist [${details.startSN}-${\n              details.endSN\n            }] parts [0-${partIndex}-${partList.length - 1}] ${\n              this.logPrefix === '[stream-controller]' ? 'level' : 'track'\n            }: ${frag.level}, target: ${parseFloat(\n              targetBufferTime.toFixed(3),\n            )}`,\n          );\n          this.nextLoadPosition = part.start + part.duration;\n          this.state = State.FRAG_LOADING;\n          let result: Promise<PartsLoadedData | FragLoadedData | null>;\n          if (keyLoadingPromise) {\n            result = keyLoadingPromise\n              .then((keyLoadedData) => {\n                if (\n                  !keyLoadedData ||\n                  this.fragContextChanged(keyLoadedData.frag)\n                ) {\n                  return null;\n                }\n                return this.doFragPartsLoad(\n                  frag,\n                  part,\n                  level,\n                  progressCallback,\n                );\n              })\n              .catch((error) => this.handleFragLoadError(error));\n          } else {\n            result = this.doFragPartsLoad(\n              frag,\n              part,\n              level,\n              progressCallback,\n            ).catch((error: LoadError) => this.handleFragLoadError(error));\n          }\n          this.hls.trigger(Events.FRAG_LOADING, {\n            frag,\n            part,\n            targetBufferTime,\n          });\n          if (this.fragCurrent === null) {\n            return Promise.reject(\n              new Error(\n                `frag load aborted, context changed in FRAG_LOADING parts`,\n              ),\n            );\n          }\n          return result;\n        } else if (\n          !frag.url ||\n          this.loadedEndOfParts(partList, targetBufferTime)\n        ) {\n          // Fragment hint has no parts\n          return Promise.resolve(null);\n        }\n      }\n    }\n\n    this.log(\n      `Loading fragment ${frag.sn} cc: ${frag.cc} ${\n        details ? 'of [' + details.startSN + '-' + details.endSN + '] ' : ''\n      }${this.logPrefix === '[stream-controller]' ? 'level' : 'track'}: ${\n        frag.level\n      }, target: ${parseFloat(targetBufferTime.toFixed(3))}`,\n    );\n    // Don't update nextLoadPosition for fragments which are not buffered\n    if (Number.isFinite(frag.sn as number) && !this.bitrateTest) {\n      this.nextLoadPosition = frag.start + frag.duration;\n    }\n    this.state = State.FRAG_LOADING;\n\n    // Load key before streaming fragment data\n    const dataOnProgress = this.config.progressive;\n    let result: Promise<PartsLoadedData | FragLoadedData | null>;\n    if (dataOnProgress && keyLoadingPromise) {\n      result = keyLoadingPromise\n        .then((keyLoadedData) => {\n          if (!keyLoadedData || this.fragContextChanged(keyLoadedData?.frag)) {\n            return null;\n          }\n          return this.fragmentLoader.load(frag, progressCallback);\n        })\n        .catch((error) => this.handleFragLoadError(error));\n    } else {\n      // load unencrypted fragment data with progress event,\n      // or handle fragment result after key and fragment are finished loading\n      result = Promise.all([\n        this.fragmentLoader.load(\n          frag,\n          dataOnProgress ? progressCallback : undefined,\n        ),\n        keyLoadingPromise,\n      ])\n        .then(([fragLoadedData]) => {\n          if (!dataOnProgress && fragLoadedData && progressCallback) {\n            progressCallback(fragLoadedData);\n          }\n          return fragLoadedData;\n        })\n        .catch((error) => this.handleFragLoadError(error));\n    }\n    this.hls.trigger(Events.FRAG_LOADING, { frag, targetBufferTime });\n    if (this.fragCurrent === null) {\n      return Promise.reject(\n        new Error(`frag load aborted, context changed in FRAG_LOADING`),\n      );\n    }\n    return result;\n  }\n\n  private doFragPartsLoad(\n    frag: Fragment,\n    fromPart: Part,\n    level: Level,\n    progressCallback: FragmentLoadProgressCallback,\n  ): Promise<PartsLoadedData | null> {\n    return new Promise(\n      (resolve: ResolveFragLoaded, reject: RejectFragLoaded) => {\n        const partsLoaded: FragLoadedData[] = [];\n        const initialPartList = level.details?.partList;\n        const loadPart = (part: Part) => {\n          this.fragmentLoader\n            .loadPart(frag, part, progressCallback)\n            .then((partLoadedData: FragLoadedData) => {\n              partsLoaded[part.index] = partLoadedData;\n              const loadedPart = partLoadedData.part as Part;\n              this.hls.trigger(Events.FRAG_LOADED, partLoadedData);\n              const nextPart =\n                getPartWith(level, frag.sn as number, part.index + 1) ||\n                findPart(initialPartList, frag.sn as number, part.index + 1);\n              if (nextPart) {\n                loadPart(nextPart);\n              } else {\n                return resolve({\n                  frag,\n                  part: loadedPart,\n                  partsLoaded,\n                });\n              }\n            })\n            .catch(reject);\n        };\n        loadPart(fromPart);\n      },\n    );\n  }\n\n  private handleFragLoadError(error: LoadError | Error) {\n    if ('data' in error) {\n      const data = error.data;\n      if (error.data && data.details === ErrorDetails.INTERNAL_ABORTED) {\n        this.handleFragLoadAborted(data.frag, data.part);\n      } else {\n        this.hls.trigger(Events.ERROR, data as ErrorData);\n      }\n    } else {\n      this.hls.trigger(Events.ERROR, {\n        type: ErrorTypes.OTHER_ERROR,\n        details: ErrorDetails.INTERNAL_EXCEPTION,\n        err: error,\n        error,\n        fatal: true,\n      });\n    }\n    return null;\n  }\n\n  protected _handleTransmuxerFlush(chunkMeta: ChunkMetadata) {\n    const context = this.getCurrentContext(chunkMeta);\n    if (!context || this.state !== State.PARSING) {\n      if (\n        !this.fragCurrent &&\n        this.state !== State.STOPPED &&\n        this.state !== State.ERROR\n      ) {\n        this.state = State.IDLE;\n      }\n      return;\n    }\n    const { frag, part, level } = context;\n    const now = self.performance.now();\n    frag.stats.parsing.end = now;\n    if (part) {\n      part.stats.parsing.end = now;\n    }\n    this.updateLevelTiming(frag, part, level, chunkMeta.partial);\n  }\n\n  protected getCurrentContext(\n    chunkMeta: ChunkMetadata,\n  ): { frag: Fragment; part: Part | null; level: Level } | null {\n    const { levels, fragCurrent } = this;\n    const { level: levelIndex, sn, part: partIndex } = chunkMeta;\n    if (!levels?.[levelIndex]) {\n      this.warn(\n        `Levels object was unset while buffering fragment ${sn} of level ${levelIndex}. The current chunk will not be buffered.`,\n      );\n      return null;\n    }\n    const level = levels[levelIndex];\n    const part = partIndex > -1 ? getPartWith(level, sn, partIndex) : null;\n    const frag = part\n      ? part.fragment\n      : getFragmentWithSN(level, sn, fragCurrent);\n    if (!frag) {\n      return null;\n    }\n    if (fragCurrent && fragCurrent !== frag) {\n      frag.stats = fragCurrent.stats;\n    }\n    return { frag, part, level };\n  }\n\n  protected bufferFragmentData(\n    data: RemuxedTrack,\n    frag: Fragment,\n    part: Part | null,\n    chunkMeta: ChunkMetadata,\n    noBacktracking?: boolean,\n  ) {\n    if (!data || this.state !== State.PARSING) {\n      return;\n    }\n\n    const { data1, data2 } = data;\n    let buffer = data1;\n    if (data1 && data2) {\n      // Combine the moof + mdat so that we buffer with a single append\n      buffer = appendUint8Array(data1, data2);\n    }\n\n    if (!buffer?.length) {\n      return;\n    }\n\n    const segment: BufferAppendingData = {\n      type: data.type,\n      frag,\n      part,\n      chunkMeta,\n      parent: frag.type,\n      data: buffer,\n    };\n    this.hls.trigger(Events.BUFFER_APPENDING, segment);\n\n    if (data.dropped && data.independent && !part) {\n      if (noBacktracking) {\n        return;\n      }\n      // Clear buffer so that we reload previous segments sequentially if required\n      this.flushBufferGap(frag);\n    }\n  }\n\n  protected flushBufferGap(frag: Fragment) {\n    const media = this.media;\n    if (!media) {\n      return;\n    }\n    // If currentTime is not buffered, clear the back buffer so that we can backtrack as much as needed\n    if (!BufferHelper.isBuffered(media, media.currentTime)) {\n      this.flushMainBuffer(0, frag.start);\n      return;\n    }\n    // Remove back-buffer without interrupting playback to allow back tracking\n    const currentTime = media.currentTime;\n    const bufferInfo = BufferHelper.bufferInfo(media, currentTime, 0);\n    const fragDuration = frag.duration;\n    const segmentFraction = Math.min(\n      this.config.maxFragLookUpTolerance * 2,\n      fragDuration * 0.25,\n    );\n    const start = Math.max(\n      Math.min(frag.start - segmentFraction, bufferInfo.end - segmentFraction),\n      currentTime + segmentFraction,\n    );\n    if (frag.start - start > segmentFraction) {\n      this.flushMainBuffer(start, frag.start);\n    }\n  }\n\n  protected getFwdBufferInfo(\n    bufferable: Bufferable | null,\n    type: PlaylistLevelType,\n  ): BufferInfo | null {\n    const pos = this.getLoadPosition();\n    if (!Number.isFinite(pos)) {\n      return null;\n    }\n    return this.getFwdBufferInfoAtPos(bufferable, pos, type);\n  }\n\n  protected getFwdBufferInfoAtPos(\n    bufferable: Bufferable | null,\n    pos: number,\n    type: PlaylistLevelType,\n  ): BufferInfo | null {\n    const {\n      config: { maxBufferHole },\n    } = this;\n    const bufferInfo = BufferHelper.bufferInfo(bufferable, pos, maxBufferHole);\n    // Workaround flaw in getting forward buffer when maxBufferHole is smaller than gap at current pos\n    if (bufferInfo.len === 0 && bufferInfo.nextStart !== undefined) {\n      const bufferedFragAtPos = this.fragmentTracker.getBufferedFrag(pos, type);\n      if (bufferedFragAtPos && bufferInfo.nextStart < bufferedFragAtPos.end) {\n        return BufferHelper.bufferInfo(\n          bufferable,\n          pos,\n          Math.max(bufferInfo.nextStart, maxBufferHole),\n        );\n      }\n    }\n    return bufferInfo;\n  }\n\n  protected getMaxBufferLength(levelBitrate?: number): number {\n    const { config } = this;\n    let maxBufLen;\n    if (levelBitrate) {\n      maxBufLen = Math.max(\n        (8 * config.maxBufferSize) / levelBitrate,\n        config.maxBufferLength,\n      );\n    } else {\n      maxBufLen = config.maxBufferLength;\n    }\n    return Math.min(maxBufLen, config.maxMaxBufferLength);\n  }\n\n  protected reduceMaxBufferLength(threshold: number, fragDuration: number) {\n    const config = this.config;\n    const minLength = Math.max(\n      Math.min(threshold - fragDuration, config.maxBufferLength),\n      fragDuration,\n    );\n    const reducedLength = Math.max(\n      threshold - fragDuration * 3,\n      config.maxMaxBufferLength / 2,\n      minLength,\n    );\n    if (reducedLength >= minLength) {\n      // reduce max buffer length as it might be too high. we do this to avoid loop flushing ...\n      config.maxMaxBufferLength = reducedLength;\n      this.warn(`Reduce max buffer length to ${reducedLength}s`);\n      return true;\n    }\n    return false;\n  }\n\n  protected getAppendedFrag(\n    position: number,\n    playlistType: PlaylistLevelType = PlaylistLevelType.MAIN,\n  ): Fragment | null {\n    const fragOrPart = this.fragmentTracker.getAppendedFrag(\n      position,\n      PlaylistLevelType.MAIN,\n    );\n    if (fragOrPart && 'fragment' in fragOrPart) {\n      return fragOrPart.fragment;\n    }\n    return fragOrPart;\n  }\n\n  protected getNextFragment(\n    pos: number,\n    levelDetails: LevelDetails,\n  ): Fragment | null {\n    const fragments = levelDetails.fragments;\n    const fragLen = fragments.length;\n\n    if (!fragLen) {\n      return null;\n    }\n\n    // find fragment index, contiguous with end of buffer position\n    const { config } = this;\n    const start = fragments[0].start;\n    let frag;\n\n    if (levelDetails.live) {\n      const initialLiveManifestSize = config.initialLiveManifestSize;\n      if (fragLen < initialLiveManifestSize) {\n        this.warn(\n          `Not enough fragments to start playback (have: ${fragLen}, need: ${initialLiveManifestSize})`,\n        );\n        return null;\n      }\n      // The real fragment start times for a live stream are only known after the PTS range for that level is known.\n      // In order to discover the range, we load the best matching fragment for that level and demux it.\n      // Do not load using live logic if the starting frag is requested - we want to use getFragmentAtPosition() so that\n      // we get the fragment matching that start time\n      if (\n        (!levelDetails.PTSKnown &&\n          !this.startFragRequested &&\n          this.startPosition === -1) ||\n        pos < start\n      ) {\n        frag = this.getInitialLiveFragment(levelDetails, fragments);\n        this.startPosition = this.nextLoadPosition = frag\n          ? this.hls.liveSyncPosition || frag.start\n          : pos;\n      }\n    } else if (pos <= start) {\n      // VoD playlist: if loadPosition before start of playlist, load first fragment\n      frag = fragments[0];\n    }\n\n    // If we haven't run into any special cases already, just load the fragment most closely matching the requested position\n    if (!frag) {\n      const end = config.lowLatencyMode\n        ? levelDetails.partEnd\n        : levelDetails.fragmentEnd;\n      frag = this.getFragmentAtPosition(pos, end, levelDetails);\n    }\n\n    return this.mapToInitFragWhenRequired(frag);\n  }\n\n  protected isLoopLoading(frag: Fragment, targetBufferTime: number): boolean {\n    const trackerState = this.fragmentTracker.getState(frag);\n    return (\n      (trackerState === FragmentState.OK ||\n        (trackerState === FragmentState.PARTIAL && !!frag.gap)) &&\n      this.nextLoadPosition > targetBufferTime\n    );\n  }\n\n  protected getNextFragmentLoopLoading(\n    frag: Fragment,\n    levelDetails: LevelDetails,\n    bufferInfo: BufferInfo,\n    playlistType: PlaylistLevelType,\n    maxBufLen: number,\n  ): Fragment | null {\n    const gapStart = frag.gap;\n    const nextFragment = this.getNextFragment(\n      this.nextLoadPosition,\n      levelDetails,\n    );\n    if (nextFragment === null) {\n      return nextFragment;\n    }\n    frag = nextFragment;\n    if (gapStart && frag && !frag.gap && bufferInfo.nextStart) {\n      // Media buffered after GAP tags should not make the next buffer timerange exceed forward buffer length\n      const nextbufferInfo = this.getFwdBufferInfoAtPos(\n        this.mediaBuffer ? this.mediaBuffer : this.media,\n        bufferInfo.nextStart,\n        playlistType,\n      );\n      if (\n        nextbufferInfo !== null &&\n        bufferInfo.len + nextbufferInfo.len >= maxBufLen\n      ) {\n        // Returning here might result in not finding an audio and video candiate to skip to\n        this.log(\n          `buffer full after gaps in \"${playlistType}\" playlist starting at sn: ${frag.sn}`,\n        );\n        return null;\n      }\n    }\n    return frag;\n  }\n\n  mapToInitFragWhenRequired(frag: Fragment | null): typeof frag {\n    // If an initSegment is present, it must be buffered first\n    if (frag?.initSegment && !frag?.initSegment.data && !this.bitrateTest) {\n      return frag.initSegment;\n    }\n\n    return frag;\n  }\n\n  getNextPart(\n    partList: Part[],\n    frag: Fragment,\n    targetBufferTime: number,\n  ): number {\n    let nextPart = -1;\n    let contiguous = false;\n    let independentAttrOmitted = true;\n    for (let i = 0, len = partList.length; i < len; i++) {\n      const part = partList[i];\n      independentAttrOmitted = independentAttrOmitted && !part.independent;\n      if (nextPart > -1 && targetBufferTime < part.start) {\n        break;\n      }\n      const loaded = part.loaded;\n      if (loaded) {\n        nextPart = -1;\n      } else if (\n        (contiguous || part.independent || independentAttrOmitted) &&\n        part.fragment === frag\n      ) {\n        nextPart = i;\n      }\n      contiguous = loaded;\n    }\n    return nextPart;\n  }\n\n  private loadedEndOfParts(\n    partList: Part[],\n    targetBufferTime: number,\n  ): boolean {\n    const lastPart = partList[partList.length - 1];\n    return lastPart && targetBufferTime > lastPart.start && lastPart.loaded;\n  }\n\n  /*\n   This method is used find the best matching first fragment for a live playlist. This fragment is used to calculate the\n   \"sliding\" of the playlist, which is its offset from the start of playback. After sliding we can compute the real\n   start and end times for each fragment in the playlist (after which this method will not need to be called).\n  */\n  protected getInitialLiveFragment(\n    levelDetails: LevelDetails,\n    fragments: Array<Fragment>,\n  ): Fragment | null {\n    const fragPrevious = this.fragPrevious;\n    let frag: Fragment | null = null;\n    if (fragPrevious) {\n      if (levelDetails.hasProgramDateTime) {\n        // Prefer using PDT, because it can be accurate enough to choose the correct fragment without knowing the level sliding\n        this.log(\n          `Live playlist, switching playlist, load frag with same PDT: ${fragPrevious.programDateTime}`,\n        );\n        frag = findFragmentByPDT(\n          fragments,\n          fragPrevious.endProgramDateTime,\n          this.config.maxFragLookUpTolerance,\n        );\n      }\n      if (!frag) {\n        // SN does not need to be accurate between renditions, but depending on the packaging it may be so.\n        const targetSN = (fragPrevious.sn as number) + 1;\n        if (\n          targetSN >= levelDetails.startSN &&\n          targetSN <= levelDetails.endSN\n        ) {\n          const fragNext = fragments[targetSN - levelDetails.startSN];\n          // Ensure that we're staying within the continuity range, since PTS resets upon a new range\n          if (fragPrevious.cc === fragNext.cc) {\n            frag = fragNext;\n            this.log(\n              `Live playlist, switching playlist, load frag with next SN: ${\n                frag!.sn\n              }`,\n            );\n          }\n        }\n        // It's important to stay within the continuity range if available; otherwise the fragments in the playlist\n        // will have the wrong start times\n        if (!frag) {\n          frag = findFragWithCC(fragments, fragPrevious.cc);\n          if (frag) {\n            this.log(\n              `Live playlist, switching playlist, load frag with same CC: ${frag.sn}`,\n            );\n          }\n        }\n      }\n    } else {\n      // Find a new start fragment when fragPrevious is null\n      const liveStart = this.hls.liveSyncPosition;\n      if (liveStart !== null) {\n        frag = this.getFragmentAtPosition(\n          liveStart,\n          this.bitrateTest ? levelDetails.fragmentEnd : levelDetails.edge,\n          levelDetails,\n        );\n      }\n    }\n\n    return frag;\n  }\n\n  /*\n  This method finds the best matching fragment given the provided position.\n   */\n  protected getFragmentAtPosition(\n    bufferEnd: number,\n    end: number,\n    levelDetails: LevelDetails,\n  ): Fragment | null {\n    const { config } = this;\n    let { fragPrevious } = this;\n    let { fragments, endSN } = levelDetails;\n    const { fragmentHint } = levelDetails;\n    const { maxFragLookUpTolerance } = config;\n    const partList = levelDetails.partList;\n\n    const loadingParts = !!(\n      config.lowLatencyMode &&\n      partList?.length &&\n      fragmentHint\n    );\n    if (loadingParts && fragmentHint && !this.bitrateTest) {\n      // Include incomplete fragment with parts at end\n      fragments = fragments.concat(fragmentHint);\n      endSN = fragmentHint.sn as number;\n    }\n\n    let frag;\n    if (bufferEnd < end) {\n      const lookupTolerance =\n        bufferEnd > end - maxFragLookUpTolerance ? 0 : maxFragLookUpTolerance;\n      // Remove the tolerance if it would put the bufferEnd past the actual end of stream\n      // Uses buffer and sequence number to calculate switch segment (required if using EXT-X-DISCONTINUITY-SEQUENCE)\n      frag = findFragmentByPTS(\n        fragPrevious,\n        fragments,\n        bufferEnd,\n        lookupTolerance,\n      );\n    } else {\n      // reach end of playlist\n      frag = fragments[fragments.length - 1];\n    }\n\n    if (frag) {\n      const curSNIdx = frag.sn - levelDetails.startSN;\n      // Move fragPrevious forward to support forcing the next fragment to load\n      // when the buffer catches up to a previously buffered range.\n      const fragState = this.fragmentTracker.getState(frag);\n      if (\n        fragState === FragmentState.OK ||\n        (fragState === FragmentState.PARTIAL && frag.gap)\n      ) {\n        fragPrevious = frag;\n      }\n      if (\n        fragPrevious &&\n        frag.sn === fragPrevious.sn &&\n        (!loadingParts || partList[0].fragment.sn > frag.sn)\n      ) {\n        // Force the next fragment to load if the previous one was already selected. This can occasionally happen with\n        // non-uniform fragment durations\n        const sameLevel = fragPrevious && frag.level === fragPrevious.level;\n        if (sameLevel) {\n          const nextFrag = fragments[curSNIdx + 1];\n          if (\n            frag.sn < endSN &&\n            this.fragmentTracker.getState(nextFrag) !== FragmentState.OK\n          ) {\n            frag = nextFrag;\n          } else {\n            frag = null;\n          }\n        }\n      }\n    }\n    return frag;\n  }\n\n  protected synchronizeToLiveEdge(levelDetails: LevelDetails) {\n    const { config, media } = this;\n    if (!media) {\n      return;\n    }\n    const liveSyncPosition = this.hls.liveSyncPosition;\n    const currentTime = media.currentTime;\n    const start = levelDetails.fragments[0].start;\n    const end = levelDetails.edge;\n    const withinSlidingWindow =\n      currentTime >= start - config.maxFragLookUpTolerance &&\n      currentTime <= end;\n    // Continue if we can seek forward to sync position or if current time is outside of sliding window\n    if (\n      liveSyncPosition !== null &&\n      media.duration > liveSyncPosition &&\n      (currentTime < liveSyncPosition || !withinSlidingWindow)\n    ) {\n      // Continue if buffer is starving or if current time is behind max latency\n      const maxLatency =\n        config.liveMaxLatencyDuration !== undefined\n          ? config.liveMaxLatencyDuration\n          : config.liveMaxLatencyDurationCount * levelDetails.targetduration;\n      if (\n        (!withinSlidingWindow && media.readyState < 4) ||\n        currentTime < end - maxLatency\n      ) {\n        if (!this.loadedmetadata) {\n          this.nextLoadPosition = liveSyncPosition;\n        }\n        // Only seek if ready and there is not a significant forward buffer available for playback\n        if (media.readyState) {\n          this.warn(\n            `Playback: ${currentTime.toFixed(\n              3,\n            )} is located too far from the end of live sliding playlist: ${end}, reset currentTime to : ${liveSyncPosition.toFixed(\n              3,\n            )}`,\n          );\n          media.currentTime = liveSyncPosition;\n        }\n      }\n    }\n  }\n\n  protected alignPlaylists(\n    details: LevelDetails,\n    previousDetails: LevelDetails | undefined,\n    switchDetails: LevelDetails | undefined,\n  ): number {\n    // FIXME: If not for `shouldAlignOnDiscontinuities` requiring fragPrevious.cc,\n    //  this could all go in level-helper mergeDetails()\n    const length = details.fragments.length;\n    if (!length) {\n      this.warn(`No fragments in live playlist`);\n      return 0;\n    }\n    const slidingStart = details.fragments[0].start;\n    const firstLevelLoad = !previousDetails;\n    const aligned = details.alignedSliding && Number.isFinite(slidingStart);\n    if (firstLevelLoad || (!aligned && !slidingStart)) {\n      const { fragPrevious } = this;\n      alignStream(fragPrevious, switchDetails, details);\n      const alignedSlidingStart = details.fragments[0].start;\n      this.log(\n        `Live playlist sliding: ${alignedSlidingStart.toFixed(2)} start-sn: ${\n          previousDetails ? previousDetails.startSN : 'na'\n        }->${details.startSN} prev-sn: ${\n          fragPrevious ? fragPrevious.sn : 'na'\n        } fragments: ${length}`,\n      );\n      return alignedSlidingStart;\n    }\n    return slidingStart;\n  }\n\n  protected waitForCdnTuneIn(details: LevelDetails) {\n    // Wait for Low-Latency CDN Tune-in to get an updated playlist\n    const advancePartLimit = 3;\n    return (\n      details.live &&\n      details.canBlockReload &&\n      details.partTarget &&\n      details.tuneInGoal >\n        Math.max(details.partHoldBack, details.partTarget * advancePartLimit)\n    );\n  }\n\n  protected setStartPosition(details: LevelDetails, sliding: number) {\n    // compute start position if set to -1. use it straight away if value is defined\n    let startPosition = this.startPosition;\n    if (startPosition < sliding) {\n      startPosition = -1;\n    }\n    if (startPosition === -1 || this.lastCurrentTime === -1) {\n      // Use Playlist EXT-X-START:TIME-OFFSET when set\n      // Prioritize Multivariant Playlist offset so that main, audio, and subtitle stream-controller start times match\n      const offsetInMultivariantPlaylist = this.startTimeOffset !== null;\n      const startTimeOffset = offsetInMultivariantPlaylist\n        ? this.startTimeOffset\n        : details.startTimeOffset;\n      if (startTimeOffset !== null && Number.isFinite(startTimeOffset)) {\n        startPosition = sliding + startTimeOffset;\n        if (startTimeOffset < 0) {\n          startPosition += details.totalduration;\n        }\n        startPosition = Math.min(\n          Math.max(sliding, startPosition),\n          sliding + details.totalduration,\n        );\n        this.log(\n          `Start time offset ${startTimeOffset} found in ${\n            offsetInMultivariantPlaylist ? 'multivariant' : 'media'\n          } playlist, adjust startPosition to ${startPosition}`,\n        );\n        this.startPosition = startPosition;\n      } else if (details.live) {\n        // Leave this.startPosition at -1, so that we can use `getInitialLiveFragment` logic when startPosition has\n        // not been specified via the config or an as an argument to startLoad (#3736).\n        startPosition = this.hls.liveSyncPosition || sliding;\n      } else {\n        this.startPosition = startPosition = 0;\n      }\n      this.lastCurrentTime = startPosition;\n    }\n    this.nextLoadPosition = startPosition;\n  }\n\n  protected getLoadPosition(): number {\n    const { media } = this;\n    // if we have not yet loaded any fragment, start loading from start position\n    let pos = 0;\n    if (this.loadedmetadata && media) {\n      pos = media.currentTime;\n    } else if (this.nextLoadPosition) {\n      pos = this.nextLoadPosition;\n    }\n\n    return pos;\n  }\n\n  private handleFragLoadAborted(frag: Fragment, part: Part | undefined) {\n    if (this.transmuxer && frag.sn !== 'initSegment' && frag.stats.aborted) {\n      this.warn(\n        `Fragment ${frag.sn}${part ? ' part ' + part.index : ''} of level ${\n          frag.level\n        } was aborted`,\n      );\n      this.resetFragmentLoading(frag);\n    }\n  }\n\n  protected resetFragmentLoading(frag: Fragment) {\n    if (\n      !this.fragCurrent ||\n      (!this.fragContextChanged(frag) &&\n        this.state !== State.FRAG_LOADING_WAITING_RETRY)\n    ) {\n      this.state = State.IDLE;\n    }\n  }\n\n  protected onFragmentOrKeyLoadError(\n    filterType: PlaylistLevelType,\n    data: ErrorData,\n  ) {\n    if (data.chunkMeta && !data.frag) {\n      const context = this.getCurrentContext(data.chunkMeta);\n      if (context) {\n        data.frag = context.frag;\n      }\n    }\n    const frag = data.frag;\n    // Handle frag error related to caller's filterType\n    if (!frag || frag.type !== filterType || !this.levels) {\n      return;\n    }\n    if (this.fragContextChanged(frag)) {\n      this.warn(\n        `Frag load error must match current frag to retry ${frag.url} > ${this.fragCurrent?.url}`,\n      );\n      return;\n    }\n    const gapTagEncountered = data.details === ErrorDetails.FRAG_GAP;\n    if (gapTagEncountered) {\n      this.fragmentTracker.fragBuffered(frag, true);\n    }\n    // keep retrying until the limit will be reached\n    const errorAction = data.errorAction;\n    const { action, retryCount = 0, retryConfig } = errorAction || {};\n    if (\n      errorAction &&\n      action === NetworkErrorAction.RetryRequest &&\n      retryConfig\n    ) {\n      this.resetStartWhenNotLoaded(this.levelLastLoaded);\n      const delay = getRetryDelay(retryConfig, retryCount);\n      this.warn(\n        `Fragment ${frag.sn} of ${filterType} ${frag.level} errored with ${\n          data.details\n        }, retrying loading ${retryCount + 1}/${\n          retryConfig.maxNumRetry\n        } in ${delay}ms`,\n      );\n      errorAction.resolved = true;\n      this.retryDate = self.performance.now() + delay;\n      this.state = State.FRAG_LOADING_WAITING_RETRY;\n    } else if (retryConfig && errorAction) {\n      this.resetFragmentErrors(filterType);\n      if (retryCount < retryConfig.maxNumRetry) {\n        // Network retry is skipped when level switch is preferred\n        if (\n          !gapTagEncountered &&\n          action !== NetworkErrorAction.RemoveAlternatePermanently\n        ) {\n          errorAction.resolved = true;\n        }\n      } else {\n        logger.warn(\n          `${data.details} reached or exceeded max retry (${retryCount})`,\n        );\n        return;\n      }\n    } else if (\n      errorAction?.action === NetworkErrorAction.SendAlternateToPenaltyBox\n    ) {\n      this.state = State.WAITING_LEVEL;\n    } else {\n      this.state = State.ERROR;\n    }\n    // Perform next async tick sooner to speed up error action resolution\n    this.tickImmediate();\n  }\n\n  protected reduceLengthAndFlushBuffer(data: ErrorData): boolean {\n    // if in appending state\n    if (this.state === State.PARSING || this.state === State.PARSED) {\n      const frag = data.frag;\n      const playlistType = data.parent as PlaylistLevelType;\n      const bufferedInfo = this.getFwdBufferInfo(\n        this.mediaBuffer,\n        playlistType,\n      );\n      // 0.5 : tolerance needed as some browsers stalls playback before reaching buffered end\n      // reduce max buf len if current position is buffered\n      const buffered = bufferedInfo && bufferedInfo.len > 0.5;\n      if (buffered) {\n        this.reduceMaxBufferLength(bufferedInfo.len, frag?.duration || 10);\n      }\n      const flushBuffer = !buffered;\n      if (flushBuffer) {\n        // current position is not buffered, but browser is still complaining about buffer full error\n        // this happens on IE/Edge, refer to https://github.com/video-dev/hls.js/pull/708\n        // in that case flush the whole audio buffer to recover\n        this.warn(\n          `Buffer full error while media.currentTime is not buffered, flush ${playlistType} buffer`,\n        );\n      }\n      if (frag) {\n        this.fragmentTracker.removeFragment(frag);\n        this.nextLoadPosition = frag.start;\n      }\n      this.resetLoadingState();\n      return flushBuffer;\n    }\n    return false;\n  }\n\n  protected resetFragmentErrors(filterType: PlaylistLevelType) {\n    if (filterType === PlaylistLevelType.AUDIO) {\n      // Reset current fragment since audio track audio is essential and may not have a fail-over track\n      this.fragCurrent = null;\n    }\n    // Fragment errors that result in a level switch or redundant fail-over\n    // should reset the stream controller state to idle\n    if (!this.loadedmetadata) {\n      this.startFragRequested = false;\n    }\n    if (this.state !== State.STOPPED) {\n      this.state = State.IDLE;\n    }\n  }\n\n  protected afterBufferFlushed(\n    media: Bufferable,\n    bufferType: SourceBufferName,\n    playlistType: PlaylistLevelType,\n  ) {\n    if (!media) {\n      return;\n    }\n    // After successful buffer flushing, filter flushed fragments from bufferedFrags use mediaBuffered instead of media\n    // (so that we will check against video.buffered ranges in case of alt audio track)\n    const bufferedTimeRanges = BufferHelper.getBuffered(media);\n    this.fragmentTracker.detectEvictedFragments(\n      bufferType,\n      bufferedTimeRanges,\n      playlistType,\n    );\n    if (this.state === State.ENDED) {\n      this.resetLoadingState();\n    }\n  }\n\n  protected resetLoadingState() {\n    this.log('Reset loading state');\n    this.fragCurrent = null;\n    this.fragPrevious = null;\n    this.state = State.IDLE;\n  }\n\n  protected resetStartWhenNotLoaded(level: Level | null): void {\n    // if loadedmetadata is not set, it means that first frag request failed\n    // in that case, reset startFragRequested flag\n    if (!this.loadedmetadata) {\n      this.startFragRequested = false;\n      const details = level ? level.details : null;\n      if (details?.live) {\n        // Update the start position and return to IDLE to recover live start\n        this.startPosition = -1;\n        this.setStartPosition(details, 0);\n        this.resetLoadingState();\n      } else {\n        this.nextLoadPosition = this.startPosition;\n      }\n    }\n  }\n\n  protected resetWhenMissingContext(chunkMeta: ChunkMetadata) {\n    this.warn(\n      `The loading context changed while buffering fragment ${chunkMeta.sn} of level ${chunkMeta.level}. This chunk will not be buffered.`,\n    );\n    this.removeUnbufferedFrags();\n    this.resetStartWhenNotLoaded(this.levelLastLoaded);\n    this.resetLoadingState();\n  }\n\n  protected removeUnbufferedFrags(start: number = 0) {\n    this.fragmentTracker.removeFragmentsInRange(\n      start,\n      Infinity,\n      this.playlistType,\n      false,\n      true,\n    );\n  }\n\n  private updateLevelTiming(\n    frag: Fragment,\n    part: Part | null,\n    level: Level,\n    partial: boolean,\n  ) {\n    const details = level.details as LevelDetails;\n    if (!details) {\n      this.warn('level.details undefined');\n      return;\n    }\n    const parsed = Object.keys(frag.elementaryStreams).reduce(\n      (result, type) => {\n        const info = frag.elementaryStreams[type];\n        if (info) {\n          const parsedDuration = info.endPTS - info.startPTS;\n          if (parsedDuration <= 0) {\n            // Destroy the transmuxer after it's next time offset failed to advance because duration was <= 0.\n            // The new transmuxer will be configured with a time offset matching the next fragment start,\n            // preventing the timeline from shifting.\n            this.warn(\n              `Could not parse fragment ${frag.sn} ${type} duration reliably (${parsedDuration})`,\n            );\n            return result || false;\n          }\n          const drift = partial\n            ? 0\n            : updateFragPTSDTS(\n                details,\n                frag,\n                info.startPTS,\n                info.endPTS,\n                info.startDTS,\n                info.endDTS,\n              );\n          this.hls.trigger(Events.LEVEL_PTS_UPDATED, {\n            details,\n            level,\n            drift,\n            type,\n            frag,\n            start: info.startPTS,\n            end: info.endPTS,\n          });\n          return true;\n        }\n        return result;\n      },\n      false,\n    );\n    if (!parsed && this.transmuxer?.error === null) {\n      const error = new Error(\n        `Found no media in fragment ${frag.sn} of level ${frag.level} resetting transmuxer to fallback to playlist timing`,\n      );\n      if (level.fragmentError === 0) {\n        // Mark and track the odd empty segment as a gap to avoid reloading\n        level.fragmentError++;\n        frag.gap = true;\n        this.fragmentTracker.removeFragment(frag);\n        this.fragmentTracker.fragBuffered(frag, true);\n      }\n      this.warn(error.message);\n      this.hls.trigger(Events.ERROR, {\n        type: ErrorTypes.MEDIA_ERROR,\n        details: ErrorDetails.FRAG_PARSING_ERROR,\n        fatal: false,\n        error,\n        frag,\n        reason: `Found no media in msn ${frag.sn} of level \"${level.url}\"`,\n      });\n      if (!this.hls) {\n        return;\n      }\n      this.resetTransmuxer();\n      // For this error fallthrough. Marking parsed will allow advancing to next fragment.\n    }\n    this.state = State.PARSED;\n    this.hls.trigger(Events.FRAG_PARSED, { frag, part });\n  }\n\n  protected resetTransmuxer() {\n    if (this.transmuxer) {\n      this.transmuxer.destroy();\n      this.transmuxer = null;\n    }\n  }\n\n  protected recoverWorkerError(data: ErrorData) {\n    if (data.event === 'demuxerWorker') {\n      this.fragmentTracker.removeAllFragments();\n      this.resetTransmuxer();\n      this.resetStartWhenNotLoaded(this.levelLastLoaded);\n      this.resetLoadingState();\n    }\n  }\n\n  set state(nextState) {\n    const previousState = this._state;\n    if (previousState !== nextState) {\n      this._state = nextState;\n      this.log(`${previousState}->${nextState}`);\n    }\n  }\n\n  get state() {\n    return this._state;\n  }\n}\n", "export default class ChunkCache {\n  private chunks: Array<Uint8Array> = [];\n  public dataLength: number = 0;\n\n  push(chunk: Uint8Array) {\n    this.chunks.push(chunk);\n    this.dataLength += chunk.length;\n  }\n\n  flush(): Uint8Array {\n    const { chunks, dataLength } = this;\n    let result;\n    if (!chunks.length) {\n      return new Uint8Array(0);\n    } else if (chunks.length === 1) {\n      result = chunks[0];\n    } else {\n      result = concatUint8Arrays(chunks, dataLength);\n    }\n    this.reset();\n    return result;\n  }\n\n  reset() {\n    this.chunks.length = 0;\n    this.dataLength = 0;\n  }\n}\n\nfunction concatUint8Arrays(\n  chunks: Array<Uint8Array>,\n  dataLength: number,\n): Uint8Array {\n  const result = new Uint8Array(dataLength);\n  let offset = 0;\n  for (let i = 0; i < chunks.length; i++) {\n    const chunk = chunks[i];\n    result.set(chunk, offset);\n    offset += chunk.length;\n  }\n  return result;\n}\n", "// ensure the worker ends up in the bundle\n// If the worker should not be included this gets aliased to empty.js\nimport './transmuxer-worker';\n\nexport function hasUMDWorker(): boolean {\n  return typeof __HLS_WORKER_BUNDLE__ === 'function';\n}\n\nexport type WorkerContext = {\n  worker: Worker;\n  objectURL?: string;\n  scriptURL?: string;\n};\n\nexport function injectWorker(): WorkerContext {\n  const blob = new self.Blob(\n    [\n      `var exports={};var module={exports:exports};function define(f){f()};define.amd=true;(${__HLS_WORKER_BUNDLE__.toString()})(true);`,\n    ],\n    {\n      type: 'text/javascript',\n    },\n  );\n  const objectURL = self.URL.createObjectURL(blob);\n  const worker = new self.Worker(objectURL);\n\n  return {\n    worker,\n    objectURL,\n  };\n}\n\nexport function loadWorker(path: string): WorkerContext {\n  const scriptURL = new self.URL(path, self.location.href).href;\n  const worker = new self.Worker(scriptURL);\n\n  return {\n    worker,\n    scriptURL,\n  };\n}\n", "import type { DemuxedTrack } from '../types/demuxer';\n\nexport function dummyTrack(type = '', inputTimeScale = 90000): DemuxedTrack {\n  return {\n    type,\n    id: -1,\n    pid: -1,\n    inputTimeScale,\n    sequenceNumber: -1,\n    samples: [],\n    dropped: 0,\n  };\n}\n", "import * as ID3 from '../id3';\nimport {\n  DemuxerResult,\n  Demuxer,\n  DemuxedAudioTrack,\n  AudioFrame,\n  DemuxedMetadataTrack,\n  DemuxedVideoTrackBase,\n  DemuxedUserdataTrack,\n  KeyData,\n  MetadataSchema,\n} from '../../types/demuxer';\nimport { dummyTrack } from '../dummy-demuxed-track';\nimport { appendUint8Array } from '../../utils/mp4-tools';\nimport { sliceUint8 } from '../../utils/typed-array';\nimport { RationalTimestamp } from '../../utils/timescale-conversion';\n\nclass BaseAudioDemuxer implements Demuxer {\n  protected _audioTrack!: DemuxedAudioTrack;\n  protected _id3Track!: DemuxedMetadataTrack;\n  protected frameIndex: number = 0;\n  protected cachedData: Uint8Array | null = null;\n  protected basePTS: number | null = null;\n  protected initPTS: RationalTimestamp | null = null;\n  protected lastPTS: number | null = null;\n\n  resetInitSegment(\n    initSegment: Uint8Array | undefined,\n    audioCodec: string | undefined,\n    videoCodec: string | undefined,\n    trackDuration: number,\n  ) {\n    this._id3Track = {\n      type: 'id3',\n      id: 3,\n      pid: -1,\n      inputTimeScale: 90000,\n      sequenceNumber: 0,\n      samples: [],\n      dropped: 0,\n    };\n  }\n\n  resetTimeStamp(deaultTimestamp: RationalTimestamp | null) {\n    this.initPTS = deaultTimestamp;\n    this.resetContiguity();\n  }\n\n  resetContiguity(): void {\n    this.basePTS = null;\n    this.lastPTS = null;\n    this.frameIndex = 0;\n  }\n\n  canParse(data: Uint8Array, offset: number): boolean {\n    return false;\n  }\n\n  appendFrame(\n    track: DemuxedAudioTrack,\n    data: Uint8Array,\n    offset: number,\n  ): AudioFrame | void {}\n\n  // feed incoming data to the front of the parsing pipeline\n  demux(data: Uint8Array, timeOffset: number): DemuxerResult {\n    if (this.cachedData) {\n      data = appendUint8Array(this.cachedData, data);\n      this.cachedData = null;\n    }\n\n    let id3Data: Uint8Array | undefined = ID3.getID3Data(data, 0);\n    let offset = id3Data ? id3Data.length : 0;\n    let lastDataIndex;\n    const track = this._audioTrack;\n    const id3Track = this._id3Track;\n    const timestamp = id3Data ? ID3.getTimeStamp(id3Data) : undefined;\n    const length = data.length;\n\n    if (\n      this.basePTS === null ||\n      (this.frameIndex === 0 && Number.isFinite(timestamp))\n    ) {\n      this.basePTS = initPTSFn(timestamp, timeOffset, this.initPTS);\n      this.lastPTS = this.basePTS;\n    }\n\n    if (this.lastPTS === null) {\n      this.lastPTS = this.basePTS;\n    }\n\n    // more expressive than alternative: id3Data?.length\n    if (id3Data && id3Data.length > 0) {\n      id3Track.samples.push({\n        pts: this.lastPTS,\n        dts: this.lastPTS,\n        data: id3Data,\n        type: MetadataSchema.audioId3,\n        duration: Number.POSITIVE_INFINITY,\n      });\n    }\n\n    while (offset < length) {\n      if (this.canParse(data, offset)) {\n        const frame = this.appendFrame(track, data, offset);\n        if (frame) {\n          this.frameIndex++;\n          this.lastPTS = frame.sample.pts;\n          offset += frame.length;\n          lastDataIndex = offset;\n        } else {\n          offset = length;\n        }\n      } else if (ID3.canParse(data, offset)) {\n        // after a ID3.canParse, a call to ID3.getID3Data *should* always returns some data\n        id3Data = ID3.getID3Data(data, offset)!;\n        id3Track.samples.push({\n          pts: this.lastPTS,\n          dts: this.lastPTS,\n          data: id3Data,\n          type: MetadataSchema.audioId3,\n          duration: Number.POSITIVE_INFINITY,\n        });\n        offset += id3Data.length;\n        lastDataIndex = offset;\n      } else {\n        offset++;\n      }\n      if (offset === length && lastDataIndex !== length) {\n        const partialData = sliceUint8(data, lastDataIndex);\n        if (this.cachedData) {\n          this.cachedData = appendUint8Array(this.cachedData, partialData);\n        } else {\n          this.cachedData = partialData;\n        }\n      }\n    }\n\n    return {\n      audioTrack: track,\n      videoTrack: dummyTrack() as DemuxedVideoTrackBase,\n      id3Track,\n      textTrack: dummyTrack() as DemuxedUserdataTrack,\n    };\n  }\n\n  demuxSampleAes(\n    data: Uint8Array,\n    keyData: KeyData,\n    timeOffset: number,\n  ): Promise<DemuxerResult> {\n    return Promise.reject(\n      new Error(\n        `[${this}] This demuxer does not support Sample-AES decryption`,\n      ),\n    );\n  }\n\n  flush(timeOffset: number): DemuxerResult {\n    // Parse cache in case of remaining frames.\n    const cachedData = this.cachedData;\n    if (cachedData) {\n      this.cachedData = null;\n      this.demux(cachedData, 0);\n    }\n\n    return {\n      audioTrack: this._audioTrack,\n      videoTrack: dummyTrack() as DemuxedVideoTrackBase,\n      id3Track: this._id3Track,\n      textTrack: dummyTrack() as DemuxedUserdataTrack,\n    };\n  }\n\n  destroy() {}\n}\n\n/**\n * Initialize PTS\n * <p>\n *    use timestamp unless it is undefined, NaN or Infinity\n * </p>\n */\nexport const initPTSFn = (\n  timestamp: number | undefined,\n  timeOffset: number,\n  initPTS: RationalTimestamp | null,\n): number => {\n  if (Number.isFinite(timestamp as number)) {\n    return timestamp! * 90;\n  }\n  const init90kHz = initPTS\n    ? (initPTS.baseTime * 90000) / initPTS.timescale\n    : 0;\n  return timeOffset * 90000 + init90kHz;\n};\nexport default BaseAudioDemuxer;\n", "/**\n * ADTS parser helper\n * @link https://wiki.multimedia.cx/index.php?title=ADTS\n */\nimport { logger } from '../../utils/logger';\nimport { ErrorTypes, ErrorDetails } from '../../errors';\nimport type { HlsEventEmitter } from '../../events';\nimport { Events } from '../../events';\nimport type {\n  DemuxedAudioTrack,\n  AudioFrame,\n  AudioSample,\n} from '../../types/demuxer';\n\ntype AudioConfig = {\n  config: number[];\n  samplerate: number;\n  channelCount: number;\n  codec: string;\n  manifestCodec: string;\n};\n\ntype FrameHeader = {\n  headerLength: number;\n  frameLength: number;\n};\n\nexport function getAudioConfig(\n  observer: HlsEventEmitter,\n  data: Uint8Array,\n  offset: number,\n  audioCodec: string,\n): AudioConfig | void {\n  let adtsObjectType: number;\n  let adtsExtensionSamplingIndex: number;\n  let adtsChannelConfig: number;\n  let config: number[];\n  const userAgent = navigator.userAgent.toLowerCase();\n  const manifestCodec = audioCodec;\n  const adtsSamplingRates = [\n    96000, 88200, 64000, 48000, 44100, 32000, 24000, 22050, 16000, 12000, 11025,\n    8000, 7350,\n  ];\n  // byte 2\n  adtsObjectType = ((data[offset + 2] & 0xc0) >>> 6) + 1;\n  const adtsSamplingIndex = (data[offset + 2] & 0x3c) >>> 2;\n  if (adtsSamplingIndex > adtsSamplingRates.length - 1) {\n    const error = new Error(`invalid ADTS sampling index:${adtsSamplingIndex}`);\n    observer.emit(Events.ERROR, Events.ERROR, {\n      type: ErrorTypes.MEDIA_ERROR,\n      details: ErrorDetails.FRAG_PARSING_ERROR,\n      fatal: true,\n      error,\n      reason: error.message,\n    });\n    return;\n  }\n  adtsChannelConfig = (data[offset + 2] & 0x01) << 2;\n  // byte 3\n  adtsChannelConfig |= (data[offset + 3] & 0xc0) >>> 6;\n  logger.log(\n    `manifest codec:${audioCodec}, ADTS type:${adtsObjectType}, samplingIndex:${adtsSamplingIndex}`,\n  );\n  // firefox: freq less than 24kHz = AAC SBR (HE-AAC)\n  if (/firefox/i.test(userAgent)) {\n    if (adtsSamplingIndex >= 6) {\n      adtsObjectType = 5;\n      config = new Array(4);\n      // HE-AAC uses SBR (Spectral Band Replication) , high frequencies are constructed from low frequencies\n      // there is a factor 2 between frame sample rate and output sample rate\n      // multiply frequency by 2 (see table below, equivalent to substract 3)\n      adtsExtensionSamplingIndex = adtsSamplingIndex - 3;\n    } else {\n      adtsObjectType = 2;\n      config = new Array(2);\n      adtsExtensionSamplingIndex = adtsSamplingIndex;\n    }\n    // Android : always use AAC\n  } else if (userAgent.indexOf('android') !== -1) {\n    adtsObjectType = 2;\n    config = new Array(2);\n    adtsExtensionSamplingIndex = adtsSamplingIndex;\n  } else {\n    /*  for other browsers (Chrome/Vivaldi/Opera ...)\n        always force audio type to be HE-AAC SBR, as some browsers do not support audio codec switch properly (like Chrome ...)\n    */\n    adtsObjectType = 5;\n    config = new Array(4);\n    // if (manifest codec is HE-AAC or HE-AACv2) OR (manifest codec not specified AND frequency less than 24kHz)\n    if (\n      (audioCodec &&\n        (audioCodec.indexOf('mp4a.40.29') !== -1 ||\n          audioCodec.indexOf('mp4a.40.5') !== -1)) ||\n      (!audioCodec && adtsSamplingIndex >= 6)\n    ) {\n      // HE-AAC uses SBR (Spectral Band Replication) , high frequencies are constructed from low frequencies\n      // there is a factor 2 between frame sample rate and output sample rate\n      // multiply frequency by 2 (see table below, equivalent to substract 3)\n      adtsExtensionSamplingIndex = adtsSamplingIndex - 3;\n    } else {\n      // if (manifest codec is AAC) AND (frequency less than 24kHz AND nb channel is 1) OR (manifest codec not specified and mono audio)\n      // Chrome fails to play back with low frequency AAC LC mono when initialized with HE-AAC.  This is not a problem with stereo.\n      if (\n        (audioCodec &&\n          audioCodec.indexOf('mp4a.40.2') !== -1 &&\n          ((adtsSamplingIndex >= 6 && adtsChannelConfig === 1) ||\n            /vivaldi/i.test(userAgent))) ||\n        (!audioCodec && adtsChannelConfig === 1)\n      ) {\n        adtsObjectType = 2;\n        config = new Array(2);\n      }\n      adtsExtensionSamplingIndex = adtsSamplingIndex;\n    }\n  }\n  /* refer to http://wiki.multimedia.cx/index.php?title=MPEG-4_Audio#Audio_Specific_Config\n      ISO 14496-3 (AAC).pdf - Table 1.13  Syntax of AudioSpecificConfig()\n    Audio Profile / Audio Object Type\n    0: Null\n    1: AAC Main\n    2: AAC LC (Low Complexity)\n    3: AAC SSR (Scalable Sample Rate)\n    4: AAC LTP (Long Term Prediction)\n    5: SBR (Spectral Band Replication)\n    6: AAC Scalable\n   sampling freq\n    0: 96000 Hz\n    1: 88200 Hz\n    2: 64000 Hz\n    3: 48000 Hz\n    4: 44100 Hz\n    5: 32000 Hz\n    6: 24000 Hz\n    7: 22050 Hz\n    8: 16000 Hz\n    9: 12000 Hz\n    10: 11025 Hz\n    11: 8000 Hz\n    12: 7350 Hz\n    13: Reserved\n    14: Reserved\n    15: frequency is written explictly\n    Channel Configurations\n    These are the channel configurations:\n    0: Defined in AOT Specifc Config\n    1: 1 channel: front-center\n    2: 2 channels: front-left, front-right\n  */\n  // audioObjectType = profile => profile, the MPEG-4 Audio Object Type minus 1\n  config[0] = adtsObjectType << 3;\n  // samplingFrequencyIndex\n  config[0] |= (adtsSamplingIndex & 0x0e) >> 1;\n  config[1] |= (adtsSamplingIndex & 0x01) << 7;\n  // channelConfiguration\n  config[1] |= adtsChannelConfig << 3;\n  if (adtsObjectType === 5) {\n    // adtsExtensionSamplingIndex\n    config[1] |= (adtsExtensionSamplingIndex & 0x0e) >> 1;\n    config[2] = (adtsExtensionSamplingIndex & 0x01) << 7;\n    // adtsObjectType (force to 2, chrome is checking that object type is less than 5 ???\n    //    https://chromium.googlesource.com/chromium/src.git/+/master/media/formats/mp4/aac.cc\n    config[2] |= 2 << 2;\n    config[3] = 0;\n  }\n  return {\n    config,\n    samplerate: adtsSamplingRates[adtsSamplingIndex],\n    channelCount: adtsChannelConfig,\n    codec: 'mp4a.40.' + adtsObjectType,\n    manifestCodec,\n  };\n}\n\nexport function isHeaderPattern(data: Uint8Array, offset: number): boolean {\n  return data[offset] === 0xff && (data[offset + 1] & 0xf6) === 0xf0;\n}\n\nexport function getHeaderLength(data: Uint8Array, offset: number): number {\n  return data[offset + 1] & 0x01 ? 7 : 9;\n}\n\nexport function getFullFrameLength(data: Uint8Array, offset: number): number {\n  return (\n    ((data[offset + 3] & 0x03) << 11) |\n    (data[offset + 4] << 3) |\n    ((data[offset + 5] & 0xe0) >>> 5)\n  );\n}\n\nexport function canGetFrameLength(data: Uint8Array, offset: number): boolean {\n  return offset + 5 < data.length;\n}\n\nexport function isHeader(data: Uint8Array, offset: number): boolean {\n  // Look for ADTS header | 1111 1111 | 1111 X00X | where X can be either 0 or 1\n  // Layer bits (position 14 and 15) in header should be always 0 for ADTS\n  // More info https://wiki.multimedia.cx/index.php?title=ADTS\n  return offset + 1 < data.length && isHeaderPattern(data, offset);\n}\n\nexport function canParse(data: Uint8Array, offset: number): boolean {\n  return (\n    canGetFrameLength(data, offset) &&\n    isHeaderPattern(data, offset) &&\n    getFullFrameLength(data, offset) <= data.length - offset\n  );\n}\n\nexport function probe(data: Uint8Array, offset: number): boolean {\n  // same as isHeader but we also check that ADTS frame follows last ADTS frame\n  // or end of data is reached\n  if (isHeader(data, offset)) {\n    // ADTS header Length\n    const headerLength = getHeaderLength(data, offset);\n    if (offset + headerLength >= data.length) {\n      return false;\n    }\n    // ADTS frame Length\n    const frameLength = getFullFrameLength(data, offset);\n    if (frameLength <= headerLength) {\n      return false;\n    }\n\n    const newOffset = offset + frameLength;\n    return newOffset === data.length || isHeader(data, newOffset);\n  }\n  return false;\n}\n\nexport function initTrackConfig(\n  track: DemuxedAudioTrack,\n  observer: HlsEventEmitter,\n  data: Uint8Array,\n  offset: number,\n  audioCodec: string,\n) {\n  if (!track.samplerate) {\n    const config = getAudioConfig(observer, data, offset, audioCodec);\n    if (!config) {\n      return;\n    }\n    track.config = config.config;\n    track.samplerate = config.samplerate;\n    track.channelCount = config.channelCount;\n    track.codec = config.codec;\n    track.manifestCodec = config.manifestCodec;\n    logger.log(\n      `parsed codec:${track.codec}, rate:${config.samplerate}, channels:${config.channelCount}`,\n    );\n  }\n}\n\nexport function getFrameDuration(samplerate: number): number {\n  return (1024 * 90000) / samplerate;\n}\n\nexport function parseFrameHeader(\n  data: Uint8Array,\n  offset: number,\n): FrameHeader | void {\n  // The protection skip bit tells us if we have 2 bytes of CRC data at the end of the ADTS header\n  const headerLength = getHeaderLength(data, offset);\n  if (offset + headerLength <= data.length) {\n    // retrieve frame size\n    const frameLength = getFullFrameLength(data, offset) - headerLength;\n    if (frameLength > 0) {\n      // logger.log(`AAC frame, offset/length/total/pts:${offset+headerLength}/${frameLength}/${data.byteLength}`);\n      return { headerLength, frameLength };\n    }\n  }\n}\n\nexport function appendFrame(\n  track: DemuxedAudioTrack,\n  data: Uint8Array,\n  offset: number,\n  pts: number,\n  frameIndex: number,\n): AudioFrame {\n  const frameDuration = getFrameDuration(track.samplerate as number);\n  const stamp = pts + frameIndex * frameDuration;\n  const header = parseFrameHeader(data, offset);\n  let unit: Uint8Array;\n  if (header) {\n    const { frameLength, headerLength } = header;\n    const length = headerLength + frameLength;\n    const missing = Math.max(0, offset + length - data.length);\n    // logger.log(`AAC frame ${frameIndex}, pts:${stamp} length@offset/total: ${frameLength}@${offset+headerLength}/${data.byteLength} missing: ${missing}`);\n    if (missing) {\n      unit = new Uint8Array(length - headerLength);\n      unit.set(data.subarray(offset + headerLength, data.length), 0);\n    } else {\n      unit = data.subarray(offset + headerLength, offset + length);\n    }\n\n    const sample: AudioSample = {\n      unit,\n      pts: stamp,\n    };\n    if (!missing) {\n      track.samples.push(sample as AudioSample);\n    }\n\n    return { sample, length, missing };\n  }\n  // overflow incomplete header\n  const length = data.length - offset;\n  unit = new Uint8Array(length);\n  unit.set(data.subarray(offset, data.length), 0);\n  const sample: AudioSample = {\n    unit,\n    pts: stamp,\n  };\n  return { sample, length, missing: -1 };\n}\n", "/**\n *  MPEG parser helper\n */\nimport { DemuxedAudioTrack } from '../../types/demuxer';\n\nlet chromeVersion: number | null = null;\n\nconst BitratesMap = [\n  32, 64, 96, 128, 160, 192, 224, 256, 288, 320, 352, 384, 416, 448, 32, 48, 56,\n  64, 80, 96, 112, 128, 160, 192, 224, 256, 320, 384, 32, 40, 48, 56, 64, 80,\n  96, 112, 128, 160, 192, 224, 256, 320, 32, 48, 56, 64, 80, 96, 112, 128, 144,\n  160, 176, 192, 224, 256, 8, 16, 24, 32, 40, 48, 56, 64, 80, 96, 112, 128, 144,\n  160,\n];\n\nconst SamplingRateMap = [\n  44100, 48000, 32000, 22050, 24000, 16000, 11025, 12000, 8000,\n];\n\nconst SamplesCoefficients = [\n  // MPEG 2.5\n  [\n    0, // Reserved\n    72, // Layer3\n    144, // Layer2\n    12, // Layer1\n  ],\n  // Reserved\n  [\n    0, // Reserved\n    0, // Layer3\n    0, // Layer2\n    0, // Layer1\n  ],\n  // MPEG 2\n  [\n    0, // Reserved\n    72, // Layer3\n    144, // Layer2\n    12, // Layer1\n  ],\n  // MPEG 1\n  [\n    0, // Reserved\n    144, // Layer3\n    144, // Layer2\n    12, // Layer1\n  ],\n];\n\nconst BytesInSlot = [\n  0, // Reserved\n  1, // Layer3\n  1, // Layer2\n  4, // Layer1\n];\n\nexport function appendFrame(\n  track: DemuxedAudioTrack,\n  data: Uint8Array,\n  offset: number,\n  pts: number,\n  frameIndex: number,\n) {\n  // Using http://www.datavoyage.com/mpgscript/mpeghdr.htm as a reference\n  if (offset + 24 > data.length) {\n    return;\n  }\n\n  const header = parseHeader(data, offset);\n  if (header && offset + header.frameLength <= data.length) {\n    const frameDuration = (header.samplesPerFrame * 90000) / header.sampleRate;\n    const stamp = pts + frameIndex * frameDuration;\n    const sample = {\n      unit: data.subarray(offset, offset + header.frameLength),\n      pts: stamp,\n      dts: stamp,\n    };\n\n    track.config = [];\n    track.channelCount = header.channelCount;\n    track.samplerate = header.sampleRate;\n    track.samples.push(sample);\n\n    return { sample, length: header.frameLength, missing: 0 };\n  }\n}\n\nexport function parseHeader(data: Uint8Array, offset: number) {\n  const mpegVersion = (data[offset + 1] >> 3) & 3;\n  const mpegLayer = (data[offset + 1] >> 1) & 3;\n  const bitRateIndex = (data[offset + 2] >> 4) & 15;\n  const sampleRateIndex = (data[offset + 2] >> 2) & 3;\n  if (\n    mpegVersion !== 1 &&\n    bitRateIndex !== 0 &&\n    bitRateIndex !== 15 &&\n    sampleRateIndex !== 3\n  ) {\n    const paddingBit = (data[offset + 2] >> 1) & 1;\n    const channelMode = data[offset + 3] >> 6;\n    const columnInBitrates =\n      mpegVersion === 3 ? 3 - mpegLayer : mpegLayer === 3 ? 3 : 4;\n    const bitRate =\n      BitratesMap[columnInBitrates * 14 + bitRateIndex - 1] * 1000;\n    const columnInSampleRates =\n      mpegVersion === 3 ? 0 : mpegVersion === 2 ? 1 : 2;\n    const sampleRate =\n      SamplingRateMap[columnInSampleRates * 3 + sampleRateIndex];\n    const channelCount = channelMode === 3 ? 1 : 2; // If bits of channel mode are `11` then it is a single channel (Mono)\n    const sampleCoefficient = SamplesCoefficients[mpegVersion][mpegLayer];\n    const bytesInSlot = BytesInSlot[mpegLayer];\n    const samplesPerFrame = sampleCoefficient * 8 * bytesInSlot;\n    const frameLength =\n      Math.floor((sampleCoefficient * bitRate) / sampleRate + paddingBit) *\n      bytesInSlot;\n\n    if (chromeVersion === null) {\n      const userAgent = navigator.userAgent || '';\n      const result = userAgent.match(/Chrome\\/(\\d+)/i);\n      chromeVersion = result ? parseInt(result[1]) : 0;\n    }\n    const needChromeFix = !!chromeVersion && chromeVersion <= 87;\n\n    if (\n      needChromeFix &&\n      mpegLayer === 2 &&\n      bitRate >= 224000 &&\n      channelMode === 0\n    ) {\n      // Work around bug in Chromium by setting channelMode to dual-channel (01) instead of stereo (00)\n      data[offset + 3] = data[offset + 3] | 0x80;\n    }\n\n    return { sampleRate, channelCount, frameLength, samplesPerFrame };\n  }\n}\n\nexport function isHeaderPattern(data: Uint8Array, offset: number): boolean {\n  return (\n    data[offset] === 0xff &&\n    (data[offset + 1] & 0xe0) === 0xe0 &&\n    (data[offset + 1] & 0x06) !== 0x00\n  );\n}\n\nexport function isHeader(data: Uint8Array, offset: number): boolean {\n  // Look for MPEG header | 1111 1111 | 111X XYZX | where X can be either 0 or 1 and Y or Z should be 1\n  // Layer bits (position 14 and 15) in header should be always different from 0 (Layer I or Layer II or Layer III)\n  // More info http://www.mp3-tech.org/programmer/frame_header.html\n  return offset + 1 < data.length && isHeaderPattern(data, offset);\n}\n\nexport function canParse(data: Uint8Array, offset: number): boolean {\n  const headerSize = 4;\n\n  return isHeaderPattern(data, offset) && headerSize <= data.length - offset;\n}\n\nexport function probe(data: Uint8Array, offset: number): boolean {\n  // same as isHeader but we also check that MPEG frame follows last MPEG frame\n  // or end of data is reached\n  if (offset + 1 < data.length && isHeaderPattern(data, offset)) {\n    // MPEG header Length\n    const headerLength = 4;\n    // MPEG frame Length\n    const header = parseHeader(data, offset);\n    let frameLength = headerLength;\n    if (header?.frameLength) {\n      frameLength = header.frameLength;\n    }\n\n    const newOffset = offset + frameLength;\n    return newOffset === data.length || isHeader(data, newOffset);\n  }\n  return false;\n}\n", "/**\n * AAC demuxer\n */\nimport BaseAudioDemuxer from './base-audio-demuxer';\nimport * as ADTS from './adts';\nimport * as MpegAudio from './mpegaudio';\nimport { logger } from '../../utils/logger';\nimport * as ID3 from '../id3';\nimport type { HlsEventEmitter } from '../../events';\nimport type { HlsConfig } from '../../config';\n\nclass AACDemuxer extends BaseAudioDemuxer {\n  private readonly observer: HlsEventEmitter;\n  private readonly config: HlsConfig;\n\n  constructor(observer, config) {\n    super();\n    this.observer = observer;\n    this.config = config;\n  }\n\n  resetInitSegment(\n    initSegment: Uint8Array | undefined,\n    audioCodec: string | undefined,\n    videoCodec: string | undefined,\n    trackDuration: number,\n  ) {\n    super.resetInitSegment(initSegment, audioCodec, videoCodec, trackDuration);\n    this._audioTrack = {\n      container: 'audio/adts',\n      type: 'audio',\n      id: 2,\n      pid: -1,\n      sequenceNumber: 0,\n      segmentCodec: 'aac',\n      samples: [],\n      manifestCodec: audioCodec,\n      duration: trackDuration,\n      inputTimeScale: 90000,\n      dropped: 0,\n    };\n  }\n\n  // Source for probe info - https://wiki.multimedia.cx/index.php?title=ADTS\n  static probe(data: Uint8Array | undefined): boolean {\n    if (!data) {\n      return false;\n    }\n\n    // Check for the ADTS sync word\n    // Look for ADTS header | 1111 1111 | 1111 X00X | where X can be either 0 or 1\n    // Layer bits (position 14 and 15) in header should be always 0 for ADTS\n    // More info https://wiki.multimedia.cx/index.php?title=ADTS\n    const id3Data = ID3.getID3Data(data, 0);\n    let offset = id3Data?.length || 0;\n\n    if (MpegAudio.probe(data, offset)) {\n      return false;\n    }\n\n    for (let length = data.length; offset < length; offset++) {\n      if (ADTS.probe(data, offset)) {\n        logger.log('ADTS sync word found !');\n        return true;\n      }\n    }\n    return false;\n  }\n\n  canParse(data, offset) {\n    return ADTS.canParse(data, offset);\n  }\n\n  appendFrame(track, data, offset) {\n    ADTS.initTrackConfig(\n      track,\n      this.observer,\n      data,\n      offset,\n      track.manifestCodec,\n    );\n    const frame = ADTS.appendFrame(\n      track,\n      data,\n      offset,\n      this.basePTS as number,\n      this.frameIndex,\n    );\n    if (frame && frame.missing === 0) {\n      return frame;\n    }\n  }\n}\n\nexport default AACDemuxer;\n", "/**\n * MP4 demuxer\n */\nimport {\n  Demuxer,\n  DemuxerResult,\n  PassthroughTrack,\n  DemuxedAudioTrack,\n  DemuxedUserdataTrack,\n  DemuxedMetadataTrack,\n  KeyData,\n  MetadataSchema,\n} from '../types/demuxer';\nimport {\n  findBox,\n  segmentValidRange,\n  appendUint8Array,\n  parseEmsg,\n  parseSamples,\n  parseInitSegment,\n  RemuxerTrackIdConfig,\n  hasMoofData,\n} from '../utils/mp4-tools';\nimport { dummyTrack } from './dummy-demuxed-track';\nimport type { HlsEventEmitter } from '../events';\nimport type { HlsConfig } from '../config';\n\nconst emsgSchemePattern = /\\/emsg[-/]ID3/i;\n\nclass MP4Demuxer implements Demuxer {\n  private remainderData: Uint8Array | null = null;\n  private timeOffset: number = 0;\n  private config: HlsConfig;\n  private videoTrack?: PassthroughTrack;\n  private audioTrack?: DemuxedAudioTrack;\n  private id3Track?: DemuxedMetadataTrack;\n  private txtTrack?: DemuxedUserdataTrack;\n\n  constructor(observer: HlsEventEmitter, config: HlsConfig) {\n    this.config = config;\n  }\n\n  public resetTimeStamp() {}\n\n  public resetInitSegment(\n    initSegment: Uint8Array | undefined,\n    audioCodec: string | undefined,\n    videoCodec: string | undefined,\n    trackDuration: number,\n  ) {\n    const videoTrack = (this.videoTrack = dummyTrack(\n      'video',\n      1,\n    ) as PassthroughTrack);\n    const audioTrack = (this.audioTrack = dummyTrack(\n      'audio',\n      1,\n    ) as DemuxedAudioTrack);\n    const captionTrack = (this.txtTrack = dummyTrack(\n      'text',\n      1,\n    ) as DemuxedUserdataTrack);\n\n    this.id3Track = dummyTrack('id3', 1) as DemuxedMetadataTrack;\n    this.timeOffset = 0;\n\n    if (!initSegment?.byteLength) {\n      return;\n    }\n    const initData = parseInitSegment(initSegment);\n\n    if (initData.video) {\n      const { id, timescale, codec } = initData.video;\n      videoTrack.id = id;\n      videoTrack.timescale = captionTrack.timescale = timescale;\n      videoTrack.codec = codec;\n    }\n\n    if (initData.audio) {\n      const { id, timescale, codec } = initData.audio;\n      audioTrack.id = id;\n      audioTrack.timescale = timescale;\n      audioTrack.codec = codec;\n    }\n\n    captionTrack.id = RemuxerTrackIdConfig.text;\n    videoTrack.sampleDuration = 0;\n    videoTrack.duration = audioTrack.duration = trackDuration;\n  }\n\n  public resetContiguity(): void {\n    this.remainderData = null;\n  }\n\n  static probe(data: Uint8Array) {\n    return hasMoofData(data);\n  }\n\n  public demux(data: Uint8Array, timeOffset: number): DemuxerResult {\n    this.timeOffset = timeOffset;\n    // Load all data into the avc track. The CMAF remuxer will look for the data in the samples object; the rest of the fields do not matter\n    let videoSamples = data;\n    const videoTrack = this.videoTrack as PassthroughTrack;\n    const textTrack = this.txtTrack as DemuxedUserdataTrack;\n    if (this.config.progressive) {\n      // Split the bytestream into two ranges: one encompassing all data up until the start of the last moof, and everything else.\n      // This is done to guarantee that we're sending valid data to MSE - when demuxing progressively, we have no guarantee\n      // that the fetch loader gives us flush moof+mdat pairs. If we push jagged data to MSE, it will throw an exception.\n      if (this.remainderData) {\n        videoSamples = appendUint8Array(this.remainderData, data);\n      }\n      const segmentedData = segmentValidRange(videoSamples);\n      this.remainderData = segmentedData.remainder;\n      videoTrack.samples = segmentedData.valid || new Uint8Array();\n    } else {\n      videoTrack.samples = videoSamples;\n    }\n\n    const id3Track = this.extractID3Track(videoTrack, timeOffset);\n    textTrack.samples = parseSamples(timeOffset, videoTrack);\n\n    return {\n      videoTrack,\n      audioTrack: this.audioTrack as DemuxedAudioTrack,\n      id3Track,\n      textTrack: this.txtTrack as DemuxedUserdataTrack,\n    };\n  }\n\n  public flush() {\n    const timeOffset = this.timeOffset;\n    const videoTrack = this.videoTrack as PassthroughTrack;\n    const textTrack = this.txtTrack as DemuxedUserdataTrack;\n    videoTrack.samples = this.remainderData || new Uint8Array();\n    this.remainderData = null;\n\n    const id3Track = this.extractID3Track(videoTrack, this.timeOffset);\n    textTrack.samples = parseSamples(timeOffset, videoTrack);\n\n    return {\n      videoTrack,\n      audioTrack: dummyTrack() as DemuxedAudioTrack,\n      id3Track,\n      textTrack: dummyTrack() as DemuxedUserdataTrack,\n    };\n  }\n\n  private extractID3Track(\n    videoTrack: PassthroughTrack,\n    timeOffset: number,\n  ): DemuxedMetadataTrack {\n    const id3Track = this.id3Track as DemuxedMetadataTrack;\n    if (videoTrack.samples.length) {\n      const emsgs = findBox(videoTrack.samples, ['emsg']);\n      if (emsgs) {\n        emsgs.forEach((data: Uint8Array) => {\n          const emsgInfo = parseEmsg(data);\n          if (emsgSchemePattern.test(emsgInfo.schemeIdUri)) {\n            const pts = Number.isFinite(emsgInfo.presentationTime)\n              ? emsgInfo.presentationTime! / emsgInfo.timeScale\n              : timeOffset +\n                emsgInfo.presentationTimeDelta! / emsgInfo.timeScale;\n            let duration =\n              emsgInfo.eventDuration === 0xffffffff\n                ? Number.POSITIVE_INFINITY\n                : emsgInfo.eventDuration / emsgInfo.timeScale;\n            // Safari takes anything <= 0.001 seconds and maps it to Infinity\n            if (duration <= 0.001) {\n              duration = Number.POSITIVE_INFINITY;\n            }\n            const payload = emsgInfo.payload;\n            id3Track.samples.push({\n              data: payload,\n              len: payload.byteLength,\n              dts: pts,\n              pts: pts,\n              type: MetadataSchema.emsg,\n              duration: duration,\n            });\n          }\n        });\n      }\n    }\n    return id3Track;\n  }\n\n  demuxSampleAes(\n    data: Uint8Array,\n    keyData: KeyData,\n    timeOffset: number,\n  ): Promise<DemuxerResult> {\n    return Promise.reject(\n      new Error('The MP4 demuxer does not support SAMPLE-AES decryption'),\n    );\n  }\n\n  destroy() {}\n}\n\nexport default MP4Demuxer;\n", "export const getAudioBSID = (data: Uint8Array, offset: number): number => {\n  // check the bsid to confirm ac-3 | ec-3\n  let bsid = 0;\n  let numBits = 5;\n  offset += numBits;\n  const temp = new Uint32Array(1); // unsigned 32 bit for temporary storage\n  const mask = new Uint32Array(1); // unsigned 32 bit mask value\n  const byte = new Uint8Array(1); // unsigned 8 bit for temporary storage\n  while (numBits > 0) {\n    byte[0] = data[offset];\n    // read remaining bits, upto 8 bits at a time\n    const bits = Math.min(numBits, 8);\n    const shift = 8 - bits;\n    mask[0] = (0xff000000 >>> (24 + shift)) << shift;\n    temp[0] = (byte[0] & mask[0]) >> shift;\n    bsid = !bsid ? temp[0] : (bsid << bits) | temp[0];\n    offset += 1;\n    numBits -= bits;\n  }\n  return bsid;\n};\n", "import BaseAudioDemuxer from './base-audio-demuxer';\nimport { getID3Data, getTimeStamp } from '../id3';\nimport { getAudioBSID } from './dolby';\nimport type { HlsEventEmitter } from '../../events';\nimport type { AudioFrame, DemuxedAudioTrack } from '../../types/demuxer';\n\nexport class AC3Demuxer extends BaseAudioDemuxer {\n  private readonly observer: HlsEventEmitter;\n\n  constructor(observer) {\n    super();\n    this.observer = observer;\n  }\n\n  resetInitSegment(\n    initSegment: Uint8Array | undefined,\n    audioCodec: string | undefined,\n    videoCodec: string | undefined,\n    trackDuration: number,\n  ) {\n    super.resetInitSegment(initSegment, audioCodec, videoCodec, trackDuration);\n    this._audioTrack = {\n      container: 'audio/ac-3',\n      type: 'audio',\n      id: 2,\n      pid: -1,\n      sequenceNumber: 0,\n      segmentCodec: 'ac3',\n      samples: [],\n      manifestCodec: audioCodec,\n      duration: trackDuration,\n      inputTimeScale: 90000,\n      dropped: 0,\n    };\n  }\n\n  canParse(data: Uint8Array, offset: number): boolean {\n    return offset + 64 < data.length;\n  }\n\n  appendFrame(\n    track: DemuxedAudioTrack,\n    data: Uint8Array,\n    offset: number,\n  ): AudioFrame | void {\n    const frameLength = appendFrame(\n      track,\n      data,\n      offset,\n      this.basePTS as number,\n      this.frameIndex,\n    );\n    if (frameLength !== -1) {\n      const sample = track.samples[track.samples.length - 1];\n      return { sample, length: frameLength, missing: 0 };\n    }\n  }\n\n  static probe(data: Uint8Array | undefined): boolean {\n    if (!data) {\n      return false;\n    }\n\n    const id3Data = getID3Data(data, 0);\n    if (!id3Data) {\n      return false;\n    }\n\n    // look for the ac-3 sync bytes\n    const offset = id3Data.length;\n    if (\n      data[offset] === 0x0b &&\n      data[offset + 1] === 0x77 &&\n      getTimeStamp(id3Data) !== undefined &&\n      // check the bsid to confirm ac-3\n      getAudioBSID(data, offset) < 16\n    ) {\n      return true;\n    }\n    return false;\n  }\n}\n\nexport function appendFrame(\n  track: DemuxedAudioTrack,\n  data: Uint8Array,\n  start: number,\n  pts: number,\n  frameIndex: number,\n): number {\n  if (start + 8 > data.length) {\n    return -1; // not enough bytes left\n  }\n\n  if (data[start] !== 0x0b || data[start + 1] !== 0x77) {\n    return -1; // invalid magic\n  }\n\n  // get sample rate\n  const samplingRateCode = data[start + 4] >> 6;\n  if (samplingRateCode >= 3) {\n    return -1; // invalid sampling rate\n  }\n\n  const samplingRateMap = [48000, 44100, 32000];\n  const sampleRate = samplingRateMap[samplingRateCode];\n\n  // get frame size\n  const frameSizeCode = data[start + 4] & 0x3f;\n  const frameSizeMap = [\n    64, 69, 96, 64, 70, 96, 80, 87, 120, 80, 88, 120, 96, 104, 144, 96, 105,\n    144, 112, 121, 168, 112, 122, 168, 128, 139, 192, 128, 140, 192, 160, 174,\n    240, 160, 175, 240, 192, 208, 288, 192, 209, 288, 224, 243, 336, 224, 244,\n    336, 256, 278, 384, 256, 279, 384, 320, 348, 480, 320, 349, 480, 384, 417,\n    576, 384, 418, 576, 448, 487, 672, 448, 488, 672, 512, 557, 768, 512, 558,\n    768, 640, 696, 960, 640, 697, 960, 768, 835, 1152, 768, 836, 1152, 896, 975,\n    1344, 896, 976, 1344, 1024, 1114, 1536, 1024, 1115, 1536, 1152, 1253, 1728,\n    1152, 1254, 1728, 1280, 1393, 1920, 1280, 1394, 1920,\n  ];\n\n  const frameLength = frameSizeMap[frameSizeCode * 3 + samplingRateCode] * 2;\n  if (start + frameLength > data.length) {\n    return -1;\n  }\n\n  // get channel count\n  const channelMode = data[start + 6] >> 5;\n  let skipCount = 0;\n  if (channelMode === 2) {\n    skipCount += 2;\n  } else {\n    if (channelMode & 1 && channelMode !== 1) {\n      skipCount += 2;\n    }\n    if (channelMode & 4) {\n      skipCount += 2;\n    }\n  }\n\n  const lfeon =\n    (((data[start + 6] << 8) | data[start + 7]) >> (12 - skipCount)) & 1;\n\n  const channelsMap = [2, 1, 2, 3, 3, 4, 4, 5];\n  const channelCount = channelsMap[channelMode] + lfeon;\n\n  // build dac3 box\n  const bsid = data[start + 5] >> 3;\n  const bsmod = data[start + 5] & 7;\n\n  const config = new Uint8Array([\n    (samplingRateCode << 6) | (bsid << 1) | (bsmod >> 2),\n    ((bsmod & 3) << 6) |\n      (channelMode << 3) |\n      (lfeon << 2) |\n      (frameSizeCode >> 4),\n    (frameSizeCode << 4) & 0xe0,\n  ]);\n\n  const frameDuration = (1536 / sampleRate) * 90000;\n  const stamp = pts + frameIndex * frameDuration;\n  const unit = data.subarray(start, start + frameLength);\n\n  track.config = config;\n  track.channelCount = channelCount;\n  track.samplerate = sampleRate;\n  track.samples.push({ unit, pts: stamp });\n\n  return frameLength;\n}\n", "import type { ParsedVideoSample } from '../tsdemuxer';\nimport {\n  DemuxedVideoTrack,\n  VideoSample,\n  VideoSampleUnit,\n} from '../../types/demuxer';\nimport { logger } from '../../utils/logger';\n\nclass BaseVideoParser {\n  protected VideoSample: ParsedVideoSample | null = null;\n\n  protected createVideoSample(\n    key: boolean,\n    pts: number | undefined,\n    dts: number | undefined,\n    debug: string,\n  ): ParsedVideoSample {\n    return {\n      key,\n      frame: false,\n      pts,\n      dts,\n      units: [],\n      debug,\n      length: 0,\n    };\n  }\n\n  protected getLastNalUnit(\n    samples: VideoSample[],\n  ): VideoSampleUnit | undefined {\n    let VideoSample = this.VideoSample;\n    let lastUnit: VideoSampleUnit | undefined;\n    // try to fallback to previous sample if current one is empty\n    if (!VideoSample || VideoSample.units.length === 0) {\n      VideoSample = samples[samples.length - 1];\n    }\n    if (VideoSample?.units) {\n      const units = VideoSample.units;\n      lastUnit = units[units.length - 1];\n    }\n    return lastUnit;\n  }\n\n  protected pushAccessUnit(\n    VideoSample: ParsedVideoSample,\n    videoTrack: DemuxedVideoTrack,\n  ) {\n    if (VideoSample.units.length && VideoSample.frame) {\n      // if sample does not have PTS/DTS, patch with last sample PTS/DTS\n      if (VideoSample.pts === undefined) {\n        const samples = videoTrack.samples;\n        const nbSamples = samples.length;\n        if (nbSamples) {\n          const lastSample = samples[nbSamples - 1];\n          VideoSample.pts = lastSample.pts;\n          VideoSample.dts = lastSample.dts;\n        } else {\n          // dropping samples, no timestamp found\n          videoTrack.dropped++;\n          return;\n        }\n      }\n      videoTrack.samples.push(VideoSample as VideoSample);\n    }\n    if (VideoSample.debug.length) {\n      logger.log(\n        VideoSample.pts + '/' + VideoSample.dts + ':' + VideoSample.debug,\n      );\n    }\n  }\n}\n\nexport default BaseVideoParser;\n", "/**\n * Parser for exponential Golomb codes, a variable-bitwidth number encoding scheme used by h264.\n */\n\nimport { logger } from '../../utils/logger';\n\nclass ExpGolomb {\n  private data: Uint8Array;\n  public bytesAvailable: number;\n  private word: number;\n  private bitsAvailable: number;\n\n  constructor(data: Uint8Array) {\n    this.data = data;\n    // the number of bytes left to examine in this.data\n    this.bytesAvailable = data.byteLength;\n    // the current word being examined\n    this.word = 0; // :uint\n    // the number of bits left to examine in the current word\n    this.bitsAvailable = 0; // :uint\n  }\n\n  // ():void\n  loadWord(): void {\n    const data = this.data;\n    const bytesAvailable = this.bytesAvailable;\n    const position = data.byteLength - bytesAvailable;\n    const workingBytes = new Uint8Array(4);\n    const availableBytes = Math.min(4, bytesAvailable);\n    if (availableBytes === 0) {\n      throw new Error('no bytes available');\n    }\n\n    workingBytes.set(data.subarray(position, position + availableBytes));\n    this.word = new DataView(workingBytes.buffer).getUint32(0);\n    // track the amount of this.data that has been processed\n    this.bitsAvailable = availableBytes * 8;\n    this.bytesAvailable -= availableBytes;\n  }\n\n  // (count:int):void\n  skipBits(count: number): void {\n    let skipBytes; // :int\n    count = Math.min(count, this.bytesAvailable * 8 + this.bitsAvailable);\n    if (this.bitsAvailable > count) {\n      this.word <<= count;\n      this.bitsAvailable -= count;\n    } else {\n      count -= this.bitsAvailable;\n      skipBytes = count >> 3;\n      count -= skipBytes << 3;\n      this.bytesAvailable -= skipBytes;\n      this.loadWord();\n      this.word <<= count;\n      this.bitsAvailable -= count;\n    }\n  }\n\n  // (size:int):uint\n  readBits(size: number): number {\n    let bits = Math.min(this.bitsAvailable, size); // :uint\n    const valu = this.word >>> (32 - bits); // :uint\n    if (size > 32) {\n      logger.error('Cannot read more than 32 bits at a time');\n    }\n\n    this.bitsAvailable -= bits;\n    if (this.bitsAvailable > 0) {\n      this.word <<= bits;\n    } else if (this.bytesAvailable > 0) {\n      this.loadWord();\n    } else {\n      throw new Error('no bits available');\n    }\n\n    bits = size - bits;\n    if (bits > 0 && this.bitsAvailable) {\n      return (valu << bits) | this.readBits(bits);\n    } else {\n      return valu;\n    }\n  }\n\n  // ():uint\n  skipLZ(): number {\n    let leadingZeroCount; // :uint\n    for (\n      leadingZeroCount = 0;\n      leadingZeroCount < this.bitsAvailable;\n      ++leadingZeroCount\n    ) {\n      if ((this.word & (0x80000000 >>> leadingZeroCount)) !== 0) {\n        // the first bit of working word is 1\n        this.word <<= leadingZeroCount;\n        this.bitsAvailable -= leadingZeroCount;\n        return leadingZeroCount;\n      }\n    }\n    // we exhausted word and still have not found a 1\n    this.loadWord();\n    return leadingZeroCount + this.skipLZ();\n  }\n\n  // ():void\n  skipUEG(): void {\n    this.skipBits(1 + this.skipLZ());\n  }\n\n  // ():void\n  skipEG(): void {\n    this.skipBits(1 + this.skipLZ());\n  }\n\n  // ():uint\n  readUEG(): number {\n    const clz = this.skipLZ(); // :uint\n    return this.readBits(clz + 1) - 1;\n  }\n\n  // ():int\n  readEG(): number {\n    const valu = this.readUEG(); // :int\n    if (0x01 & valu) {\n      // the number is odd if the low order bit is set\n      return (1 + valu) >>> 1; // add 1 to make it even, and divide by 2\n    } else {\n      return -1 * (valu >>> 1); // divide by two then make it negative\n    }\n  }\n\n  // Some convenience functions\n  // :Boolean\n  readBoolean(): boolean {\n    return this.readBits(1) === 1;\n  }\n\n  // ():int\n  readUByte(): number {\n    return this.readBits(8);\n  }\n\n  // ():int\n  readUShort(): number {\n    return this.readBits(16);\n  }\n\n  // ():int\n  readUInt(): number {\n    return this.readBits(32);\n  }\n\n  /**\n   * Advance the ExpGolomb decoder past a scaling list. The scaling\n   * list is optionally transmitted as part of a sequence parameter\n   * set and is not relevant to transmuxing.\n   * @param count the number of entries in this scaling list\n   * @see Recommendation ITU-T H.264, Section 7.3.2.1.1.1\n   */\n  skipScalingList(count: number): void {\n    let lastScale = 8;\n    let nextScale = 8;\n    let deltaScale;\n    for (let j = 0; j < count; j++) {\n      if (nextScale !== 0) {\n        deltaScale = this.readEG();\n        nextScale = (lastScale + deltaScale + 256) % 256;\n      }\n      lastScale = nextScale === 0 ? lastScale : nextScale;\n    }\n  }\n\n  /**\n   * Read a sequence parameter set and return some interesting video\n   * properties. A sequence parameter set is the H264 metadata that\n   * describes the properties of upcoming video frames.\n   * @returns an object with configuration parsed from the\n   * sequence parameter set, including the dimensions of the\n   * associated video frames.\n   */\n  readSPS(): {\n    width: number;\n    height: number;\n    pixelRatio: [number, number];\n  } {\n    let frameCropLeftOffset = 0;\n    let frameCropRightOffset = 0;\n    let frameCropTopOffset = 0;\n    let frameCropBottomOffset = 0;\n    let numRefFramesInPicOrderCntCycle;\n    let scalingListCount;\n    let i;\n    const readUByte = this.readUByte.bind(this);\n    const readBits = this.readBits.bind(this);\n    const readUEG = this.readUEG.bind(this);\n    const readBoolean = this.readBoolean.bind(this);\n    const skipBits = this.skipBits.bind(this);\n    const skipEG = this.skipEG.bind(this);\n    const skipUEG = this.skipUEG.bind(this);\n    const skipScalingList = this.skipScalingList.bind(this);\n\n    readUByte();\n    const profileIdc = readUByte(); // profile_idc\n    readBits(5); // profileCompat constraint_set[0-4]_flag, u(5)\n    skipBits(3); // reserved_zero_3bits u(3),\n    readUByte(); // level_idc u(8)\n    skipUEG(); // seq_parameter_set_id\n    // some profiles have more optional data we don't need\n    if (\n      profileIdc === 100 ||\n      profileIdc === 110 ||\n      profileIdc === 122 ||\n      profileIdc === 244 ||\n      profileIdc === 44 ||\n      profileIdc === 83 ||\n      profileIdc === 86 ||\n      profileIdc === 118 ||\n      profileIdc === 128\n    ) {\n      const chromaFormatIdc = readUEG();\n      if (chromaFormatIdc === 3) {\n        skipBits(1);\n      } // separate_colour_plane_flag\n\n      skipUEG(); // bit_depth_luma_minus8\n      skipUEG(); // bit_depth_chroma_minus8\n      skipBits(1); // qpprime_y_zero_transform_bypass_flag\n      if (readBoolean()) {\n        // seq_scaling_matrix_present_flag\n        scalingListCount = chromaFormatIdc !== 3 ? 8 : 12;\n        for (i = 0; i < scalingListCount; i++) {\n          if (readBoolean()) {\n            // seq_scaling_list_present_flag[ i ]\n            if (i < 6) {\n              skipScalingList(16);\n            } else {\n              skipScalingList(64);\n            }\n          }\n        }\n      }\n    }\n    skipUEG(); // log2_max_frame_num_minus4\n    const picOrderCntType = readUEG();\n    if (picOrderCntType === 0) {\n      readUEG(); // log2_max_pic_order_cnt_lsb_minus4\n    } else if (picOrderCntType === 1) {\n      skipBits(1); // delta_pic_order_always_zero_flag\n      skipEG(); // offset_for_non_ref_pic\n      skipEG(); // offset_for_top_to_bottom_field\n      numRefFramesInPicOrderCntCycle = readUEG();\n      for (i = 0; i < numRefFramesInPicOrderCntCycle; i++) {\n        skipEG();\n      } // offset_for_ref_frame[ i ]\n    }\n    skipUEG(); // max_num_ref_frames\n    skipBits(1); // gaps_in_frame_num_value_allowed_flag\n    const picWidthInMbsMinus1 = readUEG();\n    const picHeightInMapUnitsMinus1 = readUEG();\n    const frameMbsOnlyFlag = readBits(1);\n    if (frameMbsOnlyFlag === 0) {\n      skipBits(1);\n    } // mb_adaptive_frame_field_flag\n\n    skipBits(1); // direct_8x8_inference_flag\n    if (readBoolean()) {\n      // frame_cropping_flag\n      frameCropLeftOffset = readUEG();\n      frameCropRightOffset = readUEG();\n      frameCropTopOffset = readUEG();\n      frameCropBottomOffset = readUEG();\n    }\n    let pixelRatio: [number, number] = [1, 1];\n    if (readBoolean()) {\n      // vui_parameters_present_flag\n      if (readBoolean()) {\n        // aspect_ratio_info_present_flag\n        const aspectRatioIdc = readUByte();\n        switch (aspectRatioIdc) {\n          case 1:\n            pixelRatio = [1, 1];\n            break;\n          case 2:\n            pixelRatio = [12, 11];\n            break;\n          case 3:\n            pixelRatio = [10, 11];\n            break;\n          case 4:\n            pixelRatio = [16, 11];\n            break;\n          case 5:\n            pixelRatio = [40, 33];\n            break;\n          case 6:\n            pixelRatio = [24, 11];\n            break;\n          case 7:\n            pixelRatio = [20, 11];\n            break;\n          case 8:\n            pixelRatio = [32, 11];\n            break;\n          case 9:\n            pixelRatio = [80, 33];\n            break;\n          case 10:\n            pixelRatio = [18, 11];\n            break;\n          case 11:\n            pixelRatio = [15, 11];\n            break;\n          case 12:\n            pixelRatio = [64, 33];\n            break;\n          case 13:\n            pixelRatio = [160, 99];\n            break;\n          case 14:\n            pixelRatio = [4, 3];\n            break;\n          case 15:\n            pixelRatio = [3, 2];\n            break;\n          case 16:\n            pixelRatio = [2, 1];\n            break;\n          case 255: {\n            pixelRatio = [\n              (readUByte() << 8) | readUByte(),\n              (readUByte() << 8) | readUByte(),\n            ];\n            break;\n          }\n        }\n      }\n    }\n    return {\n      width: Math.ceil(\n        (picWidthInMbsMinus1 + 1) * 16 -\n          frameCropLeftOffset * 2 -\n          frameCropRightOffset * 2,\n      ),\n      height:\n        (2 - frameMbsOnlyFlag) * (picHeightInMapUnitsMinus1 + 1) * 16 -\n        (frameMbsOnlyFlag ? 2 : 4) *\n          (frameCropTopOffset + frameCropBottomOffset),\n      pixelRatio: pixelRatio,\n    };\n  }\n\n  readSliceType() {\n    // skip NALu type\n    this.readUByte();\n    // discard first_mb_in_slice\n    this.readUEG();\n    // return slice_type\n    return this.readUEG();\n  }\n}\n\nexport default ExpGolomb;\n", "import BaseVideoParser from './base-video-parser';\nimport {\n  DemuxedVideoTrack,\n  DemuxedUserdataTrack,\n  VideoSampleUnit,\n} from '../../types/demuxer';\nimport {\n  appendUint8Array,\n  parseSEIMessageFromNALu,\n} from '../../utils/mp4-tools';\nimport ExpGolomb from './exp-golomb';\nimport type { PES } from '../tsdemuxer';\n\nclass AvcVideoParser extends BaseVideoParser {\n  public parseAVCPES(\n    track: DemuxedVideoTrack,\n    textTrack: DemuxedUserdataTrack,\n    pes: PES,\n    last: boolean,\n    duration: number,\n  ) {\n    const units = this.parseAVCNALu(track, pes.data);\n    const debug = false;\n    let VideoSample = this.VideoSample;\n    let push: boolean;\n    let spsfound = false;\n    // free pes.data to save up some memory\n    (pes as any).data = null;\n\n    // if new NAL units found and last sample still there, let's push ...\n    // this helps parsing streams with missing AUD (only do this if AUD never found)\n    if (VideoSample && units.length && !track.audFound) {\n      this.pushAccessUnit(VideoSample, track);\n      VideoSample = this.VideoSample = this.createVideoSample(\n        false,\n        pes.pts,\n        pes.dts,\n        '',\n      );\n    }\n\n    units.forEach((unit) => {\n      switch (unit.type) {\n        // NDR\n        case 1: {\n          let iskey = false;\n          push = true;\n          const data = unit.data;\n          // only check slice type to detect KF in case SPS found in same packet (any keyframe is preceded by SPS ...)\n          if (spsfound && data.length > 4) {\n            // retrieve slice type by parsing beginning of NAL unit (follow H264 spec, slice_header definition) to detect keyframe embedded in NDR\n            const sliceType = new ExpGolomb(data).readSliceType();\n            // 2 : I slice, 4 : SI slice, 7 : I slice, 9: SI slice\n            // SI slice : A slice that is coded using intra prediction only and using quantisation of the prediction samples.\n            // An SI slice can be coded such that its decoded samples can be constructed identically to an SP slice.\n            // I slice: A slice that is not an SI slice that is decoded using intra prediction only.\n            // if (sliceType === 2 || sliceType === 7) {\n            if (\n              sliceType === 2 ||\n              sliceType === 4 ||\n              sliceType === 7 ||\n              sliceType === 9\n            ) {\n              iskey = true;\n            }\n          }\n\n          if (iskey) {\n            // if we have non-keyframe data already, that cannot belong to the same frame as a keyframe, so force a push\n            if (VideoSample?.frame && !VideoSample.key) {\n              this.pushAccessUnit(VideoSample, track);\n              VideoSample = this.VideoSample = null;\n            }\n          }\n\n          if (!VideoSample) {\n            VideoSample = this.VideoSample = this.createVideoSample(\n              true,\n              pes.pts,\n              pes.dts,\n              '',\n            );\n          }\n\n          if (debug) {\n            VideoSample.debug += 'NDR ';\n          }\n\n          VideoSample.frame = true;\n          VideoSample.key = iskey;\n\n          break;\n          // IDR\n        }\n        case 5:\n          push = true;\n          // handle PES not starting with AUD\n          // if we have frame data already, that cannot belong to the same frame, so force a push\n          if (VideoSample?.frame && !VideoSample.key) {\n            this.pushAccessUnit(VideoSample, track);\n            VideoSample = this.VideoSample = null;\n          }\n          if (!VideoSample) {\n            VideoSample = this.VideoSample = this.createVideoSample(\n              true,\n              pes.pts,\n              pes.dts,\n              '',\n            );\n          }\n\n          if (debug) {\n            VideoSample.debug += 'IDR ';\n          }\n\n          VideoSample.key = true;\n          VideoSample.frame = true;\n          break;\n        // SEI\n        case 6: {\n          push = true;\n          if (debug && VideoSample) {\n            VideoSample.debug += 'SEI ';\n          }\n          parseSEIMessageFromNALu(\n            unit.data,\n            1,\n            pes.pts as number,\n            textTrack.samples,\n          );\n          break;\n          // SPS\n        }\n        case 7: {\n          push = true;\n          spsfound = true;\n          if (debug && VideoSample) {\n            VideoSample.debug += 'SPS ';\n          }\n          const sps = unit.data;\n          const expGolombDecoder = new ExpGolomb(sps);\n          const config = expGolombDecoder.readSPS();\n\n          if (\n            !track.sps ||\n            track.width !== config.width ||\n            track.height !== config.height ||\n            track.pixelRatio?.[0] !== config.pixelRatio[0] ||\n            track.pixelRatio?.[1] !== config.pixelRatio[1]\n          ) {\n            track.width = config.width;\n            track.height = config.height;\n            track.pixelRatio = config.pixelRatio;\n            track.sps = [sps];\n            track.duration = duration;\n            const codecarray = sps.subarray(1, 4);\n            let codecstring = 'avc1.';\n            for (let i = 0; i < 3; i++) {\n              let h = codecarray[i].toString(16);\n              if (h.length < 2) {\n                h = '0' + h;\n              }\n\n              codecstring += h;\n            }\n            track.codec = codecstring;\n          }\n\n          break;\n        }\n        // PPS\n        case 8:\n          push = true;\n          if (debug && VideoSample) {\n            VideoSample.debug += 'PPS ';\n          }\n\n          track.pps = [unit.data];\n\n          break;\n        // AUD\n        case 9:\n          push = true;\n          track.audFound = true;\n          if (VideoSample) {\n            this.pushAccessUnit(VideoSample, track);\n          }\n\n          VideoSample = this.VideoSample = this.createVideoSample(\n            false,\n            pes.pts,\n            pes.dts,\n            debug ? 'AUD ' : '',\n          );\n          break;\n        // Filler Data\n        case 12:\n          push = true;\n          break;\n        default:\n          push = false;\n          if (VideoSample) {\n            VideoSample.debug += 'unknown NAL ' + unit.type + ' ';\n          }\n\n          break;\n      }\n      if (VideoSample && push) {\n        const units = VideoSample.units;\n        units.push(unit);\n      }\n    });\n    // if last PES packet, push samples\n    if (last && VideoSample) {\n      this.pushAccessUnit(VideoSample, track);\n      this.VideoSample = null;\n    }\n  }\n\n  private parseAVCNALu(\n    track: DemuxedVideoTrack,\n    array: Uint8Array,\n  ): Array<{\n    data: Uint8Array;\n    type: number;\n    state?: number;\n  }> {\n    const len = array.byteLength;\n    let state = track.naluState || 0;\n    const lastState = state;\n    const units: VideoSampleUnit[] = [];\n    let i = 0;\n    let value: number;\n    let overflow: number;\n    let unitType: number;\n    let lastUnitStart = -1;\n    let lastUnitType: number = 0;\n    // logger.log('PES:' + Hex.hexDump(array));\n\n    if (state === -1) {\n      // special use case where we found 3 or 4-byte start codes exactly at the end of previous PES packet\n      lastUnitStart = 0;\n      // NALu type is value read from offset 0\n      lastUnitType = array[0] & 0x1f;\n      state = 0;\n      i = 1;\n    }\n\n    while (i < len) {\n      value = array[i++];\n      // optimization. state 0 and 1 are the predominant case. let's handle them outside of the switch/case\n      if (!state) {\n        state = value ? 0 : 1;\n        continue;\n      }\n      if (state === 1) {\n        state = value ? 0 : 2;\n        continue;\n      }\n      // here we have state either equal to 2 or 3\n      if (!value) {\n        state = 3;\n      } else if (value === 1) {\n        overflow = i - state - 1;\n        if (lastUnitStart >= 0) {\n          const unit: VideoSampleUnit = {\n            data: array.subarray(lastUnitStart, overflow),\n            type: lastUnitType,\n          };\n          // logger.log('pushing NALU, type/size:' + unit.type + '/' + unit.data.byteLength);\n          units.push(unit);\n        } else {\n          // lastUnitStart is undefined => this is the first start code found in this PES packet\n          // first check if start code delimiter is overlapping between 2 PES packets,\n          // ie it started in last packet (lastState not zero)\n          // and ended at the beginning of this PES packet (i <= 4 - lastState)\n          const lastUnit = this.getLastNalUnit(track.samples);\n          if (lastUnit) {\n            if (lastState && i <= 4 - lastState) {\n              // start delimiter overlapping between PES packets\n              // strip start delimiter bytes from the end of last NAL unit\n              // check if lastUnit had a state different from zero\n              if (lastUnit.state) {\n                // strip last bytes\n                lastUnit.data = lastUnit.data.subarray(\n                  0,\n                  lastUnit.data.byteLength - lastState,\n                );\n              }\n            }\n            // If NAL units are not starting right at the beginning of the PES packet, push preceding data into previous NAL unit.\n\n            if (overflow > 0) {\n              // logger.log('first NALU found with overflow:' + overflow);\n              lastUnit.data = appendUint8Array(\n                lastUnit.data,\n                array.subarray(0, overflow),\n              );\n              lastUnit.state = 0;\n            }\n          }\n        }\n        // check if we can read unit type\n        if (i < len) {\n          unitType = array[i] & 0x1f;\n          // logger.log('find NALU @ offset:' + i + ',type:' + unitType);\n          lastUnitStart = i;\n          lastUnitType = unitType;\n          state = 0;\n        } else {\n          // not enough byte to read unit type. let's read it on next PES parsing\n          state = -1;\n        }\n      } else {\n        state = 0;\n      }\n    }\n    if (lastUnitStart >= 0 && state >= 0) {\n      const unit: VideoSampleUnit = {\n        data: array.subarray(lastUnitStart, len),\n        type: lastUnitType,\n        state: state,\n      };\n      units.push(unit);\n      // logger.log('pushing NALU, type/size/state:' + unit.type + '/' + unit.data.byteLength + '/' + state);\n    }\n    // no NALu found\n    if (units.length === 0) {\n      // append pes.data to previous NAL unit\n      const lastUnit = this.getLastNalUnit(track.samples);\n      if (lastUnit) {\n        lastUnit.data = appendUint8Array(lastUnit.data, array);\n      }\n    }\n    track.naluState = state;\n    return units;\n  }\n}\n\nexport default AvcVideoParser;\n", "/**\n * SAMPLE-AES decrypter\n */\n\nimport { HlsConfig } from '../config';\nimport Decrypter from '../crypt/decrypter';\nimport { HlsEventEmitter } from '../events';\nimport type {\n  AudioSample,\n  VideoSample,\n  VideoSampleUnit,\n  DemuxedVideoTrackBase,\n  KeyData,\n} from '../types/demuxer';\nimport { discardEPB } from '../utils/mp4-tools';\n\nclass SampleAesDecrypter {\n  private keyData: KeyData;\n  private decrypter: Decrypter;\n\n  constructor(observer: HlsEventEmitter, config: HlsConfig, keyData: KeyData) {\n    this.keyData = keyData;\n    this.decrypter = new Decrypter(config, {\n      removePKCS7Padding: false,\n    });\n  }\n\n  decryptBuffer(encryptedData: Uint8Array | ArrayBuffer): Promise<ArrayBuffer> {\n    return this.decrypter.decrypt(\n      encryptedData,\n      this.keyData.key.buffer,\n      this.keyData.iv.buffer,\n    );\n  }\n\n  // AAC - encrypt all full 16 bytes blocks starting from offset 16\n  private decryptAacSample(\n    samples: AudioSample[],\n    sampleIndex: number,\n    callback: () => void,\n  ) {\n    const curUnit = samples[sampleIndex].unit;\n    if (curUnit.length <= 16) {\n      // No encrypted portion in this sample (first 16 bytes is not\n      // encrypted, see https://developer.apple.com/library/archive/documentation/AudioVideo/Conceptual/HLS_Sample_Encryption/Encryption/Encryption.html),\n      return;\n    }\n    const encryptedData = curUnit.subarray(\n      16,\n      curUnit.length - (curUnit.length % 16),\n    );\n    const encryptedBuffer = encryptedData.buffer.slice(\n      encryptedData.byteOffset,\n      encryptedData.byteOffset + encryptedData.length,\n    );\n\n    this.decryptBuffer(encryptedBuffer).then((decryptedBuffer: ArrayBuffer) => {\n      const decryptedData = new Uint8Array(decryptedBuffer);\n      curUnit.set(decryptedData, 16);\n\n      if (!this.decrypter.isSync()) {\n        this.decryptAacSamples(samples, sampleIndex + 1, callback);\n      }\n    });\n  }\n\n  decryptAacSamples(\n    samples: AudioSample[],\n    sampleIndex: number,\n    callback: () => void,\n  ) {\n    for (; ; sampleIndex++) {\n      if (sampleIndex >= samples.length) {\n        callback();\n        return;\n      }\n\n      if (samples[sampleIndex].unit.length < 32) {\n        continue;\n      }\n\n      this.decryptAacSample(samples, sampleIndex, callback);\n\n      if (!this.decrypter.isSync()) {\n        return;\n      }\n    }\n  }\n\n  // AVC - encrypt one 16 bytes block out of ten, starting from offset 32\n  getAvcEncryptedData(decodedData: Uint8Array) {\n    const encryptedDataLen =\n      Math.floor((decodedData.length - 48) / 160) * 16 + 16;\n    const encryptedData = new Int8Array(encryptedDataLen);\n    let outputPos = 0;\n    for (\n      let inputPos = 32;\n      inputPos < decodedData.length - 16;\n      inputPos += 160, outputPos += 16\n    ) {\n      encryptedData.set(\n        decodedData.subarray(inputPos, inputPos + 16),\n        outputPos,\n      );\n    }\n\n    return encryptedData;\n  }\n\n  getAvcDecryptedUnit(\n    decodedData: Uint8Array,\n    decryptedData: ArrayLike<number> | ArrayBuffer | SharedArrayBuffer,\n  ) {\n    const uint8DecryptedData = new Uint8Array(decryptedData);\n    let inputPos = 0;\n    for (\n      let outputPos = 32;\n      outputPos < decodedData.length - 16;\n      outputPos += 160, inputPos += 16\n    ) {\n      decodedData.set(\n        uint8DecryptedData.subarray(inputPos, inputPos + 16),\n        outputPos,\n      );\n    }\n\n    return decodedData;\n  }\n\n  decryptAvcSample(\n    samples: VideoSample[],\n    sampleIndex: number,\n    unitIndex: number,\n    callback: () => void,\n    curUnit: VideoSampleUnit,\n  ) {\n    const decodedData = discardEPB(curUnit.data);\n    const encryptedData = this.getAvcEncryptedData(decodedData);\n\n    this.decryptBuffer(encryptedData.buffer).then(\n      (decryptedBuffer: ArrayBuffer) => {\n        curUnit.data = this.getAvcDecryptedUnit(decodedData, decryptedBuffer);\n\n        if (!this.decrypter.isSync()) {\n          this.decryptAvcSamples(samples, sampleIndex, unitIndex + 1, callback);\n        }\n      },\n    );\n  }\n\n  decryptAvcSamples(\n    samples: DemuxedVideoTrackBase['samples'],\n    sampleIndex: number,\n    unitIndex: number,\n    callback: () => void,\n  ) {\n    if (samples instanceof Uint8Array) {\n      throw new Error('Cannot decrypt samples of type Uint8Array');\n    }\n\n    for (; ; sampleIndex++, unitIndex = 0) {\n      if (sampleIndex >= samples.length) {\n        callback();\n        return;\n      }\n\n      const curUnits = samples[sampleIndex].units;\n      for (; ; unitIndex++) {\n        if (unitIndex >= curUnits.length) {\n          break;\n        }\n\n        const curUnit = curUnits[unitIndex];\n        if (\n          curUnit.data.length <= 48 ||\n          (curUnit.type !== 1 && curUnit.type !== 5)\n        ) {\n          continue;\n        }\n\n        this.decryptAvcSample(\n          samples,\n          sampleIndex,\n          unitIndex,\n          callback,\n          curUnit,\n        );\n\n        if (!this.decrypter.isSync()) {\n          return;\n        }\n      }\n    }\n  }\n}\n\nexport default SampleAesDecrypter;\n", "/**\n * highly optimized TS demuxer:\n * parse PAT, PMT\n * extract PES packet from audio and video PIDs\n * extract AVC/H264 NAL units and AAC/ADTS samples from PES packet\n * trigger the remuxer upon parsing completion\n * it also tries to workaround as best as it can audio codec switch (HE-AAC to AAC and vice versa), without having to restart the MediaSource.\n * it also controls the remuxing process :\n * upon discontinuity or level switch detection, it will also notifies the remuxer so that it can reset its state.\n */\n\nimport * as ADTS from './audio/adts';\nimport * as MpegAudio from './audio/mpegaudio';\nimport * as AC3 from './audio/ac3-demuxer';\nimport AvcVideoParser from './video/avc-video-parser';\nimport SampleAesDecrypter from './sample-aes';\nimport { Events } from '../events';\nimport { appendUint8Array, RemuxerTrackIdConfig } from '../utils/mp4-tools';\nimport { logger } from '../utils/logger';\nimport { ErrorTypes, ErrorDetails } from '../errors';\nimport type { HlsConfig } from '../config';\nimport type { HlsEventEmitter } from '../events';\nimport {\n  DemuxedVideoTrack,\n  DemuxedAudioTrack,\n  DemuxedTrack,\n  Demuxer,\n  DemuxerResult,\n  VideoSample,\n  DemuxedMetadataTrack,\n  DemuxedUserdataTrack,\n  ElementaryStreamData,\n  KeyData,\n  MetadataSchema,\n} from '../types/demuxer';\nimport { AudioFrame } from '../types/demuxer';\n\nexport type ParsedTimestamp = {\n  pts?: number;\n  dts?: number;\n};\n\nexport type PES = ParsedTimestamp & {\n  data: Uint8Array;\n  len: number;\n};\n\nexport type ParsedVideoSample = ParsedTimestamp &\n  Omit<VideoSample, 'pts' | 'dts'>;\n\nexport interface TypeSupported {\n  mpeg: boolean;\n  mp3: boolean;\n  ac3: boolean;\n}\n\nconst PACKET_LENGTH = 188;\n\nclass TSDemuxer implements Demuxer {\n  private readonly observer: HlsEventEmitter;\n  private readonly config: HlsConfig;\n  private typeSupported: TypeSupported;\n\n  private sampleAes: SampleAesDecrypter | null = null;\n  private pmtParsed: boolean = false;\n  private audioCodec?: string;\n  private videoCodec?: string;\n  private _duration: number = 0;\n  private _pmtId: number = -1;\n\n  private _videoTrack?: DemuxedVideoTrack;\n  private _audioTrack?: DemuxedAudioTrack;\n  private _id3Track?: DemuxedMetadataTrack;\n  private _txtTrack?: DemuxedUserdataTrack;\n  private aacOverFlow: AudioFrame | null = null;\n  private remainderData: Uint8Array | null = null;\n  private videoParser: AvcVideoParser;\n\n  constructor(\n    observer: HlsEventEmitter,\n    config: HlsConfig,\n    typeSupported: TypeSupported,\n  ) {\n    this.observer = observer;\n    this.config = config;\n    this.typeSupported = typeSupported;\n    this.videoParser = new AvcVideoParser();\n  }\n\n  static probe(data: Uint8Array) {\n    const syncOffset = TSDemuxer.syncOffset(data);\n    if (syncOffset > 0) {\n      logger.warn(\n        `MPEG2-TS detected but first sync word found @ offset ${syncOffset}`,\n      );\n    }\n    return syncOffset !== -1;\n  }\n\n  static syncOffset(data: Uint8Array): number {\n    const length = data.length;\n    let scanwindow = Math.min(PACKET_LENGTH * 5, length - PACKET_LENGTH) + 1;\n    let i = 0;\n    while (i < scanwindow) {\n      // a TS init segment should contain at least 2 TS packets: PAT and PMT, each starting with 0x47\n      let foundPat = false;\n      let packetStart = -1;\n      let tsPackets = 0;\n      for (let j = i; j < length; j += PACKET_LENGTH) {\n        if (\n          data[j] === 0x47 &&\n          (length - j === PACKET_LENGTH || data[j + PACKET_LENGTH] === 0x47)\n        ) {\n          tsPackets++;\n          if (packetStart === -1) {\n            packetStart = j;\n            // First sync word found at offset, increase scan length (#5251)\n            if (packetStart !== 0) {\n              scanwindow =\n                Math.min(\n                  packetStart + PACKET_LENGTH * 99,\n                  data.length - PACKET_LENGTH,\n                ) + 1;\n            }\n          }\n          if (!foundPat) {\n            foundPat = parsePID(data, j) === 0;\n          }\n          // Sync word found at 0 with 3 packets, or found at offset least 2 packets up to scanwindow (#5501)\n          if (\n            foundPat &&\n            tsPackets > 1 &&\n            ((packetStart === 0 && tsPackets > 2) ||\n              j + PACKET_LENGTH > scanwindow)\n          ) {\n            return packetStart;\n          }\n        } else if (tsPackets) {\n          // Exit if sync word found, but does not contain contiguous packets\n          return -1;\n        } else {\n          break;\n        }\n      }\n      i++;\n    }\n    return -1;\n  }\n\n  /**\n   * Creates a track model internal to demuxer used to drive remuxing input\n   */\n  static createTrack(\n    type: 'audio' | 'video' | 'id3' | 'text',\n    duration?: number,\n  ): DemuxedTrack {\n    return {\n      container:\n        type === 'video' || type === 'audio' ? 'video/mp2t' : undefined,\n      type,\n      id: RemuxerTrackIdConfig[type],\n      pid: -1,\n      inputTimeScale: 90000,\n      sequenceNumber: 0,\n      samples: [],\n      dropped: 0,\n      duration: type === 'audio' ? duration : undefined,\n    };\n  }\n\n  /**\n   * Initializes a new init segment on the demuxer/remuxer interface. Needed for discontinuities/track-switches (or at stream start)\n   * Resets all internal track instances of the demuxer.\n   */\n  public resetInitSegment(\n    initSegment: Uint8Array | undefined,\n    audioCodec: string,\n    videoCodec: string,\n    trackDuration: number,\n  ) {\n    this.pmtParsed = false;\n    this._pmtId = -1;\n\n    this._videoTrack = TSDemuxer.createTrack('video') as DemuxedVideoTrack;\n    this._audioTrack = TSDemuxer.createTrack(\n      'audio',\n      trackDuration,\n    ) as DemuxedAudioTrack;\n    this._id3Track = TSDemuxer.createTrack('id3') as DemuxedMetadataTrack;\n    this._txtTrack = TSDemuxer.createTrack('text') as DemuxedUserdataTrack;\n    this._audioTrack.segmentCodec = 'aac';\n\n    // flush any partial content\n    this.aacOverFlow = null;\n    this.remainderData = null;\n    this.audioCodec = audioCodec;\n    this.videoCodec = videoCodec;\n    this._duration = trackDuration;\n  }\n\n  public resetTimeStamp() {}\n\n  public resetContiguity(): void {\n    const { _audioTrack, _videoTrack, _id3Track } = this;\n    if (_audioTrack) {\n      _audioTrack.pesData = null;\n    }\n    if (_videoTrack) {\n      _videoTrack.pesData = null;\n    }\n    if (_id3Track) {\n      _id3Track.pesData = null;\n    }\n    this.aacOverFlow = null;\n    this.remainderData = null;\n  }\n\n  public demux(\n    data: Uint8Array,\n    timeOffset: number,\n    isSampleAes = false,\n    flush = false,\n  ): DemuxerResult {\n    if (!isSampleAes) {\n      this.sampleAes = null;\n    }\n\n    let pes: PES | null;\n\n    const videoTrack = this._videoTrack as DemuxedVideoTrack;\n    const audioTrack = this._audioTrack as DemuxedAudioTrack;\n    const id3Track = this._id3Track as DemuxedMetadataTrack;\n    const textTrack = this._txtTrack as DemuxedUserdataTrack;\n\n    let videoPid = videoTrack.pid;\n    let videoData = videoTrack.pesData;\n    let audioPid = audioTrack.pid;\n    let id3Pid = id3Track.pid;\n    let audioData = audioTrack.pesData;\n    let id3Data = id3Track.pesData;\n    let unknownPID: number | null = null;\n    let pmtParsed = this.pmtParsed;\n    let pmtId = this._pmtId;\n\n    let len = data.length;\n    if (this.remainderData) {\n      data = appendUint8Array(this.remainderData, data);\n      len = data.length;\n      this.remainderData = null;\n    }\n\n    if (len < PACKET_LENGTH && !flush) {\n      this.remainderData = data;\n      return {\n        audioTrack,\n        videoTrack,\n        id3Track,\n        textTrack,\n      };\n    }\n\n    const syncOffset = Math.max(0, TSDemuxer.syncOffset(data));\n    len -= (len - syncOffset) % PACKET_LENGTH;\n    if (len < data.byteLength && !flush) {\n      this.remainderData = new Uint8Array(\n        data.buffer,\n        len,\n        data.buffer.byteLength - len,\n      );\n    }\n\n    // loop through TS packets\n    let tsPacketErrors = 0;\n    for (let start = syncOffset; start < len; start += PACKET_LENGTH) {\n      if (data[start] === 0x47) {\n        const stt = !!(data[start + 1] & 0x40);\n        const pid = parsePID(data, start);\n        const atf = (data[start + 3] & 0x30) >> 4;\n\n        // if an adaption field is present, its length is specified by the fifth byte of the TS packet header.\n        let offset: number;\n        if (atf > 1) {\n          offset = start + 5 + data[start + 4];\n          // continue if there is only adaptation field\n          if (offset === start + PACKET_LENGTH) {\n            continue;\n          }\n        } else {\n          offset = start + 4;\n        }\n        switch (pid) {\n          case videoPid:\n            if (stt) {\n              if (videoData && (pes = parsePES(videoData))) {\n                this.videoParser.parseAVCPES(\n                  videoTrack,\n                  textTrack,\n                  pes,\n                  false,\n                  this._duration,\n                );\n              }\n\n              videoData = { data: [], size: 0 };\n            }\n            if (videoData) {\n              videoData.data.push(data.subarray(offset, start + PACKET_LENGTH));\n              videoData.size += start + PACKET_LENGTH - offset;\n            }\n            break;\n          case audioPid:\n            if (stt) {\n              if (audioData && (pes = parsePES(audioData))) {\n                switch (audioTrack.segmentCodec) {\n                  case 'aac':\n                    this.parseAACPES(audioTrack, pes);\n                    break;\n                  case 'mp3':\n                    this.parseMPEGPES(audioTrack, pes);\n                    break;\n                  case 'ac3':\n                    if (__USE_M2TS_ADVANCED_CODECS__) {\n                      this.parseAC3PES(audioTrack, pes);\n                    }\n                    break;\n                }\n              }\n              audioData = { data: [], size: 0 };\n            }\n            if (audioData) {\n              audioData.data.push(data.subarray(offset, start + PACKET_LENGTH));\n              audioData.size += start + PACKET_LENGTH - offset;\n            }\n            break;\n          case id3Pid:\n            if (stt) {\n              if (id3Data && (pes = parsePES(id3Data))) {\n                this.parseID3PES(id3Track, pes);\n              }\n\n              id3Data = { data: [], size: 0 };\n            }\n            if (id3Data) {\n              id3Data.data.push(data.subarray(offset, start + PACKET_LENGTH));\n              id3Data.size += start + PACKET_LENGTH - offset;\n            }\n            break;\n          case 0:\n            if (stt) {\n              offset += data[offset] + 1;\n            }\n\n            pmtId = this._pmtId = parsePAT(data, offset);\n            // logger.log('PMT PID:'  + this._pmtId);\n            break;\n          case pmtId: {\n            if (stt) {\n              offset += data[offset] + 1;\n            }\n\n            const parsedPIDs = parsePMT(\n              data,\n              offset,\n              this.typeSupported,\n              isSampleAes,\n              this.observer,\n            );\n\n            // only update track id if track PID found while parsing PMT\n            // this is to avoid resetting the PID to -1 in case\n            // track PID transiently disappears from the stream\n            // this could happen in case of transient missing audio samples for example\n            // NOTE this is only the PID of the track as found in TS,\n            // but we are not using this for MP4 track IDs.\n            videoPid = parsedPIDs.videoPid;\n            if (videoPid > 0) {\n              videoTrack.pid = videoPid;\n              videoTrack.segmentCodec = parsedPIDs.segmentVideoCodec;\n            }\n\n            audioPid = parsedPIDs.audioPid;\n            if (audioPid > 0) {\n              audioTrack.pid = audioPid;\n              audioTrack.segmentCodec = parsedPIDs.segmentAudioCodec;\n            }\n            id3Pid = parsedPIDs.id3Pid;\n            if (id3Pid > 0) {\n              id3Track.pid = id3Pid;\n            }\n\n            if (unknownPID !== null && !pmtParsed) {\n              logger.warn(\n                `MPEG-TS PMT found at ${start} after unknown PID '${unknownPID}'. Backtracking to sync byte @${syncOffset} to parse all TS packets.`,\n              );\n              unknownPID = null;\n              // we set it to -188, the += 188 in the for loop will reset start to 0\n              start = syncOffset - 188;\n            }\n            pmtParsed = this.pmtParsed = true;\n            break;\n          }\n          case 0x11:\n          case 0x1fff:\n            break;\n          default:\n            unknownPID = pid;\n            break;\n        }\n      } else {\n        tsPacketErrors++;\n      }\n    }\n\n    if (tsPacketErrors > 0) {\n      emitParsingError(\n        this.observer,\n        new Error(\n          `Found ${tsPacketErrors} TS packet/s that do not start with 0x47`,\n        ),\n      );\n    }\n\n    videoTrack.pesData = videoData;\n    audioTrack.pesData = audioData;\n    id3Track.pesData = id3Data;\n\n    const demuxResult: DemuxerResult = {\n      audioTrack,\n      videoTrack,\n      id3Track,\n      textTrack,\n    };\n\n    if (flush) {\n      this.extractRemainingSamples(demuxResult);\n    }\n\n    return demuxResult;\n  }\n\n  public flush(): DemuxerResult | Promise<DemuxerResult> {\n    const { remainderData } = this;\n    this.remainderData = null;\n    let result: DemuxerResult;\n    if (remainderData) {\n      result = this.demux(remainderData, -1, false, true);\n    } else {\n      result = {\n        videoTrack: this._videoTrack as DemuxedVideoTrack,\n        audioTrack: this._audioTrack as DemuxedAudioTrack,\n        id3Track: this._id3Track as DemuxedMetadataTrack,\n        textTrack: this._txtTrack as DemuxedUserdataTrack,\n      };\n    }\n    this.extractRemainingSamples(result);\n    if (this.sampleAes) {\n      return this.decrypt(result, this.sampleAes);\n    }\n    return result;\n  }\n\n  private extractRemainingSamples(demuxResult: DemuxerResult) {\n    const { audioTrack, videoTrack, id3Track, textTrack } = demuxResult;\n    const videoData = videoTrack.pesData;\n    const audioData = audioTrack.pesData;\n    const id3Data = id3Track.pesData;\n    // try to parse last PES packets\n    let pes: PES | null;\n    if (videoData && (pes = parsePES(videoData))) {\n      this.videoParser.parseAVCPES(\n        videoTrack as DemuxedVideoTrack,\n        textTrack as DemuxedUserdataTrack,\n        pes,\n        true,\n        this._duration,\n      );\n      videoTrack.pesData = null;\n    } else {\n      // either avcData null or PES truncated, keep it for next frag parsing\n      videoTrack.pesData = videoData;\n    }\n\n    if (audioData && (pes = parsePES(audioData))) {\n      switch (audioTrack.segmentCodec) {\n        case 'aac':\n          this.parseAACPES(audioTrack, pes);\n          break;\n        case 'mp3':\n          this.parseMPEGPES(audioTrack, pes);\n          break;\n        case 'ac3':\n          if (__USE_M2TS_ADVANCED_CODECS__) {\n            this.parseAC3PES(audioTrack, pes);\n          }\n          break;\n      }\n      audioTrack.pesData = null;\n    } else {\n      if (audioData?.size) {\n        logger.log(\n          'last AAC PES packet truncated,might overlap between fragments',\n        );\n      }\n\n      // either audioData null or PES truncated, keep it for next frag parsing\n      audioTrack.pesData = audioData;\n    }\n\n    if (id3Data && (pes = parsePES(id3Data))) {\n      this.parseID3PES(id3Track, pes);\n      id3Track.pesData = null;\n    } else {\n      // either id3Data null or PES truncated, keep it for next frag parsing\n      id3Track.pesData = id3Data;\n    }\n  }\n\n  public demuxSampleAes(\n    data: Uint8Array,\n    keyData: KeyData,\n    timeOffset: number,\n  ): Promise<DemuxerResult> {\n    const demuxResult = this.demux(\n      data,\n      timeOffset,\n      true,\n      !this.config.progressive,\n    );\n    const sampleAes = (this.sampleAes = new SampleAesDecrypter(\n      this.observer,\n      this.config,\n      keyData,\n    ));\n    return this.decrypt(demuxResult, sampleAes);\n  }\n\n  private decrypt(\n    demuxResult: DemuxerResult,\n    sampleAes: SampleAesDecrypter,\n  ): Promise<DemuxerResult> {\n    return new Promise((resolve) => {\n      const { audioTrack, videoTrack } = demuxResult;\n      if (audioTrack.samples && audioTrack.segmentCodec === 'aac') {\n        sampleAes.decryptAacSamples(audioTrack.samples, 0, () => {\n          if (videoTrack.samples) {\n            sampleAes.decryptAvcSamples(videoTrack.samples, 0, 0, () => {\n              resolve(demuxResult);\n            });\n          } else {\n            resolve(demuxResult);\n          }\n        });\n      } else if (videoTrack.samples) {\n        sampleAes.decryptAvcSamples(videoTrack.samples, 0, 0, () => {\n          resolve(demuxResult);\n        });\n      }\n    });\n  }\n\n  public destroy() {\n    this._duration = 0;\n  }\n\n  private parseAACPES(track: DemuxedAudioTrack, pes: PES) {\n    let startOffset = 0;\n    const aacOverFlow = this.aacOverFlow;\n    let data = pes.data;\n    if (aacOverFlow) {\n      this.aacOverFlow = null;\n      const frameMissingBytes = aacOverFlow.missing;\n      const sampleLength = aacOverFlow.sample.unit.byteLength;\n      // logger.log(`AAC: append overflowing ${sampleLength} bytes to beginning of new PES`);\n      if (frameMissingBytes === -1) {\n        data = appendUint8Array(aacOverFlow.sample.unit, data);\n      } else {\n        const frameOverflowBytes = sampleLength - frameMissingBytes;\n        aacOverFlow.sample.unit.set(\n          data.subarray(0, frameMissingBytes),\n          frameOverflowBytes,\n        );\n        track.samples.push(aacOverFlow.sample);\n        startOffset = aacOverFlow.missing;\n      }\n    }\n    // look for ADTS header (0xFFFx)\n    let offset: number;\n    let len: number;\n    for (offset = startOffset, len = data.length; offset < len - 1; offset++) {\n      if (ADTS.isHeader(data, offset)) {\n        break;\n      }\n    }\n    // if ADTS header does not start straight from the beginning of the PES payload, raise an error\n    if (offset !== startOffset) {\n      let reason: string;\n      const recoverable = offset < len - 1;\n      if (recoverable) {\n        reason = `AAC PES did not start with ADTS header,offset:${offset}`;\n      } else {\n        reason = 'No ADTS header found in AAC PES';\n      }\n      emitParsingError(this.observer, new Error(reason), recoverable);\n      if (!recoverable) {\n        return;\n      }\n    }\n\n    ADTS.initTrackConfig(\n      track,\n      this.observer,\n      data,\n      offset,\n      this.audioCodec as string,\n    );\n\n    let pts: number;\n    if (pes.pts !== undefined) {\n      pts = pes.pts;\n    } else if (aacOverFlow) {\n      // if last AAC frame is overflowing, we should ensure timestamps are contiguous:\n      // first sample PTS should be equal to last sample PTS + frameDuration\n      const frameDuration = ADTS.getFrameDuration(track.samplerate as number);\n      pts = aacOverFlow.sample.pts + frameDuration;\n    } else {\n      logger.warn('[tsdemuxer]: AAC PES unknown PTS');\n      return;\n    }\n\n    // scan for aac samples\n    let frameIndex = 0;\n    let frame;\n    while (offset < len) {\n      frame = ADTS.appendFrame(track, data, offset, pts, frameIndex);\n      offset += frame.length;\n      if (!frame.missing) {\n        frameIndex++;\n        for (; offset < len - 1; offset++) {\n          if (ADTS.isHeader(data, offset)) {\n            break;\n          }\n        }\n      } else {\n        this.aacOverFlow = frame;\n        break;\n      }\n    }\n  }\n\n  private parseMPEGPES(track: DemuxedAudioTrack, pes: PES) {\n    const data = pes.data;\n    const length = data.length;\n    let frameIndex = 0;\n    let offset = 0;\n    const pts = pes.pts;\n    if (pts === undefined) {\n      logger.warn('[tsdemuxer]: MPEG PES unknown PTS');\n      return;\n    }\n\n    while (offset < length) {\n      if (MpegAudio.isHeader(data, offset)) {\n        const frame = MpegAudio.appendFrame(\n          track,\n          data,\n          offset,\n          pts,\n          frameIndex,\n        );\n        if (frame) {\n          offset += frame.length;\n          frameIndex++;\n        } else {\n          // logger.log('Unable to parse Mpeg audio frame');\n          break;\n        }\n      } else {\n        // nothing found, keep looking\n        offset++;\n      }\n    }\n  }\n\n  private parseAC3PES(track: DemuxedAudioTrack, pes: PES) {\n    if (__USE_M2TS_ADVANCED_CODECS__) {\n      const data = pes.data;\n      const pts = pes.pts;\n      if (pts === undefined) {\n        logger.warn('[tsdemuxer]: AC3 PES unknown PTS');\n        return;\n      }\n      const length = data.length;\n      let frameIndex = 0;\n      let offset = 0;\n      let parsed;\n\n      while (\n        offset < length &&\n        (parsed = AC3.appendFrame(track, data, offset, pts, frameIndex++)) > 0\n      ) {\n        offset += parsed;\n      }\n    }\n  }\n\n  private parseID3PES(id3Track: DemuxedMetadataTrack, pes: PES) {\n    if (pes.pts === undefined) {\n      logger.warn('[tsdemuxer]: ID3 PES unknown PTS');\n      return;\n    }\n    const id3Sample = Object.assign({}, pes as Required<PES>, {\n      type: this._videoTrack ? MetadataSchema.emsg : MetadataSchema.audioId3,\n      duration: Number.POSITIVE_INFINITY,\n    });\n    id3Track.samples.push(id3Sample);\n  }\n}\n\nfunction parsePID(data: Uint8Array, offset: number): number {\n  // pid is a 13-bit field starting at the last bit of TS[1]\n  return ((data[offset + 1] & 0x1f) << 8) + data[offset + 2];\n}\n\nfunction parsePAT(data: Uint8Array, offset: number): number {\n  // skip the PSI header and parse the first PMT entry\n  return ((data[offset + 10] & 0x1f) << 8) | data[offset + 11];\n}\n\nfunction parsePMT(\n  data: Uint8Array,\n  offset: number,\n  typeSupported: TypeSupported,\n  isSampleAes: boolean,\n  observer: HlsEventEmitter,\n) {\n  const result = {\n    audioPid: -1,\n    videoPid: -1,\n    id3Pid: -1,\n    segmentVideoCodec: 'avc',\n    segmentAudioCodec: 'aac',\n  };\n  const sectionLength = ((data[offset + 1] & 0x0f) << 8) | data[offset + 2];\n  const tableEnd = offset + 3 + sectionLength - 4;\n  // to determine where the table is, we have to figure out how\n  // long the program info descriptors are\n  const programInfoLength =\n    ((data[offset + 10] & 0x0f) << 8) | data[offset + 11];\n  // advance the offset to the first entry in the mapping table\n  offset += 12 + programInfoLength;\n  while (offset < tableEnd) {\n    const pid = parsePID(data, offset);\n    const esInfoLength = ((data[offset + 3] & 0x0f) << 8) | data[offset + 4];\n    switch (data[offset]) {\n      case 0xcf: // SAMPLE-AES AAC\n        if (!isSampleAes) {\n          logEncryptedSamplesFoundInUnencryptedStream('ADTS AAC');\n          break;\n        }\n      /* falls through */\n      case 0x0f: // ISO/IEC 13818-7 ADTS AAC (MPEG-2 lower bit-rate audio)\n        // logger.log('AAC PID:'  + pid);\n        if (result.audioPid === -1) {\n          result.audioPid = pid;\n        }\n\n        break;\n\n      // Packetized metadata (ID3)\n      case 0x15:\n        // logger.log('ID3 PID:'  + pid);\n        if (result.id3Pid === -1) {\n          result.id3Pid = pid;\n        }\n\n        break;\n\n      case 0xdb: // SAMPLE-AES AVC\n        if (!isSampleAes) {\n          logEncryptedSamplesFoundInUnencryptedStream('H.264');\n          break;\n        }\n      /* falls through */\n      case 0x1b: // ITU-T Rec. H.264 and ISO/IEC 14496-10 (lower bit-rate video)\n        // logger.log('AVC PID:'  + pid);\n        if (result.videoPid === -1) {\n          result.videoPid = pid;\n          result.segmentVideoCodec = 'avc';\n        }\n\n        break;\n\n      // ISO/IEC 11172-3 (MPEG-1 audio)\n      // or ISO/IEC 13818-3 (MPEG-2 halved sample rate audio)\n      case 0x03:\n      case 0x04:\n        // logger.log('MPEG PID:'  + pid);\n        if (!typeSupported.mpeg && !typeSupported.mp3) {\n          logger.log('MPEG audio found, not supported in this browser');\n        } else if (result.audioPid === -1) {\n          result.audioPid = pid;\n          result.segmentAudioCodec = 'mp3';\n        }\n        break;\n\n      case 0xc1: // SAMPLE-AES AC3\n        if (!isSampleAes) {\n          logEncryptedSamplesFoundInUnencryptedStream('AC-3');\n          break;\n        }\n      /* falls through */\n      case 0x81:\n        if (__USE_M2TS_ADVANCED_CODECS__) {\n          if (!typeSupported.ac3) {\n            logger.log('AC-3 audio found, not supported in this browser');\n          } else if (result.audioPid === -1) {\n            result.audioPid = pid;\n            result.segmentAudioCodec = 'ac3';\n          }\n        } else {\n          logger.warn('AC-3 in M2TS support not included in build');\n        }\n        break;\n\n      case 0x06:\n        // stream_type 6 can mean a lot of different things in case of DVB.\n        // We need to look at the descriptors. Right now, we're only interested\n        // in AC-3 audio, so we do the descriptor parsing only when we don't have\n        // an audio PID yet.\n        if (result.audioPid === -1 && esInfoLength > 0) {\n          let parsePos = offset + 5;\n          let remaining = esInfoLength;\n\n          while (remaining > 2) {\n            const descriptorId = data[parsePos];\n\n            switch (descriptorId) {\n              case 0x6a: // DVB Descriptor for AC-3\n                if (__USE_M2TS_ADVANCED_CODECS__) {\n                  if (typeSupported.ac3 !== true) {\n                    logger.log(\n                      'AC-3 audio found, not supported in this browser for now',\n                    );\n                  } else {\n                    result.audioPid = pid;\n                    result.segmentAudioCodec = 'ac3';\n                  }\n                } else {\n                  logger.warn('AC-3 in M2TS support not included in build');\n                }\n                break;\n            }\n\n            const descriptorLen = data[parsePos + 1] + 2;\n            parsePos += descriptorLen;\n            remaining -= descriptorLen;\n          }\n        }\n        break;\n\n      case 0xc2: // SAMPLE-AES EC3\n      /* falls through */\n      case 0x87:\n        emitParsingError(observer, new Error('Unsupported EC-3 in M2TS found'));\n        return result;\n\n      case 0x24:\n        emitParsingError(observer, new Error('Unsupported HEVC in M2TS found'));\n        return result;\n\n      default:\n        // logger.log('unknown stream type:' + data[offset]);\n        break;\n    }\n    // move to the next table entry\n    // skip past the elementary stream descriptors, if present\n    offset += esInfoLength + 5;\n  }\n  return result;\n}\n\nfunction emitParsingError(\n  observer: HlsEventEmitter,\n  error: Error,\n  levelRetry?: boolean,\n) {\n  logger.warn(`parsing error: ${error.message}`);\n  observer.emit(Events.ERROR, Events.ERROR, {\n    type: ErrorTypes.MEDIA_ERROR,\n    details: ErrorDetails.FRAG_PARSING_ERROR,\n    fatal: false,\n    levelRetry,\n    error,\n    reason: error.message,\n  });\n}\n\nfunction logEncryptedSamplesFoundInUnencryptedStream(type: string) {\n  logger.log(`${type} with AES-128-CBC encryption found in unencrypted stream`);\n}\n\nfunction parsePES(stream: ElementaryStreamData): PES | null {\n  let i = 0;\n  let frag: Uint8Array;\n  let pesLen: number;\n  let pesHdrLen: number;\n  let pesPts: number | undefined;\n  let pesDts: number | undefined;\n  const data = stream.data;\n  // safety check\n  if (!stream || stream.size === 0) {\n    return null;\n  }\n\n  // we might need up to 19 bytes to read PES header\n  // if first chunk of data is less than 19 bytes, let's merge it with following ones until we get 19 bytes\n  // usually only one merge is needed (and this is rare ...)\n  while (data[0].length < 19 && data.length > 1) {\n    data[0] = appendUint8Array(data[0], data[1]);\n    data.splice(1, 1);\n  }\n  // retrieve PTS/DTS from first fragment\n  frag = data[0];\n  const pesPrefix = (frag[0] << 16) + (frag[1] << 8) + frag[2];\n  if (pesPrefix === 1) {\n    pesLen = (frag[4] << 8) + frag[5];\n    // if PES parsed length is not zero and greater than total received length, stop parsing. PES might be truncated\n    // minus 6 : PES header size\n    if (pesLen && pesLen > stream.size - 6) {\n      return null;\n    }\n\n    const pesFlags = frag[7];\n    if (pesFlags & 0xc0) {\n      /* PES header described here : http://dvd.sourceforge.net/dvdinfo/pes-hdr.html\n          as PTS / DTS is 33 bit we cannot use bitwise operator in JS,\n          as Bitwise operators treat their operands as a sequence of 32 bits */\n      pesPts =\n        (frag[9] & 0x0e) * 536870912 + // 1 << 29\n        (frag[10] & 0xff) * 4194304 + // 1 << 22\n        (frag[11] & 0xfe) * 16384 + // 1 << 14\n        (frag[12] & 0xff) * 128 + // 1 << 7\n        (frag[13] & 0xfe) / 2;\n\n      if (pesFlags & 0x40) {\n        pesDts =\n          (frag[14] & 0x0e) * 536870912 + // 1 << 29\n          (frag[15] & 0xff) * 4194304 + // 1 << 22\n          (frag[16] & 0xfe) * 16384 + // 1 << 14\n          (frag[17] & 0xff) * 128 + // 1 << 7\n          (frag[18] & 0xfe) / 2;\n\n        if (pesPts - pesDts > 60 * 90000) {\n          logger.warn(\n            `${Math.round(\n              (pesPts - pesDts) / 90000,\n            )}s delta between PTS and DTS, align them`,\n          );\n          pesPts = pesDts;\n        }\n      } else {\n        pesDts = pesPts;\n      }\n    }\n    pesHdrLen = frag[8];\n    // 9 bytes : 6 bytes for PES header + 3 bytes for PES extension\n    let payloadStartOffset = pesHdrLen + 9;\n    if (stream.size <= payloadStartOffset) {\n      return null;\n    }\n    stream.size -= payloadStartOffset;\n    // reassemble PES packet\n    const pesData = new Uint8Array(stream.size);\n    for (let j = 0, dataLen = data.length; j < dataLen; j++) {\n      frag = data[j];\n      let len = frag.byteLength;\n      if (payloadStartOffset) {\n        if (payloadStartOffset > len) {\n          // trim full frag if PES header bigger than frag\n          payloadStartOffset -= len;\n          continue;\n        } else {\n          // trim partial frag if PES header smaller than frag\n          frag = frag.subarray(payloadStartOffset);\n          len -= payloadStartOffset;\n          payloadStartOffset = 0;\n        }\n      }\n      pesData.set(frag, i);\n      i += len;\n    }\n    if (pesLen) {\n      // payload size : remove PES header + PES extension\n      pesLen -= pesHdrLen + 3;\n    }\n    return { data: pesData, pts: pesPts, dts: pesDts, len: pesLen };\n  }\n  return null;\n}\n\nexport default TSDemuxer;\n", "/**\n * MP3 demuxer\n */\nimport BaseAudioDemuxer from './base-audio-demuxer';\nimport { getID3Data, getTimeStamp } from '../id3';\nimport { getAudioBSID } from './dolby';\nimport { logger } from '../../utils/logger';\nimport * as MpegAudio from './mpegaudio';\n\nclass MP3Demuxer extends BaseAudioDemuxer {\n  resetInitSegment(\n    initSegment: Uint8Array | undefined,\n    audioCodec: string | undefined,\n    videoCodec: string | undefined,\n    trackDuration: number,\n  ) {\n    super.resetInitSegment(initSegment, audioCodec, videoCodec, trackDuration);\n    this._audioTrack = {\n      container: 'audio/mpeg',\n      type: 'audio',\n      id: 2,\n      pid: -1,\n      sequenceNumber: 0,\n      segmentCodec: 'mp3',\n      samples: [],\n      manifestCodec: audioCodec,\n      duration: trackDuration,\n      inputTimeScale: 90000,\n      dropped: 0,\n    };\n  }\n\n  static probe(data: Uint8Array | undefined): boolean {\n    if (!data) {\n      return false;\n    }\n\n    // check if data contains ID3 timestamp and MPEG sync word\n    // Look for MPEG header | 1111 1111 | 111X XYZX | where X can be either 0 or 1 and Y or Z should be 1\n    // Layer bits (position 14 and 15) in header should be always different from 0 (Layer I or Layer II or Layer III)\n    // More info http://www.mp3-tech.org/programmer/frame_header.html\n    const id3Data = getID3Data(data, 0);\n    let offset = id3Data?.length || 0;\n\n    // Check for ac-3|ec-3 sync bytes and return false if present\n    if (\n      id3Data &&\n      data[offset] === 0x0b &&\n      data[offset + 1] === 0x77 &&\n      getTimeStamp(id3Data) !== undefined &&\n      // check the bsid to confirm ac-3 or ec-3 (not mp3)\n      getAudioBSID(data, offset) <= 16\n    ) {\n      return false;\n    }\n\n    for (let length = data.length; offset < length; offset++) {\n      if (MpegAudio.probe(data, offset)) {\n        logger.log('MPEG Audio sync word found !');\n        return true;\n      }\n    }\n    return false;\n  }\n\n  canParse(data, offset) {\n    return MpegAudio.canParse(data, offset);\n  }\n\n  appendFrame(track, data, offset) {\n    if (this.basePTS === null) {\n      return;\n    }\n    return MpegAudio.appendFrame(\n      track,\n      data,\n      offset,\n      this.basePTS,\n      this.frameIndex,\n    );\n  }\n}\n\nexport default MP3Demuxer;\n", "/**\n *  AAC helper\n */\n\nclass AAC {\n  static getSilentFrame(\n    codec?: string,\n    channelCount?: number,\n  ): Uint8Array | undefined {\n    switch (codec) {\n      case 'mp4a.40.2':\n        if (channelCount === 1) {\n          return new Uint8Array([0x00, 0xc8, 0x00, 0x80, 0x23, 0x80]);\n        } else if (channelCount === 2) {\n          return new Uint8Array([\n            0x21, 0x00, 0x49, 0x90, 0x02, 0x19, 0x00, 0x23, 0x80,\n          ]);\n        } else if (channelCount === 3) {\n          return new Uint8Array([\n            0x00, 0xc8, 0x00, 0x80, 0x20, 0x84, 0x01, 0x26, 0x40, 0x08, 0x64,\n            0x00, 0x8e,\n          ]);\n        } else if (channelCount === 4) {\n          return new Uint8Array([\n            0x00, 0xc8, 0x00, 0x80, 0x20, 0x84, 0x01, 0x26, 0x40, 0x08, 0x64,\n            0x00, 0x80, 0x2c, 0x80, 0x08, 0x02, 0x38,\n          ]);\n        } else if (channelCount === 5) {\n          return new Uint8Array([\n            0x00, 0xc8, 0x00, 0x80, 0x20, 0x84, 0x01, 0x26, 0x40, 0x08, 0x64,\n            0x00, 0x82, 0x30, 0x04, 0x99, 0x00, 0x21, 0x90, 0x02, 0x38,\n          ]);\n        } else if (channelCount === 6) {\n          return new Uint8Array([\n            0x00, 0xc8, 0x00, 0x80, 0x20, 0x84, 0x01, 0x26, 0x40, 0x08, 0x64,\n            0x00, 0x82, 0x30, 0x04, 0x99, 0x00, 0x21, 0x90, 0x02, 0x00, 0xb2,\n            0x00, 0x20, 0x08, 0xe0,\n          ]);\n        }\n\n        break;\n      // handle HE-AAC below (mp4a.40.5 / mp4a.40.29)\n      default:\n        if (channelCount === 1) {\n          // ffmpeg -y -f lavfi -i \"aevalsrc=0:d=0.05\" -c:a libfdk_aac -profile:a aac_he -b:a 4k output.aac && hexdump -v -e '16/1 \"0x%x,\" \"\\n\"' -v output.aac\n          return new Uint8Array([\n            0x1, 0x40, 0x22, 0x80, 0xa3, 0x4e, 0xe6, 0x80, 0xba, 0x8, 0x0, 0x0,\n            0x0, 0x1c, 0x6, 0xf1, 0xc1, 0xa, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a,\n            0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a,\n            0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a,\n            0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a,\n            0x5a, 0x5e,\n          ]);\n        } else if (channelCount === 2) {\n          // ffmpeg -y -f lavfi -i \"aevalsrc=0|0:d=0.05\" -c:a libfdk_aac -profile:a aac_he_v2 -b:a 4k output.aac && hexdump -v -e '16/1 \"0x%x,\" \"\\n\"' -v output.aac\n          return new Uint8Array([\n            0x1, 0x40, 0x22, 0x80, 0xa3, 0x5e, 0xe6, 0x80, 0xba, 0x8, 0x0, 0x0,\n            0x0, 0x0, 0x95, 0x0, 0x6, 0xf1, 0xa1, 0xa, 0x5a, 0x5a, 0x5a, 0x5a,\n            0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a,\n            0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a,\n            0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a,\n            0x5a, 0x5e,\n          ]);\n        } else if (channelCount === 3) {\n          // ffmpeg -y -f lavfi -i \"aevalsrc=0|0|0:d=0.05\" -c:a libfdk_aac -profile:a aac_he_v2 -b:a 4k output.aac && hexdump -v -e '16/1 \"0x%x,\" \"\\n\"' -v output.aac\n          return new Uint8Array([\n            0x1, 0x40, 0x22, 0x80, 0xa3, 0x5e, 0xe6, 0x80, 0xba, 0x8, 0x0, 0x0,\n            0x0, 0x0, 0x95, 0x0, 0x6, 0xf1, 0xa1, 0xa, 0x5a, 0x5a, 0x5a, 0x5a,\n            0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a,\n            0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a,\n            0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a, 0x5a,\n            0x5a, 0x5e,\n          ]);\n        }\n        break;\n    }\n    return undefined;\n  }\n}\n\nexport default AAC;\n", "/**\n * Generate MP4 Box\n */\n\nimport { appendUint8Array } from '../utils/mp4-tools';\n\ntype HdlrTypes = {\n  video: Uint8Array;\n  audio: Uint8Array;\n};\n\nconst UINT32_MAX = Math.pow(2, 32) - 1;\n\nclass MP4 {\n  public static types: Record<string, number[]>;\n  private static HDLR_TYPES: HdlrTypes;\n  private static STTS: Uint8Array;\n  private static STSC: Uint8Array;\n  private static STCO: Uint8Array;\n  private static STSZ: Uint8Array;\n  private static VMHD: Uint8Array;\n  private static SMHD: Uint8Array;\n  private static STSD: Uint8Array;\n  private static FTYP: Uint8Array;\n  private static DINF: Uint8Array;\n\n  static init() {\n    MP4.types = {\n      avc1: [], // codingname\n      avcC: [],\n      btrt: [],\n      dinf: [],\n      dref: [],\n      esds: [],\n      ftyp: [],\n      hdlr: [],\n      mdat: [],\n      mdhd: [],\n      mdia: [],\n      mfhd: [],\n      minf: [],\n      moof: [],\n      moov: [],\n      mp4a: [],\n      '.mp3': [],\n      dac3: [],\n      'ac-3': [],\n      mvex: [],\n      mvhd: [],\n      pasp: [],\n      sdtp: [],\n      stbl: [],\n      stco: [],\n      stsc: [],\n      stsd: [],\n      stsz: [],\n      stts: [],\n      tfdt: [],\n      tfhd: [],\n      traf: [],\n      trak: [],\n      trun: [],\n      trex: [],\n      tkhd: [],\n      vmhd: [],\n      smhd: [],\n    };\n\n    let i: string;\n    for (i in MP4.types) {\n      if (MP4.types.hasOwnProperty(i)) {\n        MP4.types[i] = [\n          i.charCodeAt(0),\n          i.charCodeAt(1),\n          i.charCodeAt(2),\n          i.charCodeAt(3),\n        ];\n      }\n    }\n\n    const videoHdlr = new Uint8Array([\n      0x00, // version 0\n      0x00,\n      0x00,\n      0x00, // flags\n      0x00,\n      0x00,\n      0x00,\n      0x00, // pre_defined\n      0x76,\n      0x69,\n      0x64,\n      0x65, // handler_type: 'vide'\n      0x00,\n      0x00,\n      0x00,\n      0x00, // reserved\n      0x00,\n      0x00,\n      0x00,\n      0x00, // reserved\n      0x00,\n      0x00,\n      0x00,\n      0x00, // reserved\n      0x56,\n      0x69,\n      0x64,\n      0x65,\n      0x6f,\n      0x48,\n      0x61,\n      0x6e,\n      0x64,\n      0x6c,\n      0x65,\n      0x72,\n      0x00, // name: 'VideoHandler'\n    ]);\n\n    const audioHdlr = new Uint8Array([\n      0x00, // version 0\n      0x00,\n      0x00,\n      0x00, // flags\n      0x00,\n      0x00,\n      0x00,\n      0x00, // pre_defined\n      0x73,\n      0x6f,\n      0x75,\n      0x6e, // handler_type: 'soun'\n      0x00,\n      0x00,\n      0x00,\n      0x00, // reserved\n      0x00,\n      0x00,\n      0x00,\n      0x00, // reserved\n      0x00,\n      0x00,\n      0x00,\n      0x00, // reserved\n      0x53,\n      0x6f,\n      0x75,\n      0x6e,\n      0x64,\n      0x48,\n      0x61,\n      0x6e,\n      0x64,\n      0x6c,\n      0x65,\n      0x72,\n      0x00, // name: 'SoundHandler'\n    ]);\n\n    MP4.HDLR_TYPES = {\n      video: videoHdlr,\n      audio: audioHdlr,\n    };\n\n    const dref = new Uint8Array([\n      0x00, // version 0\n      0x00,\n      0x00,\n      0x00, // flags\n      0x00,\n      0x00,\n      0x00,\n      0x01, // entry_count\n      0x00,\n      0x00,\n      0x00,\n      0x0c, // entry_size\n      0x75,\n      0x72,\n      0x6c,\n      0x20, // 'url' type\n      0x00, // version 0\n      0x00,\n      0x00,\n      0x01, // entry_flags\n    ]);\n\n    const stco = new Uint8Array([\n      0x00, // version\n      0x00,\n      0x00,\n      0x00, // flags\n      0x00,\n      0x00,\n      0x00,\n      0x00, // entry_count\n    ]);\n\n    MP4.STTS = MP4.STSC = MP4.STCO = stco;\n\n    MP4.STSZ = new Uint8Array([\n      0x00, // version\n      0x00,\n      0x00,\n      0x00, // flags\n      0x00,\n      0x00,\n      0x00,\n      0x00, // sample_size\n      0x00,\n      0x00,\n      0x00,\n      0x00, // sample_count\n    ]);\n    MP4.VMHD = new Uint8Array([\n      0x00, // version\n      0x00,\n      0x00,\n      0x01, // flags\n      0x00,\n      0x00, // graphicsmode\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00, // opcolor\n    ]);\n    MP4.SMHD = new Uint8Array([\n      0x00, // version\n      0x00,\n      0x00,\n      0x00, // flags\n      0x00,\n      0x00, // balance\n      0x00,\n      0x00, // reserved\n    ]);\n\n    MP4.STSD = new Uint8Array([\n      0x00, // version 0\n      0x00,\n      0x00,\n      0x00, // flags\n      0x00,\n      0x00,\n      0x00,\n      0x01,\n    ]); // entry_count\n\n    const majorBrand = new Uint8Array([105, 115, 111, 109]); // isom\n    const avc1Brand = new Uint8Array([97, 118, 99, 49]); // avc1\n    const minorVersion = new Uint8Array([0, 0, 0, 1]);\n\n    MP4.FTYP = MP4.box(\n      MP4.types.ftyp,\n      majorBrand,\n      minorVersion,\n      majorBrand,\n      avc1Brand,\n    );\n    MP4.DINF = MP4.box(MP4.types.dinf, MP4.box(MP4.types.dref, dref));\n  }\n\n  static box(type, ...payload: Uint8Array[]) {\n    let size = 8;\n    let i = payload.length;\n    const len = i;\n    // calculate the total size we need to allocate\n    while (i--) {\n      size += payload[i].byteLength;\n    }\n\n    const result = new Uint8Array(size);\n    result[0] = (size >> 24) & 0xff;\n    result[1] = (size >> 16) & 0xff;\n    result[2] = (size >> 8) & 0xff;\n    result[3] = size & 0xff;\n    result.set(type, 4);\n    // copy the payload into the result\n    for (i = 0, size = 8; i < len; i++) {\n      // copy payload[i] array @ offset size\n      result.set(payload[i], size);\n      size += payload[i].byteLength;\n    }\n    return result;\n  }\n\n  static hdlr(type) {\n    return MP4.box(MP4.types.hdlr, MP4.HDLR_TYPES[type]);\n  }\n\n  static mdat(data) {\n    return MP4.box(MP4.types.mdat, data);\n  }\n\n  static mdhd(timescale, duration) {\n    duration *= timescale;\n    const upperWordDuration = Math.floor(duration / (UINT32_MAX + 1));\n    const lowerWordDuration = Math.floor(duration % (UINT32_MAX + 1));\n    return MP4.box(\n      MP4.types.mdhd,\n      new Uint8Array([\n        0x01, // version 1\n        0x00,\n        0x00,\n        0x00, // flags\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x02, // creation_time\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x03, // modification_time\n        (timescale >> 24) & 0xff,\n        (timescale >> 16) & 0xff,\n        (timescale >> 8) & 0xff,\n        timescale & 0xff, // timescale\n        upperWordDuration >> 24,\n        (upperWordDuration >> 16) & 0xff,\n        (upperWordDuration >> 8) & 0xff,\n        upperWordDuration & 0xff,\n        lowerWordDuration >> 24,\n        (lowerWordDuration >> 16) & 0xff,\n        (lowerWordDuration >> 8) & 0xff,\n        lowerWordDuration & 0xff,\n        0x55,\n        0xc4, // 'und' language (undetermined)\n        0x00,\n        0x00,\n      ]),\n    );\n  }\n\n  static mdia(track) {\n    return MP4.box(\n      MP4.types.mdia,\n      MP4.mdhd(track.timescale, track.duration),\n      MP4.hdlr(track.type),\n      MP4.minf(track),\n    );\n  }\n\n  static mfhd(sequenceNumber) {\n    return MP4.box(\n      MP4.types.mfhd,\n      new Uint8Array([\n        0x00,\n        0x00,\n        0x00,\n        0x00, // flags\n        sequenceNumber >> 24,\n        (sequenceNumber >> 16) & 0xff,\n        (sequenceNumber >> 8) & 0xff,\n        sequenceNumber & 0xff, // sequence_number\n      ]),\n    );\n  }\n\n  static minf(track) {\n    if (track.type === 'audio') {\n      return MP4.box(\n        MP4.types.minf,\n        MP4.box(MP4.types.smhd, MP4.SMHD),\n        MP4.DINF,\n        MP4.stbl(track),\n      );\n    } else {\n      return MP4.box(\n        MP4.types.minf,\n        MP4.box(MP4.types.vmhd, MP4.VMHD),\n        MP4.DINF,\n        MP4.stbl(track),\n      );\n    }\n  }\n\n  static moof(sn, baseMediaDecodeTime, track) {\n    return MP4.box(\n      MP4.types.moof,\n      MP4.mfhd(sn),\n      MP4.traf(track, baseMediaDecodeTime),\n    );\n  }\n\n  static moov(tracks) {\n    let i = tracks.length;\n    const boxes: Uint8Array[] = [];\n\n    while (i--) {\n      boxes[i] = MP4.trak(tracks[i]);\n    }\n\n    return MP4.box.apply(\n      null,\n      [MP4.types.moov, MP4.mvhd(tracks[0].timescale, tracks[0].duration)]\n        .concat(boxes)\n        .concat(MP4.mvex(tracks)),\n    );\n  }\n\n  static mvex(tracks) {\n    let i = tracks.length;\n    const boxes: Uint8Array[] = [];\n\n    while (i--) {\n      boxes[i] = MP4.trex(tracks[i]);\n    }\n\n    return MP4.box.apply(null, [MP4.types.mvex, ...boxes]);\n  }\n\n  static mvhd(timescale, duration) {\n    duration *= timescale;\n    const upperWordDuration = Math.floor(duration / (UINT32_MAX + 1));\n    const lowerWordDuration = Math.floor(duration % (UINT32_MAX + 1));\n    const bytes = new Uint8Array([\n      0x01, // version 1\n      0x00,\n      0x00,\n      0x00, // flags\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x02, // creation_time\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x03, // modification_time\n      (timescale >> 24) & 0xff,\n      (timescale >> 16) & 0xff,\n      (timescale >> 8) & 0xff,\n      timescale & 0xff, // timescale\n      upperWordDuration >> 24,\n      (upperWordDuration >> 16) & 0xff,\n      (upperWordDuration >> 8) & 0xff,\n      upperWordDuration & 0xff,\n      lowerWordDuration >> 24,\n      (lowerWordDuration >> 16) & 0xff,\n      (lowerWordDuration >> 8) & 0xff,\n      lowerWordDuration & 0xff,\n      0x00,\n      0x01,\n      0x00,\n      0x00, // 1.0 rate\n      0x01,\n      0x00, // 1.0 volume\n      0x00,\n      0x00, // reserved\n      0x00,\n      0x00,\n      0x00,\n      0x00, // reserved\n      0x00,\n      0x00,\n      0x00,\n      0x00, // reserved\n      0x00,\n      0x01,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x01,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x40,\n      0x00,\n      0x00,\n      0x00, // transformation: unity matrix\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00, // pre_defined\n      0xff,\n      0xff,\n      0xff,\n      0xff, // next_track_ID\n    ]);\n    return MP4.box(MP4.types.mvhd, bytes);\n  }\n\n  static sdtp(track) {\n    const samples = track.samples || [];\n    const bytes = new Uint8Array(4 + samples.length);\n    let i;\n    let flags;\n    // leave the full box header (4 bytes) all zero\n    // write the sample table\n    for (i = 0; i < samples.length; i++) {\n      flags = samples[i].flags;\n      bytes[i + 4] =\n        (flags.dependsOn << 4) |\n        (flags.isDependedOn << 2) |\n        flags.hasRedundancy;\n    }\n\n    return MP4.box(MP4.types.sdtp, bytes);\n  }\n\n  static stbl(track) {\n    return MP4.box(\n      MP4.types.stbl,\n      MP4.stsd(track),\n      MP4.box(MP4.types.stts, MP4.STTS),\n      MP4.box(MP4.types.stsc, MP4.STSC),\n      MP4.box(MP4.types.stsz, MP4.STSZ),\n      MP4.box(MP4.types.stco, MP4.STCO),\n    );\n  }\n\n  static avc1(track) {\n    let sps: number[] = [];\n    let pps: number[] = [];\n    let i;\n    let data;\n    let len;\n    // assemble the SPSs\n\n    for (i = 0; i < track.sps.length; i++) {\n      data = track.sps[i];\n      len = data.byteLength;\n      sps.push((len >>> 8) & 0xff);\n      sps.push(len & 0xff);\n\n      // SPS\n      sps = sps.concat(Array.prototype.slice.call(data));\n    }\n\n    // assemble the PPSs\n    for (i = 0; i < track.pps.length; i++) {\n      data = track.pps[i];\n      len = data.byteLength;\n      pps.push((len >>> 8) & 0xff);\n      pps.push(len & 0xff);\n\n      pps = pps.concat(Array.prototype.slice.call(data));\n    }\n\n    const avcc = MP4.box(\n      MP4.types.avcC,\n      new Uint8Array(\n        [\n          0x01, // version\n          sps[3], // profile\n          sps[4], // profile compat\n          sps[5], // level\n          0xfc | 3, // lengthSizeMinusOne, hard-coded to 4 bytes\n          0xe0 | track.sps.length, // 3bit reserved (111) + numOfSequenceParameterSets\n        ]\n          .concat(sps)\n          .concat([\n            track.pps.length, // numOfPictureParameterSets\n          ])\n          .concat(pps),\n      ),\n    ); // \"PPS\"\n    const width = track.width;\n    const height = track.height;\n    const hSpacing = track.pixelRatio[0];\n    const vSpacing = track.pixelRatio[1];\n\n    return MP4.box(\n      MP4.types.avc1,\n      new Uint8Array([\n        0x00,\n        0x00,\n        0x00, // reserved\n        0x00,\n        0x00,\n        0x00, // reserved\n        0x00,\n        0x01, // data_reference_index\n        0x00,\n        0x00, // pre_defined\n        0x00,\n        0x00, // reserved\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00, // pre_defined\n        (width >> 8) & 0xff,\n        width & 0xff, // width\n        (height >> 8) & 0xff,\n        height & 0xff, // height\n        0x00,\n        0x48,\n        0x00,\n        0x00, // horizresolution\n        0x00,\n        0x48,\n        0x00,\n        0x00, // vertresolution\n        0x00,\n        0x00,\n        0x00,\n        0x00, // reserved\n        0x00,\n        0x01, // frame_count\n        0x12,\n        0x64,\n        0x61,\n        0x69,\n        0x6c, // dailymotion/hls.js\n        0x79,\n        0x6d,\n        0x6f,\n        0x74,\n        0x69,\n        0x6f,\n        0x6e,\n        0x2f,\n        0x68,\n        0x6c,\n        0x73,\n        0x2e,\n        0x6a,\n        0x73,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00, // compressorname\n        0x00,\n        0x18, // depth = 24\n        0x11,\n        0x11,\n      ]), // pre_defined = -1\n      avcc,\n      MP4.box(\n        MP4.types.btrt,\n        new Uint8Array([\n          0x00,\n          0x1c,\n          0x9c,\n          0x80, // bufferSizeDB\n          0x00,\n          0x2d,\n          0xc6,\n          0xc0, // maxBitrate\n          0x00,\n          0x2d,\n          0xc6,\n          0xc0,\n        ]),\n      ), // avgBitrate\n      MP4.box(\n        MP4.types.pasp,\n        new Uint8Array([\n          hSpacing >> 24, // hSpacing\n          (hSpacing >> 16) & 0xff,\n          (hSpacing >> 8) & 0xff,\n          hSpacing & 0xff,\n          vSpacing >> 24, // vSpacing\n          (vSpacing >> 16) & 0xff,\n          (vSpacing >> 8) & 0xff,\n          vSpacing & 0xff,\n        ]),\n      ),\n    );\n  }\n\n  static esds(track) {\n    const configlen = track.config.length;\n    return new Uint8Array(\n      [\n        0x00, // version 0\n        0x00,\n        0x00,\n        0x00, // flags\n\n        0x03, // descriptor_type\n        0x17 + configlen, // length\n        0x00,\n        0x01, // es_id\n        0x00, // stream_priority\n\n        0x04, // descriptor_type\n        0x0f + configlen, // length\n        0x40, // codec : mpeg4_audio\n        0x15, // stream_type\n        0x00,\n        0x00,\n        0x00, // buffer_size\n        0x00,\n        0x00,\n        0x00,\n        0x00, // maxBitrate\n        0x00,\n        0x00,\n        0x00,\n        0x00, // avgBitrate\n\n        0x05, // descriptor_type\n      ]\n        .concat([configlen])\n        .concat(track.config)\n        .concat([0x06, 0x01, 0x02]),\n    ); // GASpecificConfig)); // length + audio config descriptor\n  }\n\n  static audioStsd(track) {\n    const samplerate = track.samplerate;\n    return new Uint8Array([\n      0x00,\n      0x00,\n      0x00, // reserved\n      0x00,\n      0x00,\n      0x00, // reserved\n      0x00,\n      0x01, // data_reference_index\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00,\n      0x00, // reserved\n      0x00,\n      track.channelCount, // channelcount\n      0x00,\n      0x10, // sampleSize:16bits\n      0x00,\n      0x00,\n      0x00,\n      0x00, // reserved2\n      (samplerate >> 8) & 0xff,\n      samplerate & 0xff, //\n      0x00,\n      0x00,\n    ]);\n  }\n\n  static mp4a(track) {\n    return MP4.box(\n      MP4.types.mp4a,\n      MP4.audioStsd(track),\n      MP4.box(MP4.types.esds, MP4.esds(track)),\n    );\n  }\n\n  static mp3(track) {\n    return MP4.box(MP4.types['.mp3'], MP4.audioStsd(track));\n  }\n\n  static ac3(track) {\n    return MP4.box(\n      MP4.types['ac-3'],\n      MP4.audioStsd(track),\n      MP4.box(MP4.types.dac3, track.config),\n    );\n  }\n\n  static stsd(track) {\n    if (track.type === 'audio') {\n      if (track.segmentCodec === 'mp3' && track.codec === 'mp3') {\n        return MP4.box(MP4.types.stsd, MP4.STSD, MP4.mp3(track));\n      }\n      if (track.segmentCodec === 'ac3') {\n        return MP4.box(MP4.types.stsd, MP4.STSD, MP4.ac3(track));\n      }\n      return MP4.box(MP4.types.stsd, MP4.STSD, MP4.mp4a(track));\n    } else {\n      return MP4.box(MP4.types.stsd, MP4.STSD, MP4.avc1(track));\n    }\n  }\n\n  static tkhd(track) {\n    const id = track.id;\n    const duration = track.duration * track.timescale;\n    const width = track.width;\n    const height = track.height;\n    const upperWordDuration = Math.floor(duration / (UINT32_MAX + 1));\n    const lowerWordDuration = Math.floor(duration % (UINT32_MAX + 1));\n    return MP4.box(\n      MP4.types.tkhd,\n      new Uint8Array([\n        0x01, // version 1\n        0x00,\n        0x00,\n        0x07, // flags\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x02, // creation_time\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x03, // modification_time\n        (id >> 24) & 0xff,\n        (id >> 16) & 0xff,\n        (id >> 8) & 0xff,\n        id & 0xff, // track_ID\n        0x00,\n        0x00,\n        0x00,\n        0x00, // reserved\n        upperWordDuration >> 24,\n        (upperWordDuration >> 16) & 0xff,\n        (upperWordDuration >> 8) & 0xff,\n        upperWordDuration & 0xff,\n        lowerWordDuration >> 24,\n        (lowerWordDuration >> 16) & 0xff,\n        (lowerWordDuration >> 8) & 0xff,\n        lowerWordDuration & 0xff,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00, // reserved\n        0x00,\n        0x00, // layer\n        0x00,\n        0x00, // alternate_group\n        0x00,\n        0x00, // non-audio track volume\n        0x00,\n        0x00, // reserved\n        0x00,\n        0x01,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x01,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x00,\n        0x40,\n        0x00,\n        0x00,\n        0x00, // transformation: unity matrix\n        (width >> 8) & 0xff,\n        width & 0xff,\n        0x00,\n        0x00, // width\n        (height >> 8) & 0xff,\n        height & 0xff,\n        0x00,\n        0x00, // height\n      ]),\n    );\n  }\n\n  static traf(track, baseMediaDecodeTime) {\n    const sampleDependencyTable = MP4.sdtp(track);\n    const id = track.id;\n    const upperWordBaseMediaDecodeTime = Math.floor(\n      baseMediaDecodeTime / (UINT32_MAX + 1),\n    );\n    const lowerWordBaseMediaDecodeTime = Math.floor(\n      baseMediaDecodeTime % (UINT32_MAX + 1),\n    );\n    return MP4.box(\n      MP4.types.traf,\n      MP4.box(\n        MP4.types.tfhd,\n        new Uint8Array([\n          0x00, // version 0\n          0x00,\n          0x00,\n          0x00, // flags\n          id >> 24,\n          (id >> 16) & 0xff,\n          (id >> 8) & 0xff,\n          id & 0xff, // track_ID\n        ]),\n      ),\n      MP4.box(\n        MP4.types.tfdt,\n        new Uint8Array([\n          0x01, // version 1\n          0x00,\n          0x00,\n          0x00, // flags\n          upperWordBaseMediaDecodeTime >> 24,\n          (upperWordBaseMediaDecodeTime >> 16) & 0xff,\n          (upperWordBaseMediaDecodeTime >> 8) & 0xff,\n          upperWordBaseMediaDecodeTime & 0xff,\n          lowerWordBaseMediaDecodeTime >> 24,\n          (lowerWordBaseMediaDecodeTime >> 16) & 0xff,\n          (lowerWordBaseMediaDecodeTime >> 8) & 0xff,\n          lowerWordBaseMediaDecodeTime & 0xff,\n        ]),\n      ),\n      MP4.trun(\n        track,\n        sampleDependencyTable.length +\n          16 + // tfhd\n          20 + // tfdt\n          8 + // traf header\n          16 + // mfhd\n          8 + // moof header\n          8,\n      ), // mdat header\n      sampleDependencyTable,\n    );\n  }\n\n  /**\n   * Generate a track box.\n   * @param track a track definition\n   */\n  static trak(track) {\n    track.duration = track.duration || 0xffffffff;\n    return MP4.box(MP4.types.trak, MP4.tkhd(track), MP4.mdia(track));\n  }\n\n  static trex(track) {\n    const id = track.id;\n    return MP4.box(\n      MP4.types.trex,\n      new Uint8Array([\n        0x00, // version 0\n        0x00,\n        0x00,\n        0x00, // flags\n        id >> 24,\n        (id >> 16) & 0xff,\n        (id >> 8) & 0xff,\n        id & 0xff, // track_ID\n        0x00,\n        0x00,\n        0x00,\n        0x01, // default_sample_description_index\n        0x00,\n        0x00,\n        0x00,\n        0x00, // default_sample_duration\n        0x00,\n        0x00,\n        0x00,\n        0x00, // default_sample_size\n        0x00,\n        0x01,\n        0x00,\n        0x01, // default_sample_flags\n      ]),\n    );\n  }\n\n  static trun(track, offset) {\n    const samples = track.samples || [];\n    const len = samples.length;\n    const arraylen = 12 + 16 * len;\n    const array = new Uint8Array(arraylen);\n    let i;\n    let sample;\n    let duration;\n    let size;\n    let flags;\n    let cts;\n    offset += 8 + arraylen;\n    array.set(\n      [\n        track.type === 'video' ? 0x01 : 0x00, // version 1 for video with signed-int sample_composition_time_offset\n        0x00,\n        0x0f,\n        0x01, // flags\n        (len >>> 24) & 0xff,\n        (len >>> 16) & 0xff,\n        (len >>> 8) & 0xff,\n        len & 0xff, // sample_count\n        (offset >>> 24) & 0xff,\n        (offset >>> 16) & 0xff,\n        (offset >>> 8) & 0xff,\n        offset & 0xff, // data_offset\n      ],\n      0,\n    );\n    for (i = 0; i < len; i++) {\n      sample = samples[i];\n      duration = sample.duration;\n      size = sample.size;\n      flags = sample.flags;\n      cts = sample.cts;\n      array.set(\n        [\n          (duration >>> 24) & 0xff,\n          (duration >>> 16) & 0xff,\n          (duration >>> 8) & 0xff,\n          duration & 0xff, // sample_duration\n          (size >>> 24) & 0xff,\n          (size >>> 16) & 0xff,\n          (size >>> 8) & 0xff,\n          size & 0xff, // sample_size\n          (flags.isLeading << 2) | flags.dependsOn,\n          (flags.isDependedOn << 6) |\n            (flags.hasRedundancy << 4) |\n            (flags.paddingValue << 1) |\n            flags.isNonSync,\n          flags.degradPrio & (0xf0 << 8),\n          flags.degradPrio & 0x0f, // sample_flags\n          (cts >>> 24) & 0xff,\n          (cts >>> 16) & 0xff,\n          (cts >>> 8) & 0xff,\n          cts & 0xff, // sample_composition_time_offset\n        ],\n        12 + 16 * i,\n      );\n    }\n    return MP4.box(MP4.types.trun, array);\n  }\n\n  static initSegment(tracks) {\n    if (!MP4.types) {\n      MP4.init();\n    }\n\n    const movie = MP4.moov(tracks);\n    const result = appendUint8Array(MP4.FTYP, movie);\n    return result;\n  }\n}\n\nexport default MP4;\n", "const MPEG_TS_CLOCK_FREQ_HZ = 90000;\n\nexport type RationalTimestamp = {\n  baseTime: number; // ticks\n  timescale: number; // ticks per second\n};\n\nexport function toTimescaleFromBase(\n  baseTime: number,\n  destScale: number,\n  srcBase: number = 1,\n  round: boolean = false,\n): number {\n  const result = baseTime * destScale * srcBase; // equivalent to `(value * scale) / (1 / base)`\n  return round ? Math.round(result) : result;\n}\n\nexport function toTimescaleFromScale(\n  baseTime: number,\n  destScale: number,\n  srcScale: number = 1,\n  round: boolean = false,\n): number {\n  return toTimescaleFromBase(baseTime, destScale, 1 / srcScale, round);\n}\n\nexport function toMsFromMpegTsClock(\n  baseTime: number,\n  round: boolean = false,\n): number {\n  return toTimescaleFromBase(baseTime, 1000, 1 / MPEG_TS_CLOCK_FREQ_HZ, round);\n}\n\nexport function toMpegTsClockFromTimescale(\n  baseTime: number,\n  srcScale: number = 1,\n): number {\n  return toTimescaleFromBase(baseTime, MPEG_TS_CLOCK_FREQ_HZ, 1 / srcScale);\n}\n", "import AAC from './aac-helper';\nimport MP4 from './mp4-generator';\nimport type { HlsEventEmitter } from '../events';\nimport { Events } from '../events';\nimport { ErrorTypes, ErrorDetails } from '../errors';\nimport { logger } from '../utils/logger';\nimport {\n  InitSegmentData,\n  Remuxer,\n  RemuxerResult,\n  RemuxedMetadata,\n  RemuxedTrack,\n  RemuxedUserdata,\n} from '../types/remuxer';\nimport { PlaylistLevelType } from '../types/loader';\nimport {\n  RationalTimestamp,\n  toMsFromMpegTsClock,\n} from '../utils/timescale-conversion';\nimport type {\n  AudioSample,\n  VideoSample,\n  DemuxedAudioTrack,\n  DemuxedVideoTrack,\n  DemuxedMetadataTrack,\n  DemuxedUserdataTrack,\n} from '../types/demuxer';\nimport type { TrackSet } from '../types/track';\nimport type { SourceBufferName } from '../types/buffer';\nimport type { Fragment } from '../loader/fragment';\nimport type { HlsConfig } from '../config';\n\nconst MAX_SILENT_FRAME_DURATION = 10 * 1000; // 10 seconds\nconst AAC_SAMPLES_PER_FRAME = 1024;\nconst MPEG_AUDIO_SAMPLE_PER_FRAME = 1152;\nconst AC3_SAMPLES_PER_FRAME = 1536;\n\nlet chromeVersion: number | null = null;\nlet safariWebkitVersion: number | null = null;\n\nexport default class MP4Remuxer implements Remuxer {\n  private observer: HlsEventEmitter;\n  private config: HlsConfig;\n  private typeSupported: any;\n  private ISGenerated: boolean = false;\n  private _initPTS: RationalTimestamp | null = null;\n  private _initDTS: RationalTimestamp | null = null;\n  private nextAvcDts: number | null = null;\n  private nextAudioPts: number | null = null;\n  private videoSampleDuration: number | null = null;\n  private isAudioContiguous: boolean = false;\n  private isVideoContiguous: boolean = false;\n  private videoTrackConfig?: {\n    width?: number;\n    height?: number;\n    pixelRatio?: [number, number];\n  };\n\n  constructor(\n    observer: HlsEventEmitter,\n    config: HlsConfig,\n    typeSupported,\n    vendor = '',\n  ) {\n    this.observer = observer;\n    this.config = config;\n    this.typeSupported = typeSupported;\n    this.ISGenerated = false;\n\n    if (chromeVersion === null) {\n      const userAgent = navigator.userAgent || '';\n      const result = userAgent.match(/Chrome\\/(\\d+)/i);\n      chromeVersion = result ? parseInt(result[1]) : 0;\n    }\n    if (safariWebkitVersion === null) {\n      const result = navigator.userAgent.match(/Safari\\/(\\d+)/i);\n      safariWebkitVersion = result ? parseInt(result[1]) : 0;\n    }\n  }\n\n  destroy() {\n    // @ts-ignore\n    this.config = this.videoTrackConfig = this._initPTS = this._initDTS = null;\n  }\n\n  resetTimeStamp(defaultTimeStamp: RationalTimestamp | null) {\n    logger.log('[mp4-remuxer]: initPTS & initDTS reset');\n    this._initPTS = this._initDTS = defaultTimeStamp;\n  }\n\n  resetNextTimestamp() {\n    logger.log('[mp4-remuxer]: reset next timestamp');\n    this.isVideoContiguous = false;\n    this.isAudioContiguous = false;\n  }\n\n  resetInitSegment() {\n    logger.log('[mp4-remuxer]: ISGenerated flag reset');\n    this.ISGenerated = false;\n    this.videoTrackConfig = undefined;\n  }\n\n  getVideoStartPts(videoSamples) {\n    let rolloverDetected = false;\n    const startPTS = videoSamples.reduce((minPTS, sample) => {\n      const delta = sample.pts - minPTS;\n      if (delta < -4294967296) {\n        // 2^32, see PTSNormalize for reasoning, but we're hitting a rollover here, and we don't want that to impact the timeOffset calculation\n        rolloverDetected = true;\n        return normalizePts(minPTS, sample.pts);\n      } else if (delta > 0) {\n        return minPTS;\n      } else {\n        return sample.pts;\n      }\n    }, videoSamples[0].pts);\n    if (rolloverDetected) {\n      logger.debug('PTS rollover detected');\n    }\n    return startPTS;\n  }\n\n  remux(\n    audioTrack: DemuxedAudioTrack,\n    videoTrack: DemuxedVideoTrack,\n    id3Track: DemuxedMetadataTrack,\n    textTrack: DemuxedUserdataTrack,\n    timeOffset: number,\n    accurateTimeOffset: boolean,\n    flush: boolean,\n    playlistType: PlaylistLevelType,\n  ): RemuxerResult {\n    let video: RemuxedTrack | undefined;\n    let audio: RemuxedTrack | undefined;\n    let initSegment: InitSegmentData | undefined;\n    let text: RemuxedUserdata | undefined;\n    let id3: RemuxedMetadata | undefined;\n    let independent: boolean | undefined;\n    let audioTimeOffset = timeOffset;\n    let videoTimeOffset = timeOffset;\n\n    // If we're remuxing audio and video progressively, wait until we've received enough samples for each track before proceeding.\n    // This is done to synchronize the audio and video streams. We know if the current segment will have samples if the \"pid\"\n    // parameter is greater than -1. The pid is set when the PMT is parsed, which contains the tracks list.\n    // However, if the initSegment has already been generated, or we've reached the end of a segment (flush),\n    // then we can remux one track without waiting for the other.\n    const hasAudio = audioTrack.pid > -1;\n    const hasVideo = videoTrack.pid > -1;\n    const length = videoTrack.samples.length;\n    const enoughAudioSamples = audioTrack.samples.length > 0;\n    const enoughVideoSamples = (flush && length > 0) || length > 1;\n    const canRemuxAvc =\n      ((!hasAudio || enoughAudioSamples) &&\n        (!hasVideo || enoughVideoSamples)) ||\n      this.ISGenerated ||\n      flush;\n\n    if (canRemuxAvc) {\n      if (this.ISGenerated) {\n        const config = this.videoTrackConfig;\n        if (\n          config &&\n          (videoTrack.width !== config.width ||\n            videoTrack.height !== config.height ||\n            videoTrack.pixelRatio?.[0] !== config.pixelRatio?.[0] ||\n            videoTrack.pixelRatio?.[1] !== config.pixelRatio?.[1])\n        ) {\n          this.resetInitSegment();\n        }\n      } else {\n        initSegment = this.generateIS(\n          audioTrack,\n          videoTrack,\n          timeOffset,\n          accurateTimeOffset,\n        );\n      }\n\n      const isVideoContiguous = this.isVideoContiguous;\n      let firstKeyFrameIndex = -1;\n      let firstKeyFramePTS;\n\n      if (enoughVideoSamples) {\n        firstKeyFrameIndex = findKeyframeIndex(videoTrack.samples);\n        if (!isVideoContiguous && this.config.forceKeyFrameOnDiscontinuity) {\n          independent = true;\n          if (firstKeyFrameIndex > 0) {\n            logger.warn(\n              `[mp4-remuxer]: Dropped ${firstKeyFrameIndex} out of ${length} video samples due to a missing keyframe`,\n            );\n            const startPTS = this.getVideoStartPts(videoTrack.samples);\n            videoTrack.samples = videoTrack.samples.slice(firstKeyFrameIndex);\n            videoTrack.dropped += firstKeyFrameIndex;\n            videoTimeOffset +=\n              (videoTrack.samples[0].pts - startPTS) /\n              videoTrack.inputTimeScale;\n            firstKeyFramePTS = videoTimeOffset;\n          } else if (firstKeyFrameIndex === -1) {\n            logger.warn(\n              `[mp4-remuxer]: No keyframe found out of ${length} video samples`,\n            );\n            independent = false;\n          }\n        }\n      }\n\n      if (this.ISGenerated) {\n        if (enoughAudioSamples && enoughVideoSamples) {\n          // timeOffset is expected to be the offset of the first timestamp of this fragment (first DTS)\n          // if first audio DTS is not aligned with first video DTS then we need to take that into account\n          // when providing timeOffset to remuxAudio / remuxVideo. if we don't do that, there might be a permanent / small\n          // drift between audio and video streams\n          const startPTS = this.getVideoStartPts(videoTrack.samples);\n          const tsDelta =\n            normalizePts(audioTrack.samples[0].pts, startPTS) - startPTS;\n          const audiovideoTimestampDelta = tsDelta / videoTrack.inputTimeScale;\n          audioTimeOffset += Math.max(0, audiovideoTimestampDelta);\n          videoTimeOffset += Math.max(0, -audiovideoTimestampDelta);\n        }\n\n        // Purposefully remuxing audio before video, so that remuxVideo can use nextAudioPts, which is calculated in remuxAudio.\n        if (enoughAudioSamples) {\n          // if initSegment was generated without audio samples, regenerate it again\n          if (!audioTrack.samplerate) {\n            logger.warn(\n              '[mp4-remuxer]: regenerate InitSegment as audio detected',\n            );\n            initSegment = this.generateIS(\n              audioTrack,\n              videoTrack,\n              timeOffset,\n              accurateTimeOffset,\n            );\n          }\n          audio = this.remuxAudio(\n            audioTrack,\n            audioTimeOffset,\n            this.isAudioContiguous,\n            accurateTimeOffset,\n            hasVideo ||\n              enoughVideoSamples ||\n              playlistType === PlaylistLevelType.AUDIO\n              ? videoTimeOffset\n              : undefined,\n          );\n          if (enoughVideoSamples) {\n            const audioTrackLength = audio ? audio.endPTS - audio.startPTS : 0;\n            // if initSegment was generated without video samples, regenerate it again\n            if (!videoTrack.inputTimeScale) {\n              logger.warn(\n                '[mp4-remuxer]: regenerate InitSegment as video detected',\n              );\n              initSegment = this.generateIS(\n                audioTrack,\n                videoTrack,\n                timeOffset,\n                accurateTimeOffset,\n              );\n            }\n            video = this.remuxVideo(\n              videoTrack,\n              videoTimeOffset,\n              isVideoContiguous,\n              audioTrackLength,\n            );\n          }\n        } else if (enoughVideoSamples) {\n          video = this.remuxVideo(\n            videoTrack,\n            videoTimeOffset,\n            isVideoContiguous,\n            0,\n          );\n        }\n        if (video) {\n          video.firstKeyFrame = firstKeyFrameIndex;\n          video.independent = firstKeyFrameIndex !== -1;\n          video.firstKeyFramePTS = firstKeyFramePTS;\n        }\n      }\n    }\n\n    // Allow ID3 and text to remux, even if more audio/video samples are required\n    if (this.ISGenerated && this._initPTS && this._initDTS) {\n      if (id3Track.samples.length) {\n        id3 = flushTextTrackMetadataCueSamples(\n          id3Track,\n          timeOffset,\n          this._initPTS,\n          this._initDTS,\n        );\n      }\n\n      if (textTrack.samples.length) {\n        text = flushTextTrackUserdataCueSamples(\n          textTrack,\n          timeOffset,\n          this._initPTS,\n        );\n      }\n    }\n\n    return {\n      audio,\n      video,\n      initSegment,\n      independent,\n      text,\n      id3,\n    };\n  }\n\n  generateIS(\n    audioTrack: DemuxedAudioTrack,\n    videoTrack: DemuxedVideoTrack,\n    timeOffset: number,\n    accurateTimeOffset: boolean,\n  ): InitSegmentData | undefined {\n    const audioSamples = audioTrack.samples;\n    const videoSamples = videoTrack.samples;\n    const typeSupported = this.typeSupported;\n    const tracks: TrackSet = {};\n    const _initPTS = this._initPTS;\n    let computePTSDTS = !_initPTS || accurateTimeOffset;\n    let container = 'audio/mp4';\n    let initPTS: number | undefined;\n    let initDTS: number | undefined;\n    let timescale: number | undefined;\n\n    if (computePTSDTS) {\n      initPTS = initDTS = Infinity;\n    }\n\n    if (audioTrack.config && audioSamples.length) {\n      // let's use audio sampling rate as MP4 time scale.\n      // rationale is that there is a integer nb of audio frames per audio sample (1024 for AAC)\n      // using audio sampling rate here helps having an integer MP4 frame duration\n      // this avoids potential rounding issue and AV sync issue\n      audioTrack.timescale = audioTrack.samplerate;\n      switch (audioTrack.segmentCodec) {\n        case 'mp3':\n          if (typeSupported.mpeg) {\n            // Chrome and Safari\n            container = 'audio/mpeg';\n            audioTrack.codec = '';\n          } else if (typeSupported.mp3) {\n            // Firefox\n            audioTrack.codec = 'mp3';\n          }\n          break;\n\n        case 'ac3':\n          audioTrack.codec = 'ac-3';\n          break;\n      }\n      tracks.audio = {\n        id: 'audio',\n        container: container,\n        codec: audioTrack.codec,\n        initSegment:\n          audioTrack.segmentCodec === 'mp3' && typeSupported.mpeg\n            ? new Uint8Array(0)\n            : MP4.initSegment([audioTrack]),\n        metadata: {\n          channelCount: audioTrack.channelCount,\n        },\n      };\n      if (computePTSDTS) {\n        timescale = audioTrack.inputTimeScale;\n        if (!_initPTS || timescale !== _initPTS.timescale) {\n          // remember first PTS of this demuxing context. for audio, PTS = DTS\n          initPTS = initDTS =\n            audioSamples[0].pts - Math.round(timescale * timeOffset);\n        } else {\n          computePTSDTS = false;\n        }\n      }\n    }\n\n    if (videoTrack.sps && videoTrack.pps && videoSamples.length) {\n      // let's use input time scale as MP4 video timescale\n      // we use input time scale straight away to avoid rounding issues on frame duration / cts computation\n      videoTrack.timescale = videoTrack.inputTimeScale;\n      tracks.video = {\n        id: 'main',\n        container: 'video/mp4',\n        codec: videoTrack.codec,\n        initSegment: MP4.initSegment([videoTrack]),\n        metadata: {\n          width: videoTrack.width,\n          height: videoTrack.height,\n        },\n      };\n      if (computePTSDTS) {\n        timescale = videoTrack.inputTimeScale;\n        if (!_initPTS || timescale !== _initPTS.timescale) {\n          const startPTS = this.getVideoStartPts(videoSamples);\n          const startOffset = Math.round(timescale * timeOffset);\n          initDTS = Math.min(\n            initDTS as number,\n            normalizePts(videoSamples[0].dts, startPTS) - startOffset,\n          );\n          initPTS = Math.min(initPTS as number, startPTS - startOffset);\n        } else {\n          computePTSDTS = false;\n        }\n      }\n      this.videoTrackConfig = {\n        width: videoTrack.width,\n        height: videoTrack.height,\n        pixelRatio: videoTrack.pixelRatio,\n      };\n    }\n\n    if (Object.keys(tracks).length) {\n      this.ISGenerated = true;\n      if (computePTSDTS) {\n        this._initPTS = {\n          baseTime: initPTS as number,\n          timescale: timescale as number,\n        };\n        this._initDTS = {\n          baseTime: initDTS as number,\n          timescale: timescale as number,\n        };\n      } else {\n        initPTS = timescale = undefined;\n      }\n\n      return {\n        tracks,\n        initPTS,\n        timescale,\n      };\n    }\n  }\n\n  remuxVideo(\n    track: DemuxedVideoTrack,\n    timeOffset: number,\n    contiguous: boolean,\n    audioTrackLength: number,\n  ): RemuxedTrack | undefined {\n    const timeScale: number = track.inputTimeScale;\n    const inputSamples: Array<VideoSample> = track.samples;\n    const outputSamples: Array<Mp4Sample> = [];\n    const nbSamples = inputSamples.length;\n    const initPTS = this._initPTS as RationalTimestamp;\n    let nextAvcDts = this.nextAvcDts;\n    let offset = 8;\n    let mp4SampleDuration = this.videoSampleDuration;\n    let firstDTS;\n    let lastDTS;\n    let minPTS: number = Number.POSITIVE_INFINITY;\n    let maxPTS: number = Number.NEGATIVE_INFINITY;\n    let sortSamples = false;\n\n    // if parsed fragment is contiguous with last one, let's use last DTS value as reference\n    if (!contiguous || nextAvcDts === null) {\n      const pts = timeOffset * timeScale;\n      const cts =\n        inputSamples[0].pts -\n        normalizePts(inputSamples[0].dts, inputSamples[0].pts);\n      if (\n        chromeVersion &&\n        nextAvcDts !== null &&\n        Math.abs(pts - cts - nextAvcDts) < 15000\n      ) {\n        // treat as contigous to adjust samples that would otherwise produce video buffer gaps in Chrome\n        contiguous = true;\n      } else {\n        // if not contiguous, let's use target timeOffset\n        nextAvcDts = pts - cts;\n      }\n    }\n\n    // PTS is coded on 33bits, and can loop from -2^32 to 2^32\n    // PTSNormalize will make PTS/DTS value monotonic, we use last known DTS value as reference value\n    const initTime = (initPTS.baseTime * timeScale) / initPTS.timescale;\n    for (let i = 0; i < nbSamples; i++) {\n      const sample = inputSamples[i];\n      sample.pts = normalizePts(sample.pts - initTime, nextAvcDts);\n      sample.dts = normalizePts(sample.dts - initTime, nextAvcDts);\n      if (sample.dts < inputSamples[i > 0 ? i - 1 : i].dts) {\n        sortSamples = true;\n      }\n    }\n\n    // sort video samples by DTS then PTS then demux id order\n    if (sortSamples) {\n      inputSamples.sort(function (a, b) {\n        const deltadts = a.dts - b.dts;\n        const deltapts = a.pts - b.pts;\n        return deltadts || deltapts;\n      });\n    }\n\n    // Get first/last DTS\n    firstDTS = inputSamples[0].dts;\n    lastDTS = inputSamples[inputSamples.length - 1].dts;\n\n    // Sample duration (as expected by trun MP4 boxes), should be the delta between sample DTS\n    // set this constant duration as being the avg delta between consecutive DTS.\n    const inputDuration = lastDTS - firstDTS;\n    const averageSampleDuration = inputDuration\n      ? Math.round(inputDuration / (nbSamples - 1))\n      : mp4SampleDuration || track.inputTimeScale / 30;\n\n    // if fragment are contiguous, detect hole/overlapping between fragments\n    if (contiguous) {\n      // check timestamp continuity across consecutive fragments (this is to remove inter-fragment gap/hole)\n      const delta = firstDTS - nextAvcDts;\n      const foundHole = delta > averageSampleDuration;\n      const foundOverlap = delta < -1;\n      if (foundHole || foundOverlap) {\n        if (foundHole) {\n          logger.warn(\n            `AVC: ${toMsFromMpegTsClock(\n              delta,\n              true,\n            )} ms (${delta}dts) hole between fragments detected at ${timeOffset.toFixed(\n              3,\n            )}`,\n          );\n        } else {\n          logger.warn(\n            `AVC: ${toMsFromMpegTsClock(\n              -delta,\n              true,\n            )} ms (${delta}dts) overlapping between fragments detected at ${timeOffset.toFixed(\n              3,\n            )}`,\n          );\n        }\n        if (\n          !foundOverlap ||\n          nextAvcDts >= inputSamples[0].pts ||\n          chromeVersion\n        ) {\n          firstDTS = nextAvcDts;\n          const firstPTS = inputSamples[0].pts - delta;\n          if (foundHole) {\n            inputSamples[0].dts = firstDTS;\n            inputSamples[0].pts = firstPTS;\n          } else {\n            for (let i = 0; i < inputSamples.length; i++) {\n              if (inputSamples[i].dts > firstPTS) {\n                break;\n              }\n              inputSamples[i].dts -= delta;\n              inputSamples[i].pts -= delta;\n            }\n          }\n          logger.log(\n            `Video: Initial PTS/DTS adjusted: ${toMsFromMpegTsClock(\n              firstPTS,\n              true,\n            )}/${toMsFromMpegTsClock(\n              firstDTS,\n              true,\n            )}, delta: ${toMsFromMpegTsClock(delta, true)} ms`,\n          );\n        }\n      }\n    }\n\n    firstDTS = Math.max(0, firstDTS);\n\n    let nbNalu = 0;\n    let naluLen = 0;\n    let dtsStep = firstDTS;\n    for (let i = 0; i < nbSamples; i++) {\n      // compute total/avc sample length and nb of NAL units\n      const sample = inputSamples[i];\n      const units = sample.units;\n      const nbUnits = units.length;\n      let sampleLen = 0;\n      for (let j = 0; j < nbUnits; j++) {\n        sampleLen += units[j].data.length;\n      }\n\n      naluLen += sampleLen;\n      nbNalu += nbUnits;\n      sample.length = sampleLen;\n\n      // ensure sample monotonic DTS\n      if (sample.dts < dtsStep) {\n        sample.dts = dtsStep;\n        dtsStep += (averageSampleDuration / 4) | 0 || 1;\n      } else {\n        dtsStep = sample.dts;\n      }\n\n      minPTS = Math.min(sample.pts, minPTS);\n      maxPTS = Math.max(sample.pts, maxPTS);\n    }\n    lastDTS = inputSamples[nbSamples - 1].dts;\n\n    /* concatenate the video data and construct the mdat in place\n      (need 8 more bytes to fill length and mpdat type) */\n    const mdatSize = naluLen + 4 * nbNalu + 8;\n    let mdat;\n    try {\n      mdat = new Uint8Array(mdatSize);\n    } catch (err) {\n      this.observer.emit(Events.ERROR, Events.ERROR, {\n        type: ErrorTypes.MUX_ERROR,\n        details: ErrorDetails.REMUX_ALLOC_ERROR,\n        fatal: false,\n        error: err,\n        bytes: mdatSize,\n        reason: `fail allocating video mdat ${mdatSize}`,\n      });\n      return;\n    }\n    const view = new DataView(mdat.buffer);\n    view.setUint32(0, mdatSize);\n    mdat.set(MP4.types.mdat, 4);\n\n    let stretchedLastFrame = false;\n    let minDtsDelta = Number.POSITIVE_INFINITY;\n    let minPtsDelta = Number.POSITIVE_INFINITY;\n    let maxDtsDelta = Number.NEGATIVE_INFINITY;\n    let maxPtsDelta = Number.NEGATIVE_INFINITY;\n    for (let i = 0; i < nbSamples; i++) {\n      const VideoSample = inputSamples[i];\n      const VideoSampleUnits = VideoSample.units;\n      let mp4SampleLength = 0;\n      // convert NALU bitstream to MP4 format (prepend NALU with size field)\n      for (let j = 0, nbUnits = VideoSampleUnits.length; j < nbUnits; j++) {\n        const unit = VideoSampleUnits[j];\n        const unitData = unit.data;\n        const unitDataLen = unit.data.byteLength;\n        view.setUint32(offset, unitDataLen);\n        offset += 4;\n        mdat.set(unitData, offset);\n        offset += unitDataLen;\n        mp4SampleLength += 4 + unitDataLen;\n      }\n\n      // expected sample duration is the Decoding Timestamp diff of consecutive samples\n      let ptsDelta;\n      if (i < nbSamples - 1) {\n        mp4SampleDuration = inputSamples[i + 1].dts - VideoSample.dts;\n        ptsDelta = inputSamples[i + 1].pts - VideoSample.pts;\n      } else {\n        const config = this.config;\n        const lastFrameDuration =\n          i > 0\n            ? VideoSample.dts - inputSamples[i - 1].dts\n            : averageSampleDuration;\n        ptsDelta =\n          i > 0\n            ? VideoSample.pts - inputSamples[i - 1].pts\n            : averageSampleDuration;\n        if (config.stretchShortVideoTrack && this.nextAudioPts !== null) {\n          // In some cases, a segment's audio track duration may exceed the video track duration.\n          // Since we've already remuxed audio, and we know how long the audio track is, we look to\n          // see if the delta to the next segment is longer than maxBufferHole.\n          // If so, playback would potentially get stuck, so we artificially inflate\n          // the duration of the last frame to minimize any potential gap between segments.\n          const gapTolerance = Math.floor(config.maxBufferHole * timeScale);\n          const deltaToFrameEnd =\n            (audioTrackLength\n              ? minPTS + audioTrackLength * timeScale\n              : this.nextAudioPts) - VideoSample.pts;\n          if (deltaToFrameEnd > gapTolerance) {\n            // We subtract lastFrameDuration from deltaToFrameEnd to try to prevent any video\n            // frame overlap. maxBufferHole should be >> lastFrameDuration anyway.\n            mp4SampleDuration = deltaToFrameEnd - lastFrameDuration;\n            if (mp4SampleDuration < 0) {\n              mp4SampleDuration = lastFrameDuration;\n            } else {\n              stretchedLastFrame = true;\n            }\n            logger.log(\n              `[mp4-remuxer]: It is approximately ${\n                deltaToFrameEnd / 90\n              } ms to the next segment; using duration ${\n                mp4SampleDuration / 90\n              } ms for the last video frame.`,\n            );\n          } else {\n            mp4SampleDuration = lastFrameDuration;\n          }\n        } else {\n          mp4SampleDuration = lastFrameDuration;\n        }\n      }\n      const compositionTimeOffset = Math.round(\n        VideoSample.pts - VideoSample.dts,\n      );\n      minDtsDelta = Math.min(minDtsDelta, mp4SampleDuration);\n      maxDtsDelta = Math.max(maxDtsDelta, mp4SampleDuration);\n      minPtsDelta = Math.min(minPtsDelta, ptsDelta);\n      maxPtsDelta = Math.max(maxPtsDelta, ptsDelta);\n\n      outputSamples.push(\n        new Mp4Sample(\n          VideoSample.key,\n          mp4SampleDuration,\n          mp4SampleLength,\n          compositionTimeOffset,\n        ),\n      );\n    }\n\n    if (outputSamples.length) {\n      if (chromeVersion) {\n        if (chromeVersion < 70) {\n          // Chrome workaround, mark first sample as being a Random Access Point (keyframe) to avoid sourcebuffer append issue\n          // https://code.google.com/p/chromium/issues/detail?id=229412\n          const flags = outputSamples[0].flags;\n          flags.dependsOn = 2;\n          flags.isNonSync = 0;\n        }\n      } else if (safariWebkitVersion) {\n        // Fix for \"CNN special report, with CC\" in test-streams (Safari browser only)\n        // Ignore DTS when frame durations are irregular. Safari MSE does not handle this leading to gaps.\n        if (\n          maxPtsDelta - minPtsDelta < maxDtsDelta - minDtsDelta &&\n          averageSampleDuration / maxDtsDelta < 0.025 &&\n          outputSamples[0].cts === 0\n        ) {\n          logger.warn(\n            'Found irregular gaps in sample duration. Using PTS instead of DTS to determine MP4 sample duration.',\n          );\n          let dts = firstDTS;\n          for (let i = 0, len = outputSamples.length; i < len; i++) {\n            const nextDts = dts + outputSamples[i].duration;\n            const pts = dts + outputSamples[i].cts;\n            if (i < len - 1) {\n              const nextPts = nextDts + outputSamples[i + 1].cts;\n              outputSamples[i].duration = nextPts - pts;\n            } else {\n              outputSamples[i].duration = i\n                ? outputSamples[i - 1].duration\n                : averageSampleDuration;\n            }\n            outputSamples[i].cts = 0;\n            dts = nextDts;\n          }\n        }\n      }\n    }\n    // next AVC sample DTS should be equal to last sample DTS + last sample duration (in PES timescale)\n    mp4SampleDuration =\n      stretchedLastFrame || !mp4SampleDuration\n        ? averageSampleDuration\n        : mp4SampleDuration;\n    this.nextAvcDts = nextAvcDts = lastDTS + mp4SampleDuration;\n    this.videoSampleDuration = mp4SampleDuration;\n    this.isVideoContiguous = true;\n    const moof = MP4.moof(\n      track.sequenceNumber++,\n      firstDTS,\n      Object.assign({}, track, {\n        samples: outputSamples,\n      }),\n    );\n    const type: SourceBufferName = 'video';\n    const data = {\n      data1: moof,\n      data2: mdat,\n      startPTS: minPTS / timeScale,\n      endPTS: (maxPTS + mp4SampleDuration) / timeScale,\n      startDTS: firstDTS / timeScale,\n      endDTS: (nextAvcDts as number) / timeScale,\n      type,\n      hasAudio: false,\n      hasVideo: true,\n      nb: outputSamples.length,\n      dropped: track.dropped,\n    };\n    track.samples = [];\n    track.dropped = 0;\n    return data;\n  }\n\n  getSamplesPerFrame(track: DemuxedAudioTrack) {\n    switch (track.segmentCodec) {\n      case 'mp3':\n        return MPEG_AUDIO_SAMPLE_PER_FRAME;\n      case 'ac3':\n        return AC3_SAMPLES_PER_FRAME;\n      default:\n        return AAC_SAMPLES_PER_FRAME;\n    }\n  }\n\n  remuxAudio(\n    track: DemuxedAudioTrack,\n    timeOffset: number,\n    contiguous: boolean,\n    accurateTimeOffset: boolean,\n    videoTimeOffset?: number,\n  ): RemuxedTrack | undefined {\n    const inputTimeScale: number = track.inputTimeScale;\n    const mp4timeScale: number = track.samplerate\n      ? track.samplerate\n      : inputTimeScale;\n    const scaleFactor: number = inputTimeScale / mp4timeScale;\n    const mp4SampleDuration: number = this.getSamplesPerFrame(track);\n    const inputSampleDuration: number = mp4SampleDuration * scaleFactor;\n    const initPTS = this._initPTS as RationalTimestamp;\n    const rawMPEG: boolean =\n      track.segmentCodec === 'mp3' && this.typeSupported.mpeg;\n    const outputSamples: Array<Mp4Sample> = [];\n    const alignedWithVideo = videoTimeOffset !== undefined;\n\n    let inputSamples: Array<AudioSample> = track.samples;\n    let offset: number = rawMPEG ? 0 : 8;\n    let nextAudioPts: number = this.nextAudioPts || -1;\n\n    // window.audioSamples ? window.audioSamples.push(inputSamples.map(s => s.pts)) : (window.audioSamples = [inputSamples.map(s => s.pts)]);\n\n    // for audio samples, also consider consecutive fragments as being contiguous (even if a level switch occurs),\n    // for sake of clarity:\n    // consecutive fragments are frags with\n    //  - less than 100ms gaps between new time offset (if accurate) and next expected PTS OR\n    //  - less than 20 audio frames distance\n    // contiguous fragments are consecutive fragments from same quality level (same level, new SN = old SN + 1)\n    // this helps ensuring audio continuity\n    // and this also avoids audio glitches/cut when switching quality, or reporting wrong duration on first audio frame\n    const timeOffsetMpegTS = timeOffset * inputTimeScale;\n    const initTime = (initPTS.baseTime * inputTimeScale) / initPTS.timescale;\n    this.isAudioContiguous = contiguous =\n      contiguous ||\n      ((inputSamples.length &&\n        nextAudioPts > 0 &&\n        ((accurateTimeOffset &&\n          Math.abs(timeOffsetMpegTS - nextAudioPts) < 9000) ||\n          Math.abs(\n            normalizePts(inputSamples[0].pts - initTime, timeOffsetMpegTS) -\n              nextAudioPts,\n          ) <\n            20 * inputSampleDuration)) as boolean);\n\n    // compute normalized PTS\n    inputSamples.forEach(function (sample) {\n      sample.pts = normalizePts(sample.pts - initTime, timeOffsetMpegTS);\n    });\n\n    if (!contiguous || nextAudioPts < 0) {\n      // filter out sample with negative PTS that are not playable anyway\n      // if we don't remove these negative samples, they will shift all audio samples forward.\n      // leading to audio overlap between current / next fragment\n      inputSamples = inputSamples.filter((sample) => sample.pts >= 0);\n\n      // in case all samples have negative PTS, and have been filtered out, return now\n      if (!inputSamples.length) {\n        return;\n      }\n\n      if (videoTimeOffset === 0) {\n        // Set the start to 0 to match video so that start gaps larger than inputSampleDuration are filled with silence\n        nextAudioPts = 0;\n      } else if (accurateTimeOffset && !alignedWithVideo) {\n        // When not seeking, not live, and LevelDetails.PTSKnown, use fragment start as predicted next audio PTS\n        nextAudioPts = Math.max(0, timeOffsetMpegTS);\n      } else {\n        // if frags are not contiguous and if we cant trust time offset, let's use first sample PTS as next audio PTS\n        nextAudioPts = inputSamples[0].pts;\n      }\n    }\n\n    // If the audio track is missing samples, the frames seem to get \"left-shifted\" within the\n    // resulting mp4 segment, causing sync issues and leaving gaps at the end of the audio segment.\n    // In an effort to prevent this from happening, we inject frames here where there are gaps.\n    // When possible, we inject a silent frame; when that's not possible, we duplicate the last\n    // frame.\n\n    if (track.segmentCodec === 'aac') {\n      const maxAudioFramesDrift = this.config.maxAudioFramesDrift;\n      for (let i = 0, nextPts = nextAudioPts; i < inputSamples.length; i++) {\n        // First, let's see how far off this frame is from where we expect it to be\n        const sample = inputSamples[i];\n        const pts = sample.pts;\n        const delta = pts - nextPts;\n        const duration = Math.abs((1000 * delta) / inputTimeScale);\n\n        // When remuxing with video, if we're overlapping by more than a duration, drop this sample to stay in sync\n        if (\n          delta <= -maxAudioFramesDrift * inputSampleDuration &&\n          alignedWithVideo\n        ) {\n          if (i === 0) {\n            logger.warn(\n              `Audio frame @ ${(pts / inputTimeScale).toFixed(\n                3,\n              )}s overlaps nextAudioPts by ${Math.round(\n                (1000 * delta) / inputTimeScale,\n              )} ms.`,\n            );\n            this.nextAudioPts = nextAudioPts = nextPts = pts;\n          }\n        } // eslint-disable-line brace-style\n\n        // Insert missing frames if:\n        // 1: We're more than maxAudioFramesDrift frame away\n        // 2: Not more than MAX_SILENT_FRAME_DURATION away\n        // 3: currentTime (aka nextPtsNorm) is not 0\n        // 4: remuxing with video (videoTimeOffset !== undefined)\n        else if (\n          delta >= maxAudioFramesDrift * inputSampleDuration &&\n          duration < MAX_SILENT_FRAME_DURATION &&\n          alignedWithVideo\n        ) {\n          let missing = Math.round(delta / inputSampleDuration);\n          // Adjust nextPts so that silent samples are aligned with media pts. This will prevent media samples from\n          // later being shifted if nextPts is based on timeOffset and delta is not a multiple of inputSampleDuration.\n          nextPts = pts - missing * inputSampleDuration;\n          if (nextPts < 0) {\n            missing--;\n            nextPts += inputSampleDuration;\n          }\n          if (i === 0) {\n            this.nextAudioPts = nextAudioPts = nextPts;\n          }\n          logger.warn(\n            `[mp4-remuxer]: Injecting ${missing} audio frame @ ${(\n              nextPts / inputTimeScale\n            ).toFixed(3)}s due to ${Math.round(\n              (1000 * delta) / inputTimeScale,\n            )} ms gap.`,\n          );\n          for (let j = 0; j < missing; j++) {\n            const newStamp = Math.max(nextPts as number, 0);\n            let fillFrame = AAC.getSilentFrame(\n              track.manifestCodec || track.codec,\n              track.channelCount,\n            );\n            if (!fillFrame) {\n              logger.log(\n                '[mp4-remuxer]: Unable to get silent frame for given audio codec; duplicating last frame instead.',\n              );\n              fillFrame = sample.unit.subarray();\n            }\n            inputSamples.splice(i, 0, {\n              unit: fillFrame,\n              pts: newStamp,\n            });\n            nextPts += inputSampleDuration;\n            i++;\n          }\n        }\n        sample.pts = nextPts;\n        nextPts += inputSampleDuration;\n      }\n    }\n    let firstPTS: number | null = null;\n    let lastPTS: number | null = null;\n    let mdat: any;\n    let mdatSize: number = 0;\n    let sampleLength: number = inputSamples.length;\n    while (sampleLength--) {\n      mdatSize += inputSamples[sampleLength].unit.byteLength;\n    }\n    for (let j = 0, nbSamples = inputSamples.length; j < nbSamples; j++) {\n      const audioSample = inputSamples[j];\n      const unit = audioSample.unit;\n      let pts = audioSample.pts;\n      if (lastPTS !== null) {\n        // If we have more than one sample, set the duration of the sample to the \"real\" duration; the PTS diff with\n        // the previous sample\n        const prevSample = outputSamples[j - 1];\n        prevSample.duration = Math.round((pts - lastPTS) / scaleFactor);\n      } else {\n        if (contiguous && track.segmentCodec === 'aac') {\n          // set PTS/DTS to expected PTS/DTS\n          pts = nextAudioPts;\n        }\n        // remember first PTS of our audioSamples\n        firstPTS = pts;\n        if (mdatSize > 0) {\n          /* concatenate the audio data and construct the mdat in place\n            (need 8 more bytes to fill length and mdat type) */\n          mdatSize += offset;\n          try {\n            mdat = new Uint8Array(mdatSize);\n          } catch (err) {\n            this.observer.emit(Events.ERROR, Events.ERROR, {\n              type: ErrorTypes.MUX_ERROR,\n              details: ErrorDetails.REMUX_ALLOC_ERROR,\n              fatal: false,\n              error: err,\n              bytes: mdatSize,\n              reason: `fail allocating audio mdat ${mdatSize}`,\n            });\n            return;\n          }\n          if (!rawMPEG) {\n            const view = new DataView(mdat.buffer);\n            view.setUint32(0, mdatSize);\n            mdat.set(MP4.types.mdat, 4);\n          }\n        } else {\n          // no audio samples\n          return;\n        }\n      }\n      mdat.set(unit, offset);\n      const unitLen = unit.byteLength;\n      offset += unitLen;\n      // Default the sample's duration to the computed mp4SampleDuration, which will either be 1024 for AAC or 1152 for MPEG\n      // In the case that we have 1 sample, this will be the duration. If we have more than one sample, the duration\n      // becomes the PTS diff with the previous sample\n      outputSamples.push(new Mp4Sample(true, mp4SampleDuration, unitLen, 0));\n      lastPTS = pts;\n    }\n\n    // We could end up with no audio samples if all input samples were overlapping with the previously remuxed ones\n    const nbSamples = outputSamples.length;\n    if (!nbSamples) {\n      return;\n    }\n\n    // The next audio sample PTS should be equal to last sample PTS + duration\n    const lastSample = outputSamples[outputSamples.length - 1];\n    this.nextAudioPts = nextAudioPts =\n      lastPTS! + scaleFactor * lastSample.duration;\n\n    // Set the track samples from inputSamples to outputSamples before remuxing\n    const moof = rawMPEG\n      ? new Uint8Array(0)\n      : MP4.moof(\n          track.sequenceNumber++,\n          firstPTS! / scaleFactor,\n          Object.assign({}, track, { samples: outputSamples }),\n        );\n\n    // Clear the track samples. This also clears the samples array in the demuxer, since the reference is shared\n    track.samples = [];\n    const start = firstPTS! / inputTimeScale;\n    const end = nextAudioPts / inputTimeScale;\n    const type: SourceBufferName = 'audio';\n    const audioData = {\n      data1: moof,\n      data2: mdat,\n      startPTS: start,\n      endPTS: end,\n      startDTS: start,\n      endDTS: end,\n      type,\n      hasAudio: true,\n      hasVideo: false,\n      nb: nbSamples,\n    };\n\n    this.isAudioContiguous = true;\n    return audioData;\n  }\n\n  remuxEmptyAudio(\n    track: DemuxedAudioTrack,\n    timeOffset: number,\n    contiguous: boolean,\n    videoData: Fragment,\n  ): RemuxedTrack | undefined {\n    const inputTimeScale: number = track.inputTimeScale;\n    const mp4timeScale: number = track.samplerate\n      ? track.samplerate\n      : inputTimeScale;\n    const scaleFactor: number = inputTimeScale / mp4timeScale;\n    const nextAudioPts: number | null = this.nextAudioPts;\n    // sync with video's timestamp\n    const initDTS = this._initDTS as RationalTimestamp;\n    const init90kHz = (initDTS.baseTime * 90000) / initDTS.timescale;\n    const startDTS: number =\n      (nextAudioPts !== null\n        ? nextAudioPts\n        : videoData.startDTS * inputTimeScale) + init90kHz;\n    const endDTS: number = videoData.endDTS * inputTimeScale + init90kHz;\n    // one sample's duration value\n    const frameDuration: number = scaleFactor * AAC_SAMPLES_PER_FRAME;\n    // samples count of this segment's duration\n    const nbSamples: number = Math.ceil((endDTS - startDTS) / frameDuration);\n    // silent frame\n    const silentFrame: Uint8Array | undefined = AAC.getSilentFrame(\n      track.manifestCodec || track.codec,\n      track.channelCount,\n    );\n\n    logger.warn('[mp4-remuxer]: remux empty Audio');\n    // Can't remux if we can't generate a silent frame...\n    if (!silentFrame) {\n      logger.trace(\n        '[mp4-remuxer]: Unable to remuxEmptyAudio since we were unable to get a silent frame for given audio codec',\n      );\n      return;\n    }\n\n    const samples: Array<any> = [];\n    for (let i = 0; i < nbSamples; i++) {\n      const stamp = startDTS + i * frameDuration;\n      samples.push({ unit: silentFrame, pts: stamp, dts: stamp });\n    }\n    track.samples = samples;\n\n    return this.remuxAudio(track, timeOffset, contiguous, false);\n  }\n}\n\nexport function normalizePts(value: number, reference: number | null): number {\n  let offset;\n  if (reference === null) {\n    return value;\n  }\n\n  if (reference < value) {\n    // - 2^33\n    offset = -8589934592;\n  } else {\n    // + 2^33\n    offset = 8589934592;\n  }\n  /* PTS is 33bit (from 0 to 2^33 -1)\n    if diff between value and reference is bigger than half of the amplitude (2^32) then it means that\n    PTS looping occured. fill the gap */\n  while (Math.abs(value - reference) > 4294967296) {\n    value += offset;\n  }\n\n  return value;\n}\n\nfunction findKeyframeIndex(samples: Array<VideoSample>): number {\n  for (let i = 0; i < samples.length; i++) {\n    if (samples[i].key) {\n      return i;\n    }\n  }\n  return -1;\n}\n\nexport function flushTextTrackMetadataCueSamples(\n  track: DemuxedMetadataTrack,\n  timeOffset: number,\n  initPTS: RationalTimestamp,\n  initDTS: RationalTimestamp,\n): RemuxedMetadata | undefined {\n  const length = track.samples.length;\n  if (!length) {\n    return;\n  }\n  const inputTimeScale = track.inputTimeScale;\n  for (let index = 0; index < length; index++) {\n    const sample = track.samples[index];\n    // setting id3 pts, dts to relative time\n    // using this._initPTS and this._initDTS to calculate relative time\n    sample.pts =\n      normalizePts(\n        sample.pts - (initPTS.baseTime * inputTimeScale) / initPTS.timescale,\n        timeOffset * inputTimeScale,\n      ) / inputTimeScale;\n    sample.dts =\n      normalizePts(\n        sample.dts - (initDTS.baseTime * inputTimeScale) / initDTS.timescale,\n        timeOffset * inputTimeScale,\n      ) / inputTimeScale;\n  }\n  const samples = track.samples;\n  track.samples = [];\n  return {\n    samples,\n  };\n}\n\nexport function flushTextTrackUserdataCueSamples(\n  track: DemuxedUserdataTrack,\n  timeOffset: number,\n  initPTS: RationalTimestamp,\n): RemuxedUserdata | undefined {\n  const length = track.samples.length;\n  if (!length) {\n    return;\n  }\n\n  const inputTimeScale = track.inputTimeScale;\n  for (let index = 0; index < length; index++) {\n    const sample = track.samples[index];\n    // setting text pts, dts to relative time\n    // using this._initPTS and this._initDTS to calculate relative time\n    sample.pts =\n      normalizePts(\n        sample.pts - (initPTS.baseTime * inputTimeScale) / initPTS.timescale,\n        timeOffset * inputTimeScale,\n      ) / inputTimeScale;\n  }\n  track.samples.sort((a, b) => a.pts - b.pts);\n  const samples = track.samples;\n  track.samples = [];\n  return {\n    samples,\n  };\n}\n\ntype Mp4SampleFlags = {\n  isLeading: 0;\n  isDependedOn: 0;\n  hasRedundancy: 0;\n  degradPrio: 0;\n  dependsOn: 1 | 2;\n  isNonSync: 0 | 1;\n};\n\nclass Mp4Sample {\n  public size: number;\n  public duration: number;\n  public cts: number;\n  public flags: Mp4SampleFlags;\n\n  constructor(\n    isKeyframe: boolean,\n    duration: number,\n    size: number,\n    cts: number,\n  ) {\n    this.duration = duration;\n    this.size = size;\n    this.cts = cts;\n    this.flags = {\n      isLeading: 0,\n      isDependedOn: 0,\n      hasRedundancy: 0,\n      degradPrio: 0,\n      dependsOn: isKeyframe ? 2 : 1,\n      isNonSync: isKeyframe ? 0 : 1,\n    };\n  }\n}\n", "import {\n  flushTextTrackMetadataCueSamples,\n  flushTextTrackUserdataCueSamples,\n} from './mp4-remuxer';\nimport {\n  InitData,\n  InitDataTrack,\n  patchEncyptionData,\n} from '../utils/mp4-tools';\nimport {\n  getDuration,\n  getStartDTS,\n  offsetStartDTS,\n  parseInitSegment,\n} from '../utils/mp4-tools';\nimport { ElementaryStreamTypes } from '../loader/fragment';\nimport { logger } from '../utils/logger';\nimport { getCodecCompatibleName } from '../utils/codecs';\nimport type { TrackSet } from '../types/track';\nimport type {\n  InitSegmentData,\n  RemuxedTrack,\n  Remuxer,\n  RemuxerResult,\n} from '../types/remuxer';\nimport type {\n  DemuxedAudioTrack,\n  DemuxedMetadataTrack,\n  DemuxedUserdataTrack,\n  PassthroughTrack,\n} from '../types/demuxer';\nimport type { DecryptData } from '../loader/level-key';\nimport type { RationalTimestamp } from '../utils/timescale-conversion';\n\nclass PassThroughRemuxer implements Remuxer {\n  private emitInitSegment: boolean = false;\n  private audioCodec?: string;\n  private videoCodec?: string;\n  private initData?: InitData;\n  private initPTS: RationalTimestamp | null = null;\n  private initTracks?: TrackSet;\n  private lastEndTime: number | null = null;\n\n  public destroy() {}\n\n  public resetTimeStamp(defaultInitPTS: RationalTimestamp | null) {\n    this.initPTS = defaultInitPTS;\n    this.lastEndTime = null;\n  }\n\n  public resetNextTimestamp() {\n    this.lastEndTime = null;\n  }\n\n  public resetInitSegment(\n    initSegment: Uint8Array | undefined,\n    audioCodec: string | undefined,\n    videoCodec: string | undefined,\n    decryptdata: DecryptData | null,\n  ) {\n    this.audioCodec = audioCodec;\n    this.videoCodec = videoCodec;\n    this.generateInitSegment(patchEncyptionData(initSegment, decryptdata));\n    this.emitInitSegment = true;\n  }\n\n  private generateInitSegment(initSegment: Uint8Array | undefined): void {\n    let { audioCodec, videoCodec } = this;\n    if (!initSegment?.byteLength) {\n      this.initTracks = undefined;\n      this.initData = undefined;\n      return;\n    }\n    const initData = (this.initData = parseInitSegment(initSegment));\n\n    // Get codec from initSegment or fallback to default\n    if (initData.audio) {\n      audioCodec = getParsedTrackCodec(\n        initData.audio,\n        ElementaryStreamTypes.AUDIO,\n      );\n    }\n\n    if (initData.video) {\n      videoCodec = getParsedTrackCodec(\n        initData.video,\n        ElementaryStreamTypes.VIDEO,\n      );\n    }\n\n    const tracks: TrackSet = {};\n    if (initData.audio && initData.video) {\n      tracks.audiovideo = {\n        container: 'video/mp4',\n        codec: audioCodec + ',' + videoCodec,\n        initSegment,\n        id: 'main',\n      };\n    } else if (initData.audio) {\n      tracks.audio = {\n        container: 'audio/mp4',\n        codec: audioCodec,\n        initSegment,\n        id: 'audio',\n      };\n    } else if (initData.video) {\n      tracks.video = {\n        container: 'video/mp4',\n        codec: videoCodec,\n        initSegment,\n        id: 'main',\n      };\n    } else {\n      logger.warn(\n        '[passthrough-remuxer.ts]: initSegment does not contain moov or trak boxes.',\n      );\n    }\n    this.initTracks = tracks;\n  }\n\n  public remux(\n    audioTrack: DemuxedAudioTrack,\n    videoTrack: PassthroughTrack,\n    id3Track: DemuxedMetadataTrack,\n    textTrack: DemuxedUserdataTrack,\n    timeOffset: number,\n    accurateTimeOffset: boolean,\n  ): RemuxerResult {\n    let { initPTS, lastEndTime } = this;\n    const result: RemuxerResult = {\n      audio: undefined,\n      video: undefined,\n      text: textTrack,\n      id3: id3Track,\n      initSegment: undefined,\n    };\n\n    // If we haven't yet set a lastEndDTS, or it was reset, set it to the provided timeOffset. We want to use the\n    // lastEndDTS over timeOffset whenever possible; during progressive playback, the media source will not update\n    // the media duration (which is what timeOffset is provided as) before we need to process the next chunk.\n    if (!Number.isFinite(lastEndTime!)) {\n      lastEndTime = this.lastEndTime = timeOffset || 0;\n    }\n\n    // The binary segment data is added to the videoTrack in the mp4demuxer. We don't check to see if the data is only\n    // audio or video (or both); adding it to video was an arbitrary choice.\n    const data = videoTrack.samples;\n    if (!data?.length) {\n      return result;\n    }\n\n    const initSegment: InitSegmentData = {\n      initPTS: undefined,\n      timescale: 1,\n    };\n    let initData = this.initData;\n    if (!initData?.length) {\n      this.generateInitSegment(data);\n      initData = this.initData;\n    }\n    if (!initData?.length) {\n      // We can't remux if the initSegment could not be generated\n      logger.warn('[passthrough-remuxer.ts]: Failed to generate initSegment.');\n      return result;\n    }\n    if (this.emitInitSegment) {\n      initSegment.tracks = this.initTracks as TrackSet;\n      this.emitInitSegment = false;\n    }\n\n    const duration = getDuration(data, initData);\n    const startDTS = getStartDTS(initData, data);\n    const decodeTime = startDTS === null ? timeOffset : startDTS;\n    if (\n      isInvalidInitPts(initPTS, decodeTime, timeOffset, duration) ||\n      (initSegment.timescale !== initPTS.timescale && accurateTimeOffset)\n    ) {\n      initSegment.initPTS = decodeTime - timeOffset;\n      if (initPTS && initPTS.timescale === 1) {\n        logger.warn(\n          `Adjusting initPTS by ${initSegment.initPTS - initPTS.baseTime}`,\n        );\n      }\n      this.initPTS = initPTS = {\n        baseTime: initSegment.initPTS,\n        timescale: 1,\n      };\n    }\n\n    const startTime = audioTrack\n      ? decodeTime - initPTS.baseTime / initPTS.timescale\n      : (lastEndTime as number);\n    const endTime = startTime + duration;\n    offsetStartDTS(initData, data, initPTS.baseTime / initPTS.timescale);\n\n    if (duration > 0) {\n      this.lastEndTime = endTime;\n    } else {\n      logger.warn('Duration parsed from mp4 should be greater than zero');\n      this.resetNextTimestamp();\n    }\n\n    const hasAudio = !!initData.audio;\n    const hasVideo = !!initData.video;\n\n    let type: any = '';\n    if (hasAudio) {\n      type += 'audio';\n    }\n\n    if (hasVideo) {\n      type += 'video';\n    }\n\n    const track: RemuxedTrack = {\n      data1: data,\n      startPTS: startTime,\n      startDTS: startTime,\n      endPTS: endTime,\n      endDTS: endTime,\n      type,\n      hasAudio,\n      hasVideo,\n      nb: 1,\n      dropped: 0,\n    };\n\n    result.audio = track.type === 'audio' ? track : undefined;\n    result.video = track.type !== 'audio' ? track : undefined;\n    result.initSegment = initSegment;\n    result.id3 = flushTextTrackMetadataCueSamples(\n      id3Track,\n      timeOffset,\n      initPTS,\n      initPTS,\n    );\n\n    if (textTrack.samples.length) {\n      result.text = flushTextTrackUserdataCueSamples(\n        textTrack,\n        timeOffset,\n        initPTS,\n      );\n    }\n\n    return result;\n  }\n}\n\nfunction isInvalidInitPts(\n  initPTS: RationalTimestamp | null,\n  startDTS: number,\n  timeOffset: number,\n  duration: number,\n): initPTS is null {\n  if (initPTS === null) {\n    return true;\n  }\n  // InitPTS is invalid when distance from program would be more than segment duration or a minimum of one second\n  const minDuration = Math.max(duration, 1);\n  const startTime = startDTS - initPTS.baseTime / initPTS.timescale;\n  return Math.abs(startTime - timeOffset) > minDuration;\n}\n\nfunction getParsedTrackCodec(\n  track: InitDataTrack,\n  type: ElementaryStreamTypes.AUDIO | ElementaryStreamTypes.VIDEO,\n): string {\n  const parsedCodec = track?.codec;\n  if (parsedCodec && parsedCodec.length > 4) {\n    return parsedCodec;\n  }\n  if (type === ElementaryStreamTypes.AUDIO) {\n    if (\n      parsedCodec === 'ec-3' ||\n      parsedCodec === 'ac-3' ||\n      parsedCodec === 'alac'\n    ) {\n      return parsedCodec;\n    }\n    if (parsedCodec === 'fLaC' || parsedCodec === 'Opus') {\n      // Opting not to get `preferManagedMediaSource` from player config for isSupported() check for simplicity\n      const preferManagedMediaSource = false;\n      return getCodecCompatibleName(parsedCodec, preferManagedMediaSource);\n    }\n    const result = 'mp4a.40.5';\n    logger.info(\n      `Parsed audio codec \"${parsedCodec}\" or audio object type not handled. Using \"${result}\"`,\n    );\n    return result;\n  }\n  // Provide defaults based on codec type\n  // This allows for some playback of some fmp4 playlists without CODECS defined in manifest\n  logger.warn(`Unhandled video codec \"${parsedCodec}\"`);\n  if (parsedCodec === 'hvc1' || parsedCodec === 'hev1') {\n    return 'hvc1.1.6.L120.90';\n  }\n  if (parsedCodec === 'av01') {\n    return 'av01.0.04M.08';\n  }\n  return 'avc1.42e01e';\n}\nexport default PassThroughRemuxer;\n", "import type { HlsEventEmitter } from '../events';\nimport { Events } from '../events';\nimport { ErrorTypes, ErrorDetails } from '../errors';\nimport Decrypter from '../crypt/decrypter';\nimport AACDemuxer from './audio/aacdemuxer';\nimport MP4Demuxer from '../demux/mp4demuxer';\nimport TSDemuxer, { TypeSupported } from '../demux/tsdemuxer';\nimport MP3Demuxer from './audio/mp3demuxer';\nimport { AC3Demuxer } from './audio/ac3-demuxer';\nimport MP4Remuxer from '../remux/mp4-remuxer';\nimport PassThroughRemuxer from '../remux/passthrough-remuxer';\nimport { logger } from '../utils/logger';\nimport type { Demuxer, DemuxerResult, KeyData } from '../types/demuxer';\nimport type { Remuxer } from '../types/remuxer';\nimport type { TransmuxerResult, ChunkMetadata } from '../types/transmuxer';\nimport type { HlsConfig } from '../config';\nimport type { DecryptData } from '../loader/level-key';\nimport type { PlaylistLevelType } from '../types/loader';\nimport type { RationalTimestamp } from '../utils/timescale-conversion';\nimport { optionalSelf } from '../utils/global';\n\nlet now;\n// performance.now() not available on WebWorker, at least on Safari Desktop\ntry {\n  now = self.performance.now.bind(self.performance);\n} catch (err) {\n  logger.debug('Unable to use Performance API on this environment');\n  now = optionalSelf?.Date.now;\n}\n\ntype MuxConfig =\n  | { demux: typeof MP4Demuxer; remux: typeof PassThroughRemuxer }\n  | { demux: typeof TSDemuxer; remux: typeof MP4Remuxer }\n  | { demux: typeof AC3Demuxer; remux: typeof MP4Remuxer }\n  | { demux: typeof AACDemuxer; remux: typeof MP4Remuxer }\n  | { demux: typeof MP3Demuxer; remux: typeof MP4Remuxer };\n\nconst muxConfig: MuxConfig[] = [\n  { demux: MP4Demuxer, remux: PassThroughRemuxer },\n  { demux: TSDemuxer, remux: MP4Remuxer },\n  { demux: AACDemuxer, remux: MP4Remuxer },\n  { demux: MP3Demuxer, remux: MP4Remuxer },\n];\n\nif (__USE_M2TS_ADVANCED_CODECS__) {\n  muxConfig.splice(2, 0, { demux: AC3Demuxer, remux: MP4Remuxer });\n}\n\nexport default class Transmuxer {\n  public async: boolean = false;\n  private observer: HlsEventEmitter;\n  private typeSupported: TypeSupported;\n  private config: HlsConfig;\n  private vendor: string;\n  private id: PlaylistLevelType;\n  private demuxer?: Demuxer;\n  private remuxer?: Remuxer;\n  private decrypter?: Decrypter;\n  private probe!: Function;\n  private decryptionPromise: Promise<TransmuxerResult> | null = null;\n  private transmuxConfig!: TransmuxConfig;\n  private currentTransmuxState!: TransmuxState;\n\n  constructor(\n    observer: HlsEventEmitter,\n    typeSupported: TypeSupported,\n    config: HlsConfig,\n    vendor: string,\n    id: PlaylistLevelType,\n  ) {\n    this.observer = observer;\n    this.typeSupported = typeSupported;\n    this.config = config;\n    this.vendor = vendor;\n    this.id = id;\n  }\n\n  configure(transmuxConfig: TransmuxConfig) {\n    this.transmuxConfig = transmuxConfig;\n    if (this.decrypter) {\n      this.decrypter.reset();\n    }\n  }\n\n  push(\n    data: ArrayBuffer,\n    decryptdata: DecryptData | null,\n    chunkMeta: ChunkMetadata,\n    state?: TransmuxState,\n  ): TransmuxerResult | Promise<TransmuxerResult> {\n    const stats = chunkMeta.transmuxing;\n    stats.executeStart = now();\n\n    let uintData: Uint8Array = new Uint8Array(data);\n    const { currentTransmuxState, transmuxConfig } = this;\n    if (state) {\n      this.currentTransmuxState = state;\n    }\n\n    const {\n      contiguous,\n      discontinuity,\n      trackSwitch,\n      accurateTimeOffset,\n      timeOffset,\n      initSegmentChange,\n    } = state || currentTransmuxState;\n    const {\n      audioCodec,\n      videoCodec,\n      defaultInitPts,\n      duration,\n      initSegmentData,\n    } = transmuxConfig;\n\n    const keyData = getEncryptionType(uintData, decryptdata);\n    if (keyData && keyData.method === 'AES-128') {\n      const decrypter = this.getDecrypter();\n      // Software decryption is synchronous; webCrypto is not\n      if (decrypter.isSync()) {\n        // Software decryption is progressive. Progressive decryption may not return a result on each call. Any cached\n        // data is handled in the flush() call\n        let decryptedData = decrypter.softwareDecrypt(\n          uintData,\n          keyData.key.buffer,\n          keyData.iv.buffer,\n        );\n        // For Low-Latency HLS Parts, decrypt in place, since part parsing is expected on push progress\n        const loadingParts = chunkMeta.part > -1;\n        if (loadingParts) {\n          decryptedData = decrypter.flush();\n        }\n        if (!decryptedData) {\n          stats.executeEnd = now();\n          return emptyResult(chunkMeta);\n        }\n        uintData = new Uint8Array(decryptedData);\n      } else {\n        this.decryptionPromise = decrypter\n          .webCryptoDecrypt(uintData, keyData.key.buffer, keyData.iv.buffer)\n          .then((decryptedData): TransmuxerResult => {\n            // Calling push here is important; if flush() is called while this is still resolving, this ensures that\n            // the decrypted data has been transmuxed\n            const result = this.push(\n              decryptedData,\n              null,\n              chunkMeta,\n            ) as TransmuxerResult;\n            this.decryptionPromise = null;\n            return result;\n          });\n        return this.decryptionPromise!;\n      }\n    }\n\n    const resetMuxers = this.needsProbing(discontinuity, trackSwitch);\n    if (resetMuxers) {\n      const error = this.configureTransmuxer(uintData);\n      if (error) {\n        logger.warn(`[transmuxer] ${error.message}`);\n        this.observer.emit(Events.ERROR, Events.ERROR, {\n          type: ErrorTypes.MEDIA_ERROR,\n          details: ErrorDetails.FRAG_PARSING_ERROR,\n          fatal: false,\n          error,\n          reason: error.message,\n        });\n        stats.executeEnd = now();\n        return emptyResult(chunkMeta);\n      }\n    }\n\n    if (discontinuity || trackSwitch || initSegmentChange || resetMuxers) {\n      this.resetInitSegment(\n        initSegmentData,\n        audioCodec,\n        videoCodec,\n        duration,\n        decryptdata,\n      );\n    }\n\n    if (discontinuity || initSegmentChange || resetMuxers) {\n      this.resetInitialTimestamp(defaultInitPts);\n    }\n\n    if (!contiguous) {\n      this.resetContiguity();\n    }\n\n    const result = this.transmux(\n      uintData,\n      keyData,\n      timeOffset,\n      accurateTimeOffset,\n      chunkMeta,\n    );\n    const currentState = this.currentTransmuxState;\n\n    currentState.contiguous = true;\n    currentState.discontinuity = false;\n    currentState.trackSwitch = false;\n\n    stats.executeEnd = now();\n    return result;\n  }\n\n  // Due to data caching, flush calls can produce more than one TransmuxerResult (hence the Array type)\n  flush(\n    chunkMeta: ChunkMetadata,\n  ): TransmuxerResult[] | Promise<TransmuxerResult[]> {\n    const stats = chunkMeta.transmuxing;\n    stats.executeStart = now();\n\n    const { decrypter, currentTransmuxState, decryptionPromise } = this;\n\n    if (decryptionPromise) {\n      // Upon resolution, the decryption promise calls push() and returns its TransmuxerResult up the stack. Therefore\n      // only flushing is required for async decryption\n      return decryptionPromise.then(() => {\n        return this.flush(chunkMeta);\n      });\n    }\n\n    const transmuxResults: TransmuxerResult[] = [];\n    const { timeOffset } = currentTransmuxState;\n    if (decrypter) {\n      // The decrypter may have data cached, which needs to be demuxed. In this case we'll have two TransmuxResults\n      // This happens in the case that we receive only 1 push call for a segment (either for non-progressive downloads,\n      // or for progressive downloads with small segments)\n      const decryptedData = decrypter.flush();\n      if (decryptedData) {\n        // Push always returns a TransmuxerResult if decryptdata is null\n        transmuxResults.push(\n          this.push(decryptedData, null, chunkMeta) as TransmuxerResult,\n        );\n      }\n    }\n\n    const { demuxer, remuxer } = this;\n    if (!demuxer || !remuxer) {\n      // If probing failed, then Hls.js has been given content its not able to handle\n      stats.executeEnd = now();\n      return [emptyResult(chunkMeta)];\n    }\n\n    const demuxResultOrPromise = demuxer.flush(timeOffset);\n    if (isPromise(demuxResultOrPromise)) {\n      // Decrypt final SAMPLE-AES samples\n      return demuxResultOrPromise.then((demuxResult) => {\n        this.flushRemux(transmuxResults, demuxResult, chunkMeta);\n        return transmuxResults;\n      });\n    }\n\n    this.flushRemux(transmuxResults, demuxResultOrPromise, chunkMeta);\n    return transmuxResults;\n  }\n\n  private flushRemux(\n    transmuxResults: TransmuxerResult[],\n    demuxResult: DemuxerResult,\n    chunkMeta: ChunkMetadata,\n  ) {\n    const { audioTrack, videoTrack, id3Track, textTrack } = demuxResult;\n    const { accurateTimeOffset, timeOffset } = this.currentTransmuxState;\n    logger.log(\n      `[transmuxer.ts]: Flushed fragment ${chunkMeta.sn}${\n        chunkMeta.part > -1 ? ' p: ' + chunkMeta.part : ''\n      } of level ${chunkMeta.level}`,\n    );\n    const remuxResult = this.remuxer!.remux(\n      audioTrack,\n      videoTrack,\n      id3Track,\n      textTrack,\n      timeOffset,\n      accurateTimeOffset,\n      true,\n      this.id,\n    );\n    transmuxResults.push({\n      remuxResult,\n      chunkMeta,\n    });\n\n    chunkMeta.transmuxing.executeEnd = now();\n  }\n\n  resetInitialTimestamp(defaultInitPts: RationalTimestamp | null) {\n    const { demuxer, remuxer } = this;\n    if (!demuxer || !remuxer) {\n      return;\n    }\n    demuxer.resetTimeStamp(defaultInitPts);\n    remuxer.resetTimeStamp(defaultInitPts);\n  }\n\n  resetContiguity() {\n    const { demuxer, remuxer } = this;\n    if (!demuxer || !remuxer) {\n      return;\n    }\n    demuxer.resetContiguity();\n    remuxer.resetNextTimestamp();\n  }\n\n  resetInitSegment(\n    initSegmentData: Uint8Array | undefined,\n    audioCodec: string | undefined,\n    videoCodec: string | undefined,\n    trackDuration: number,\n    decryptdata: DecryptData | null,\n  ) {\n    const { demuxer, remuxer } = this;\n    if (!demuxer || !remuxer) {\n      return;\n    }\n    demuxer.resetInitSegment(\n      initSegmentData,\n      audioCodec,\n      videoCodec,\n      trackDuration,\n    );\n    remuxer.resetInitSegment(\n      initSegmentData,\n      audioCodec,\n      videoCodec,\n      decryptdata,\n    );\n  }\n\n  destroy(): void {\n    if (this.demuxer) {\n      this.demuxer.destroy();\n      this.demuxer = undefined;\n    }\n    if (this.remuxer) {\n      this.remuxer.destroy();\n      this.remuxer = undefined;\n    }\n  }\n\n  private transmux(\n    data: Uint8Array,\n    keyData: KeyData | null,\n    timeOffset: number,\n    accurateTimeOffset: boolean,\n    chunkMeta: ChunkMetadata,\n  ): TransmuxerResult | Promise<TransmuxerResult> {\n    let result: TransmuxerResult | Promise<TransmuxerResult>;\n    if (keyData && keyData.method === 'SAMPLE-AES') {\n      result = this.transmuxSampleAes(\n        data,\n        keyData,\n        timeOffset,\n        accurateTimeOffset,\n        chunkMeta,\n      );\n    } else {\n      result = this.transmuxUnencrypted(\n        data,\n        timeOffset,\n        accurateTimeOffset,\n        chunkMeta,\n      );\n    }\n    return result;\n  }\n\n  private transmuxUnencrypted(\n    data: Uint8Array,\n    timeOffset: number,\n    accurateTimeOffset: boolean,\n    chunkMeta: ChunkMetadata,\n  ): TransmuxerResult {\n    const { audioTrack, videoTrack, id3Track, textTrack } = (\n      this.demuxer as Demuxer\n    ).demux(data, timeOffset, false, !this.config.progressive);\n    const remuxResult = this.remuxer!.remux(\n      audioTrack,\n      videoTrack,\n      id3Track,\n      textTrack,\n      timeOffset,\n      accurateTimeOffset,\n      false,\n      this.id,\n    );\n    return {\n      remuxResult,\n      chunkMeta,\n    };\n  }\n\n  private transmuxSampleAes(\n    data: Uint8Array,\n    decryptData: KeyData,\n    timeOffset: number,\n    accurateTimeOffset: boolean,\n    chunkMeta: ChunkMetadata,\n  ): Promise<TransmuxerResult> {\n    return (this.demuxer as Demuxer)\n      .demuxSampleAes(data, decryptData, timeOffset)\n      .then((demuxResult) => {\n        const remuxResult = this.remuxer!.remux(\n          demuxResult.audioTrack,\n          demuxResult.videoTrack,\n          demuxResult.id3Track,\n          demuxResult.textTrack,\n          timeOffset,\n          accurateTimeOffset,\n          false,\n          this.id,\n        );\n        return {\n          remuxResult,\n          chunkMeta,\n        };\n      });\n  }\n\n  private configureTransmuxer(data: Uint8Array): void | Error {\n    const { config, observer, typeSupported, vendor } = this;\n    // probe for content type\n    let mux;\n    for (let i = 0, len = muxConfig.length; i < len; i++) {\n      if (muxConfig[i].demux?.probe(data)) {\n        mux = muxConfig[i];\n        break;\n      }\n    }\n    if (!mux) {\n      return new Error('Failed to find demuxer by probing fragment data');\n    }\n    // so let's check that current remuxer and demuxer are still valid\n    const demuxer = this.demuxer;\n    const remuxer = this.remuxer;\n    const Remuxer: MuxConfig['remux'] = mux.remux;\n    const Demuxer: MuxConfig['demux'] = mux.demux;\n    if (!remuxer || !(remuxer instanceof Remuxer)) {\n      this.remuxer = new Remuxer(observer, config, typeSupported, vendor);\n    }\n    if (!demuxer || !(demuxer instanceof Demuxer)) {\n      this.demuxer = new Demuxer(observer, config, typeSupported);\n      this.probe = Demuxer.probe;\n    }\n  }\n\n  private needsProbing(discontinuity: boolean, trackSwitch: boolean): boolean {\n    // in case of continuity change, or track switch\n    // we might switch from content type (AAC container to TS container, or TS to fmp4 for example)\n    return !this.demuxer || !this.remuxer || discontinuity || trackSwitch;\n  }\n\n  private getDecrypter(): Decrypter {\n    let decrypter = this.decrypter;\n    if (!decrypter) {\n      decrypter = this.decrypter = new Decrypter(this.config);\n    }\n    return decrypter;\n  }\n}\n\nfunction getEncryptionType(\n  data: Uint8Array,\n  decryptData: DecryptData | null,\n): KeyData | null {\n  let encryptionType: KeyData | null = null;\n  if (\n    data.byteLength > 0 &&\n    decryptData?.key != null &&\n    decryptData.iv !== null &&\n    decryptData.method != null\n  ) {\n    encryptionType = decryptData as KeyData;\n  }\n  return encryptionType;\n}\n\nconst emptyResult = (chunkMeta): TransmuxerResult => ({\n  remuxResult: {},\n  chunkMeta,\n});\n\nexport function isPromise<T>(p: Promise<T> | any): p is Promise<T> {\n  return 'then' in p && p.then instanceof Function;\n}\n\nexport class TransmuxConfig {\n  public audioCodec?: string;\n  public videoCodec?: string;\n  public initSegmentData?: Uint8Array;\n  public duration: number;\n  public defaultInitPts: RationalTimestamp | null;\n\n  constructor(\n    audioCodec: string | undefined,\n    videoCodec: string | undefined,\n    initSegmentData: Uint8Array | undefined,\n    duration: number,\n    defaultInitPts?: RationalTimestamp,\n  ) {\n    this.audioCodec = audioCodec;\n    this.videoCodec = videoCodec;\n    this.initSegmentData = initSegmentData;\n    this.duration = duration;\n    this.defaultInitPts = defaultInitPts || null;\n  }\n}\n\nexport class TransmuxState {\n  public discontinuity: boolean;\n  public contiguous: boolean;\n  public accurateTimeOffset: boolean;\n  public trackSwitch: boolean;\n  public timeOffset: number;\n  public initSegmentChange: boolean;\n\n  constructor(\n    discontinuity: boolean,\n    contiguous: boolean,\n    accurateTimeOffset: boolean,\n    trackSwitch: boolean,\n    timeOffset: number,\n    initSegmentChange: boolean,\n  ) {\n    this.discontinuity = discontinuity;\n    this.contiguous = contiguous;\n    this.accurateTimeOffset = accurateTimeOffset;\n    this.trackSwitch = trackSwitch;\n    this.timeOffset = timeOffset;\n    this.initSegmentChange = initSegmentChange;\n  }\n}\n", "'use strict';\n\nvar has = Object.prototype.hasOwnProperty\n  , prefix = '~';\n\n/**\n * Constructor to create a storage for our `EE` objects.\n * An `Events` instance is a plain object whose properties are event names.\n *\n * @constructor\n * @private\n */\nfunction Events() {}\n\n//\n// We try to not inherit from `Object.prototype`. In some engines creating an\n// instance in this way is faster than calling `Object.create(null)` directly.\n// If `Object.create(null)` is not supported we prefix the event names with a\n// character to make sure that the built-in object properties are not\n// overridden or used as an attack vector.\n//\nif (Object.create) {\n  Events.prototype = Object.create(null);\n\n  //\n  // This hack is needed because the `__proto__` property is still inherited in\n  // some old browsers like Android 4, iPhone 5.1, Opera 11 and Safari 5.\n  //\n  if (!new Events().__proto__) prefix = false;\n}\n\n/**\n * Representation of a single event listener.\n *\n * @param {Function} fn The listener function.\n * @param {*} context The context to invoke the listener with.\n * @param {Boolean} [once=false] Specify if the listener is a one-time listener.\n * @constructor\n * @private\n */\nfunction EE(fn, context, once) {\n  this.fn = fn;\n  this.context = context;\n  this.once = once || false;\n}\n\n/**\n * Add a listener for a given event.\n *\n * @param {EventEmitter} emitter Reference to the `EventEmitter` instance.\n * @param {(String|Symbol)} event The event name.\n * @param {Function} fn The listener function.\n * @param {*} context The context to invoke the listener with.\n * @param {Boolean} once Specify if the listener is a one-time listener.\n * @returns {EventEmitter}\n * @private\n */\nfunction addListener(emitter, event, fn, context, once) {\n  if (typeof fn !== 'function') {\n    throw new TypeError('The listener must be a function');\n  }\n\n  var listener = new EE(fn, context || emitter, once)\n    , evt = prefix ? prefix + event : event;\n\n  if (!emitter._events[evt]) emitter._events[evt] = listener, emitter._eventsCount++;\n  else if (!emitter._events[evt].fn) emitter._events[evt].push(listener);\n  else emitter._events[evt] = [emitter._events[evt], listener];\n\n  return emitter;\n}\n\n/**\n * Clear event by name.\n *\n * @param {EventEmitter} emitter Reference to the `EventEmitter` instance.\n * @param {(String|Symbol)} evt The Event name.\n * @private\n */\nfunction clearEvent(emitter, evt) {\n  if (--emitter._eventsCount === 0) emitter._events = new Events();\n  else delete emitter._events[evt];\n}\n\n/**\n * Minimal `EventEmitter` interface that is molded against the Node.js\n * `EventEmitter` interface.\n *\n * @constructor\n * @public\n */\nfunction EventEmitter() {\n  this._events = new Events();\n  this._eventsCount = 0;\n}\n\n/**\n * Return an array listing the events for which the emitter has registered\n * listeners.\n *\n * @returns {Array}\n * @public\n */\nEventEmitter.prototype.eventNames = function eventNames() {\n  var names = []\n    , events\n    , name;\n\n  if (this._eventsCount === 0) return names;\n\n  for (name in (events = this._events)) {\n    if (has.call(events, name)) names.push(prefix ? name.slice(1) : name);\n  }\n\n  if (Object.getOwnPropertySymbols) {\n    return names.concat(Object.getOwnPropertySymbols(events));\n  }\n\n  return names;\n};\n\n/**\n * Return the listeners registered for a given event.\n *\n * @param {(String|Symbol)} event The event name.\n * @returns {Array} The registered listeners.\n * @public\n */\nEventEmitter.prototype.listeners = function listeners(event) {\n  var evt = prefix ? prefix + event : event\n    , handlers = this._events[evt];\n\n  if (!handlers) return [];\n  if (handlers.fn) return [handlers.fn];\n\n  for (var i = 0, l = handlers.length, ee = new Array(l); i < l; i++) {\n    ee[i] = handlers[i].fn;\n  }\n\n  return ee;\n};\n\n/**\n * Return the number of listeners listening to a given event.\n *\n * @param {(String|Symbol)} event The event name.\n * @returns {Number} The number of listeners.\n * @public\n */\nEventEmitter.prototype.listenerCount = function listenerCount(event) {\n  var evt = prefix ? prefix + event : event\n    , listeners = this._events[evt];\n\n  if (!listeners) return 0;\n  if (listeners.fn) return 1;\n  return listeners.length;\n};\n\n/**\n * Calls each of the listeners registered for a given event.\n *\n * @param {(String|Symbol)} event The event name.\n * @returns {Boolean} `true` if the event had listeners, else `false`.\n * @public\n */\nEventEmitter.prototype.emit = function emit(event, a1, a2, a3, a4, a5) {\n  var evt = prefix ? prefix + event : event;\n\n  if (!this._events[evt]) return false;\n\n  var listeners = this._events[evt]\n    , len = arguments.length\n    , args\n    , i;\n\n  if (listeners.fn) {\n    if (listeners.once) this.removeListener(event, listeners.fn, undefined, true);\n\n    switch (len) {\n      case 1: return listeners.fn.call(listeners.context), true;\n      case 2: return listeners.fn.call(listeners.context, a1), true;\n      case 3: return listeners.fn.call(listeners.context, a1, a2), true;\n      case 4: return listeners.fn.call(listeners.context, a1, a2, a3), true;\n      case 5: return listeners.fn.call(listeners.context, a1, a2, a3, a4), true;\n      case 6: return listeners.fn.call(listeners.context, a1, a2, a3, a4, a5), true;\n    }\n\n    for (i = 1, args = new Array(len -1); i < len; i++) {\n      args[i - 1] = arguments[i];\n    }\n\n    listeners.fn.apply(listeners.context, args);\n  } else {\n    var length = listeners.length\n      , j;\n\n    for (i = 0; i < length; i++) {\n      if (listeners[i].once) this.removeListener(event, listeners[i].fn, undefined, true);\n\n      switch (len) {\n        case 1: listeners[i].fn.call(listeners[i].context); break;\n        case 2: listeners[i].fn.call(listeners[i].context, a1); break;\n        case 3: listeners[i].fn.call(listeners[i].context, a1, a2); break;\n        case 4: listeners[i].fn.call(listeners[i].context, a1, a2, a3); break;\n        default:\n          if (!args) for (j = 1, args = new Array(len -1); j < len; j++) {\n            args[j - 1] = arguments[j];\n          }\n\n          listeners[i].fn.apply(listeners[i].context, args);\n      }\n    }\n  }\n\n  return true;\n};\n\n/**\n * Add a listener for a given event.\n *\n * @param {(String|Symbol)} event The event name.\n * @param {Function} fn The listener function.\n * @param {*} [context=this] The context to invoke the listener with.\n * @returns {EventEmitter} `this`.\n * @public\n */\nEventEmitter.prototype.on = function on(event, fn, context) {\n  return addListener(this, event, fn, context, false);\n};\n\n/**\n * Add a one-time listener for a given event.\n *\n * @param {(String|Symbol)} event The event name.\n * @param {Function} fn The listener function.\n * @param {*} [context=this] The context to invoke the listener with.\n * @returns {EventEmitter} `this`.\n * @public\n */\nEventEmitter.prototype.once = function once(event, fn, context) {\n  return addListener(this, event, fn, context, true);\n};\n\n/**\n * Remove the listeners of a given event.\n *\n * @param {(String|Symbol)} event The event name.\n * @param {Function} fn Only remove the listeners that match this function.\n * @param {*} context Only remove the listeners that have this context.\n * @param {Boolean} once Only remove one-time listeners.\n * @returns {EventEmitter} `this`.\n * @public\n */\nEventEmitter.prototype.removeListener = function removeListener(event, fn, context, once) {\n  var evt = prefix ? prefix + event : event;\n\n  if (!this._events[evt]) return this;\n  if (!fn) {\n    clearEvent(this, evt);\n    return this;\n  }\n\n  var listeners = this._events[evt];\n\n  if (listeners.fn) {\n    if (\n      listeners.fn === fn &&\n      (!once || listeners.once) &&\n      (!context || listeners.context === context)\n    ) {\n      clearEvent(this, evt);\n    }\n  } else {\n    for (var i = 0, events = [], length = listeners.length; i < length; i++) {\n      if (\n        listeners[i].fn !== fn ||\n        (once && !listeners[i].once) ||\n        (context && listeners[i].context !== context)\n      ) {\n        events.push(listeners[i]);\n      }\n    }\n\n    //\n    // Reset the array, or remove it completely if we have no more listeners.\n    //\n    if (events.length) this._events[evt] = events.length === 1 ? events[0] : events;\n    else clearEvent(this, evt);\n  }\n\n  return this;\n};\n\n/**\n * Remove all listeners, or those of the specified event.\n *\n * @param {(String|Symbol)} [event] The event name.\n * @returns {EventEmitter} `this`.\n * @public\n */\nEventEmitter.prototype.removeAllListeners = function removeAllListeners(event) {\n  var evt;\n\n  if (event) {\n    evt = prefix ? prefix + event : event;\n    if (this._events[evt]) clearEvent(this, evt);\n  } else {\n    this._events = new Events();\n    this._eventsCount = 0;\n  }\n\n  return this;\n};\n\n//\n// Alias methods names because people roll like that.\n//\nEventEmitter.prototype.off = EventEmitter.prototype.removeListener;\nEventEmitter.prototype.addListener = EventEmitter.prototype.on;\n\n//\n// Expose the prefix.\n//\nEventEmitter.prefixed = prefix;\n\n//\n// Allow `EventEmitter` to be imported as module namespace.\n//\nEventEmitter.EventEmitter = EventEmitter;\n\n//\n// Expose the module.\n//\nif ('undefined' !== typeof module) {\n  module.exports = EventEmitter;\n}\n", "import {\n  WorkerContext,\n  hasUMDWorker,\n  injectWorker,\n  loadWorker,\n} from './inject-worker';\nimport { Events } from '../events';\nimport Transmuxer, {\n  TransmuxConfig,\n  TransmuxState,\n  isPromise,\n} from '../demux/transmuxer';\nimport { logger } from '../utils/logger';\nimport { ErrorTypes, ErrorDetails } from '../errors';\nimport { getMediaSource } from '../utils/mediasource-helper';\nimport { EventEmitter } from 'eventemitter3';\nimport { Fragment, Part } from '../loader/fragment';\nimport type { ChunkMetadata, TransmuxerResult } from '../types/transmuxer';\nimport type Hls from '../hls';\nimport type { HlsEventEmitter, HlsListeners } from '../events';\nimport type { PlaylistLevelType } from '../types/loader';\nimport type { TypeSupported } from './tsdemuxer';\nimport type { RationalTimestamp } from '../utils/timescale-conversion';\n\nexport default class TransmuxerInterface {\n  public error: Error | null = null;\n  private hls: Hls;\n  private id: PlaylistLevelType;\n  private observer: HlsEventEmitter;\n  private frag: Fragment | null = null;\n  private part: Part | null = null;\n  private useWorker: boolean;\n  private workerContext: WorkerContext | null = null;\n  private onwmsg?: (\n    event: MessageEvent<{ event: string; data?: any } | null>,\n  ) => void;\n  private transmuxer: Transmuxer | null = null;\n  private onTransmuxComplete: (transmuxResult: TransmuxerResult) => void;\n  private onFlush: (chunkMeta: ChunkMetadata) => void;\n\n  constructor(\n    hls: Hls,\n    id: PlaylistLevelType,\n    onTransmuxComplete: (transmuxResult: TransmuxerResult) => void,\n    onFlush: (chunkMeta: ChunkMetadata) => void,\n  ) {\n    const config = hls.config;\n    this.hls = hls;\n    this.id = id;\n    this.useWorker = !!config.enableWorker;\n    this.onTransmuxComplete = onTransmuxComplete;\n    this.onFlush = onFlush;\n\n    const forwardMessage = (ev, data) => {\n      data = data || {};\n      data.frag = this.frag;\n      data.id = this.id;\n      if (ev === Events.ERROR) {\n        this.error = data.error;\n      }\n      this.hls.trigger(ev, data);\n    };\n\n    // forward events to main thread\n    this.observer = new EventEmitter() as HlsEventEmitter;\n    this.observer.on(Events.FRAG_DECRYPTED, forwardMessage);\n    this.observer.on(Events.ERROR, forwardMessage);\n\n    const MediaSource = getMediaSource(config.preferManagedMediaSource) || {\n      isTypeSupported: () => false,\n    };\n    const m2tsTypeSupported: TypeSupported = {\n      mpeg: MediaSource.isTypeSupported('audio/mpeg'),\n      mp3: MediaSource.isTypeSupported('audio/mp4; codecs=\"mp3\"'),\n      ac3: __USE_M2TS_ADVANCED_CODECS__\n        ? MediaSource.isTypeSupported('audio/mp4; codecs=\"ac-3\"')\n        : false,\n    };\n\n    if (this.useWorker && typeof Worker !== 'undefined') {\n      const canCreateWorker = config.workerPath || hasUMDWorker();\n      if (canCreateWorker) {\n        try {\n          if (config.workerPath) {\n            logger.log(`loading Web Worker ${config.workerPath} for \"${id}\"`);\n            this.workerContext = loadWorker(config.workerPath);\n          } else {\n            logger.log(`injecting Web Worker for \"${id}\"`);\n            this.workerContext = injectWorker();\n          }\n          this.onwmsg = (event) => this.onWorkerMessage(event);\n          const { worker } = this.workerContext;\n          worker.addEventListener('message', this.onwmsg);\n          worker.onerror = (event) => {\n            const error = new Error(\n              `${event.message}  (${event.filename}:${event.lineno})`,\n            );\n            config.enableWorker = false;\n            logger.warn(`Error in \"${id}\" Web Worker, fallback to inline`);\n            this.hls.trigger(Events.ERROR, {\n              type: ErrorTypes.OTHER_ERROR,\n              details: ErrorDetails.INTERNAL_EXCEPTION,\n              fatal: false,\n              event: 'demuxerWorker',\n              error,\n            });\n          };\n          worker.postMessage({\n            cmd: 'init',\n            typeSupported: m2tsTypeSupported,\n            vendor: '',\n            id: id,\n            config: JSON.stringify(config),\n          });\n        } catch (err) {\n          logger.warn(\n            `Error setting up \"${id}\" Web Worker, fallback to inline`,\n            err,\n          );\n          this.resetWorker();\n          this.error = null;\n          this.transmuxer = new Transmuxer(\n            this.observer,\n            m2tsTypeSupported,\n            config,\n            '',\n            id,\n          );\n        }\n        return;\n      }\n    }\n\n    this.transmuxer = new Transmuxer(\n      this.observer,\n      m2tsTypeSupported,\n      config,\n      '',\n      id,\n    );\n  }\n\n  resetWorker() {\n    if (this.workerContext) {\n      const { worker, objectURL } = this.workerContext;\n      if (objectURL) {\n        // revoke the Object URL that was used to create transmuxer worker, so as not to leak it\n        self.URL.revokeObjectURL(objectURL);\n      }\n      worker.removeEventListener('message', this.onwmsg as any);\n      worker.onerror = null;\n      worker.terminate();\n      this.workerContext = null;\n    }\n  }\n\n  destroy() {\n    if (this.workerContext) {\n      this.resetWorker();\n      this.onwmsg = undefined;\n    } else {\n      const transmuxer = this.transmuxer;\n      if (transmuxer) {\n        transmuxer.destroy();\n        this.transmuxer = null;\n      }\n    }\n    const observer = this.observer;\n    if (observer) {\n      observer.removeAllListeners();\n    }\n    this.frag = null;\n    // @ts-ignore\n    this.observer = null;\n    // @ts-ignore\n    this.hls = null;\n  }\n\n  push(\n    data: ArrayBuffer,\n    initSegmentData: Uint8Array | undefined,\n    audioCodec: string | undefined,\n    videoCodec: string | undefined,\n    frag: Fragment,\n    part: Part | null,\n    duration: number,\n    accurateTimeOffset: boolean,\n    chunkMeta: ChunkMetadata,\n    defaultInitPTS?: RationalTimestamp,\n  ) {\n    chunkMeta.transmuxing.start = self.performance.now();\n    const { transmuxer } = this;\n    const timeOffset = part ? part.start : frag.start;\n    // TODO: push \"clear-lead\" decrypt data for unencrypted fragments in streams with encrypted ones\n    const decryptdata = frag.decryptdata;\n    const lastFrag = this.frag;\n\n    const discontinuity = !(lastFrag && frag.cc === lastFrag.cc);\n    const trackSwitch = !(lastFrag && chunkMeta.level === lastFrag.level);\n    const snDiff = lastFrag ? chunkMeta.sn - (lastFrag.sn as number) : -1;\n    const partDiff = this.part ? chunkMeta.part - this.part.index : -1;\n    const progressive =\n      snDiff === 0 &&\n      chunkMeta.id > 1 &&\n      chunkMeta.id === lastFrag?.stats.chunkCount;\n    const contiguous =\n      !trackSwitch &&\n      (snDiff === 1 ||\n        (snDiff === 0 && (partDiff === 1 || (progressive && partDiff <= 0))));\n    const now = self.performance.now();\n\n    if (trackSwitch || snDiff || frag.stats.parsing.start === 0) {\n      frag.stats.parsing.start = now;\n    }\n    if (part && (partDiff || !contiguous)) {\n      part.stats.parsing.start = now;\n    }\n    const initSegmentChange = !(\n      lastFrag && frag.initSegment?.url === lastFrag.initSegment?.url\n    );\n    const state = new TransmuxState(\n      discontinuity,\n      contiguous,\n      accurateTimeOffset,\n      trackSwitch,\n      timeOffset,\n      initSegmentChange,\n    );\n    if (!contiguous || discontinuity || initSegmentChange) {\n      logger.log(`[transmuxer-interface, ${frag.type}]: Starting new transmux session for sn: ${chunkMeta.sn} p: ${chunkMeta.part} level: ${chunkMeta.level} id: ${chunkMeta.id}\n        discontinuity: ${discontinuity}\n        trackSwitch: ${trackSwitch}\n        contiguous: ${contiguous}\n        accurateTimeOffset: ${accurateTimeOffset}\n        timeOffset: ${timeOffset}\n        initSegmentChange: ${initSegmentChange}`);\n      const config = new TransmuxConfig(\n        audioCodec,\n        videoCodec,\n        initSegmentData,\n        duration,\n        defaultInitPTS,\n      );\n      this.configureTransmuxer(config);\n    }\n\n    this.frag = frag;\n    this.part = part;\n\n    // Frags with sn of 'initSegment' are not transmuxed\n    if (this.workerContext) {\n      // post fragment payload as transferable objects for ArrayBuffer (no copy)\n      this.workerContext.worker.postMessage(\n        {\n          cmd: 'demux',\n          data,\n          decryptdata,\n          chunkMeta,\n          state,\n        },\n        data instanceof ArrayBuffer ? [data] : [],\n      );\n    } else if (transmuxer) {\n      const transmuxResult = transmuxer.push(\n        data,\n        decryptdata,\n        chunkMeta,\n        state,\n      );\n      if (isPromise(transmuxResult)) {\n        transmuxer.async = true;\n        transmuxResult\n          .then((data) => {\n            this.handleTransmuxComplete(data);\n          })\n          .catch((error) => {\n            this.transmuxerError(\n              error,\n              chunkMeta,\n              'transmuxer-interface push error',\n            );\n          });\n      } else {\n        transmuxer.async = false;\n        this.handleTransmuxComplete(transmuxResult as TransmuxerResult);\n      }\n    }\n  }\n\n  flush(chunkMeta: ChunkMetadata) {\n    chunkMeta.transmuxing.start = self.performance.now();\n    const { transmuxer } = this;\n    if (this.workerContext) {\n      1;\n      this.workerContext.worker.postMessage({\n        cmd: 'flush',\n        chunkMeta,\n      });\n    } else if (transmuxer) {\n      let transmuxResult = transmuxer.flush(chunkMeta);\n      const asyncFlush = isPromise(transmuxResult);\n      if (asyncFlush || transmuxer.async) {\n        if (!isPromise(transmuxResult)) {\n          transmuxResult = Promise.resolve(transmuxResult);\n        }\n        transmuxResult\n          .then((data) => {\n            this.handleFlushResult(data, chunkMeta);\n          })\n          .catch((error) => {\n            this.transmuxerError(\n              error,\n              chunkMeta,\n              'transmuxer-interface flush error',\n            );\n          });\n      } else {\n        this.handleFlushResult(\n          transmuxResult as Array<TransmuxerResult>,\n          chunkMeta,\n        );\n      }\n    }\n  }\n\n  private transmuxerError(\n    error: Error,\n    chunkMeta: ChunkMetadata,\n    reason: string,\n  ) {\n    if (!this.hls) {\n      return;\n    }\n    this.error = error;\n    this.hls.trigger(Events.ERROR, {\n      type: ErrorTypes.MEDIA_ERROR,\n      details: ErrorDetails.FRAG_PARSING_ERROR,\n      chunkMeta,\n      frag: this.frag || undefined,\n      fatal: false,\n      error,\n      err: error,\n      reason,\n    });\n  }\n\n  private handleFlushResult(\n    results: Array<TransmuxerResult>,\n    chunkMeta: ChunkMetadata,\n  ) {\n    results.forEach((result) => {\n      this.handleTransmuxComplete(result);\n    });\n    this.onFlush(chunkMeta);\n  }\n\n  private onWorkerMessage(\n    event: MessageEvent<{ event: string; data?: any } | null>,\n  ) {\n    const data = event.data;\n    if (!data?.event) {\n      logger.warn(\n        `worker message received with no ${data ? 'event name' : 'data'}`,\n      );\n      return;\n    }\n    const hls = this.hls;\n    if (!this.hls) {\n      return;\n    }\n    switch (data.event) {\n      case 'init': {\n        const objectURL = this.workerContext?.objectURL;\n        if (objectURL) {\n          // revoke the Object URL that was used to create transmuxer worker, so as not to leak it\n          self.URL.revokeObjectURL(objectURL);\n        }\n        break;\n      }\n\n      case 'transmuxComplete': {\n        this.handleTransmuxComplete(data.data);\n        break;\n      }\n\n      case 'flush': {\n        this.onFlush(data.data);\n        break;\n      }\n\n      // pass logs from the worker thread to the main logger\n      case 'workerLog':\n        if (logger[data.data.logType]) {\n          logger[data.data.logType](data.data.message);\n        }\n        break;\n\n      default: {\n        data.data = data.data || {};\n        data.data.frag = this.frag;\n        data.data.id = this.id;\n        hls.trigger(data.event as keyof HlsListeners, data.data);\n        break;\n      }\n    }\n  }\n\n  private configureTransmuxer(config: TransmuxConfig) {\n    const { transmuxer } = this;\n    if (this.workerContext) {\n      this.workerContext.worker.postMessage({\n        cmd: 'configure',\n        config,\n      });\n    } else if (transmuxer) {\n      transmuxer.configure(config);\n    }\n  }\n\n  private handleTransmuxComplete(result: TransmuxerResult) {\n    result.chunkMeta.transmuxing.end = self.performance.now();\n    this.onTransmuxComplete(result);\n  }\n}\n", "import type { Level } from '../types/level';\nimport type { MediaAttributes, MediaPlaylist } from '../types/media-playlist';\n\nexport function subtitleOptionsIdentical(\n  trackList1: MediaPlaylist[] | Level[],\n  trackList2: MediaPlaylist[],\n): boolean {\n  if (trackList1.length !== trackList2.length) {\n    return false;\n  }\n  for (let i = 0; i < trackList1.length; i++) {\n    if (\n      !mediaAttributesIdentical(\n        trackList1[i].attrs as MediaAttributes,\n        trackList2[i].attrs,\n      )\n    ) {\n      return false;\n    }\n  }\n  return true;\n}\n\nexport function mediaAttributesIdentical(\n  attrs1: MediaAttributes,\n  attrs2: MediaAttributes,\n  customAttributes?: string[],\n): boolean {\n  // Media options with the same rendition ID must be bit identical\n  const stableRenditionId = attrs1['STABLE-RENDITION-ID'];\n  if (stableRenditionId && !customAttributes) {\n    return stableRenditionId === attrs2['STABLE-RENDITION-ID'];\n  }\n  // When rendition ID is not present, compare attributes\n  return !(\n    customAttributes || [\n      'LANGUAGE',\n      'NAME',\n      'CHARACTERISTICS',\n      'AUTOSELECT',\n      'DEFAULT',\n      'FORCED',\n      'ASSOC-LANGUAGE',\n    ]\n  ).some(\n    (subtitleAttribute) =>\n      attrs1[subtitleAttribute] !== attrs2[subtitleAttribute],\n  );\n}\n\nexport function subtitleTrackMatchesTextTrack(\n  subtitleTrack: Pick<MediaPlaylist, 'name' | 'lang' | 'attrs'>,\n  textTrack: TextTrack,\n) {\n  return (\n    textTrack.label.toLowerCase() === subtitleTrack.name.toLowerCase() &&\n    (!textTrack.language ||\n      textTrack.language.toLowerCase() ===\n        (subtitleTrack.lang || '').toLowerCase())\n  );\n}\n", "import BaseStreamController, { State } from './base-stream-controller';\nimport { Events } from '../events';\nimport { Bufferable, BufferHelper } from '../utils/buffer-helper';\nimport { FragmentState } from './fragment-tracker';\nimport { Level } from '../types/level';\nimport { PlaylistContextType, PlaylistLevelType } from '../types/loader';\nimport { Fragment, ElementaryStreamTypes, Part } from '../loader/fragment';\nimport ChunkCache from '../demux/chunk-cache';\nimport TransmuxerInterface from '../demux/transmuxer-interface';\nimport { ChunkMetadata } from '../types/transmuxer';\nimport { fragmentWithinToleranceTest } from './fragment-finders';\nimport { alignMediaPlaylistByPDT } from '../utils/discontinuities';\nimport { mediaAttributesIdentical } from '../utils/media-option-attributes';\nimport { ErrorDetails } from '../errors';\nimport type { NetworkComponentAPI } from '../types/component-api';\nimport type Hls from '../hls';\nimport type { FragmentTracker } from './fragment-tracker';\nimport type KeyLoader from '../loader/key-loader';\nimport type { TransmuxerResult } from '../types/transmuxer';\nimport type { LevelDetails } from '../loader/level-details';\nimport type { TrackSet } from '../types/track';\nimport type {\n  BufferCreatedData,\n  AudioTracksUpdatedData,\n  AudioTrackSwitchingData,\n  LevelLoadedData,\n  TrackLoadedData,\n  BufferAppendingData,\n  BufferFlushedData,\n  InitPTSFoundData,\n  FragLoadedData,\n  FragParsingMetadataData,\n  FragParsingUserdataData,\n  FragBufferedData,\n  ErrorData,\n  BufferFlushingData,\n} from '../types/events';\nimport type { MediaPlaylist } from '../types/media-playlist';\n\nconst TICK_INTERVAL = 100; // how often to tick in ms\n\ntype WaitingForPTSData = {\n  frag: Fragment;\n  part: Part | null;\n  cache: ChunkCache;\n  complete: boolean;\n};\n\nclass AudioStreamController\n  extends BaseStreamController\n  implements NetworkComponentAPI\n{\n  private videoBuffer: Bufferable | null = null;\n  private videoTrackCC: number = -1;\n  private waitingVideoCC: number = -1;\n  private bufferedTrack: MediaPlaylist | null = null;\n  private switchingTrack: MediaPlaylist | null = null;\n  private trackId: number = -1;\n  private waitingData: WaitingForPTSData | null = null;\n  private mainDetails: LevelDetails | null = null;\n  private flushing: boolean = false;\n  private bufferFlushed: boolean = false;\n  private cachedTrackLoadedData: TrackLoadedData | null = null;\n\n  constructor(\n    hls: Hls,\n    fragmentTracker: FragmentTracker,\n    keyLoader: KeyLoader,\n  ) {\n    super(\n      hls,\n      fragmentTracker,\n      keyLoader,\n      '[audio-stream-controller]',\n      PlaylistLevelType.AUDIO,\n    );\n    this._registerListeners();\n  }\n\n  protected onHandlerDestroying() {\n    this._unregisterListeners();\n    super.onHandlerDestroying();\n    this.mainDetails = null;\n    this.bufferedTrack = null;\n    this.switchingTrack = null;\n  }\n\n  private _registerListeners() {\n    const { hls } = this;\n    hls.on(Events.MEDIA_ATTACHED, this.onMediaAttached, this);\n    hls.on(Events.MEDIA_DETACHING, this.onMediaDetaching, this);\n    hls.on(Events.MANIFEST_LOADING, this.onManifestLoading, this);\n    hls.on(Events.LEVEL_LOADED, this.onLevelLoaded, this);\n    hls.on(Events.AUDIO_TRACKS_UPDATED, this.onAudioTracksUpdated, this);\n    hls.on(Events.AUDIO_TRACK_SWITCHING, this.onAudioTrackSwitching, this);\n    hls.on(Events.AUDIO_TRACK_LOADED, this.onAudioTrackLoaded, this);\n    hls.on(Events.ERROR, this.onError, this);\n    hls.on(Events.BUFFER_RESET, this.onBufferReset, this);\n    hls.on(Events.BUFFER_CREATED, this.onBufferCreated, this);\n    hls.on(Events.BUFFER_FLUSHING, this.onBufferFlushing, this);\n    hls.on(Events.BUFFER_FLUSHED, this.onBufferFlushed, this);\n    hls.on(Events.INIT_PTS_FOUND, this.onInitPtsFound, this);\n    hls.on(Events.FRAG_BUFFERED, this.onFragBuffered, this);\n  }\n\n  private _unregisterListeners() {\n    const { hls } = this;\n    hls.off(Events.MEDIA_ATTACHED, this.onMediaAttached, this);\n    hls.off(Events.MEDIA_DETACHING, this.onMediaDetaching, this);\n    hls.off(Events.MANIFEST_LOADING, this.onManifestLoading, this);\n    hls.off(Events.LEVEL_LOADED, this.onLevelLoaded, this);\n    hls.off(Events.AUDIO_TRACKS_UPDATED, this.onAudioTracksUpdated, this);\n    hls.off(Events.AUDIO_TRACK_SWITCHING, this.onAudioTrackSwitching, this);\n    hls.off(Events.AUDIO_TRACK_LOADED, this.onAudioTrackLoaded, this);\n    hls.off(Events.ERROR, this.onError, this);\n    hls.off(Events.BUFFER_RESET, this.onBufferReset, this);\n    hls.off(Events.BUFFER_CREATED, this.onBufferCreated, this);\n    hls.off(Events.BUFFER_FLUSHING, this.onBufferFlushing, this);\n    hls.off(Events.BUFFER_FLUSHED, this.onBufferFlushed, this);\n    hls.off(Events.INIT_PTS_FOUND, this.onInitPtsFound, this);\n    hls.off(Events.FRAG_BUFFERED, this.onFragBuffered, this);\n  }\n\n  // INIT_PTS_FOUND is triggered when the video track parsed in the stream-controller has a new PTS value\n  onInitPtsFound(\n    event: Events.INIT_PTS_FOUND,\n    { frag, id, initPTS, timescale }: InitPTSFoundData,\n  ) {\n    // Always update the new INIT PTS\n    // Can change due level switch\n    if (id === 'main') {\n      const cc = frag.cc;\n      this.initPTS[frag.cc] = { baseTime: initPTS, timescale };\n      this.log(`InitPTS for cc: ${cc} found from main: ${initPTS}`);\n      this.videoTrackCC = cc;\n      // If we are waiting, tick immediately to unblock audio fragment transmuxing\n      if (this.state === State.WAITING_INIT_PTS) {\n        this.tick();\n      }\n    }\n  }\n\n  startLoad(startPosition: number) {\n    if (!this.levels) {\n      this.startPosition = startPosition;\n      this.state = State.STOPPED;\n      return;\n    }\n    const lastCurrentTime = this.lastCurrentTime;\n    this.stopLoad();\n    this.setInterval(TICK_INTERVAL);\n    if (lastCurrentTime > 0 && startPosition === -1) {\n      this.log(\n        `Override startPosition with lastCurrentTime @${lastCurrentTime.toFixed(\n          3,\n        )}`,\n      );\n      startPosition = lastCurrentTime;\n      this.state = State.IDLE;\n    } else {\n      this.loadedmetadata = false;\n      this.state = State.WAITING_TRACK;\n    }\n    this.nextLoadPosition =\n      this.startPosition =\n      this.lastCurrentTime =\n        startPosition;\n\n    this.tick();\n  }\n\n  doTick() {\n    switch (this.state) {\n      case State.IDLE:\n        this.doTickIdle();\n        break;\n      case State.WAITING_TRACK: {\n        const { levels, trackId } = this;\n        const details = levels?.[trackId]?.details;\n        if (details) {\n          if (this.waitForCdnTuneIn(details)) {\n            break;\n          }\n          this.state = State.WAITING_INIT_PTS;\n        }\n        break;\n      }\n      case State.FRAG_LOADING_WAITING_RETRY: {\n        const now = performance.now();\n        const retryDate = this.retryDate;\n        // if current time is gt than retryDate, or if media seeking let's switch to IDLE state to retry loading\n        if (!retryDate || now >= retryDate || this.media?.seeking) {\n          const { levels, trackId } = this;\n          this.log('RetryDate reached, switch back to IDLE state');\n          this.resetStartWhenNotLoaded(levels?.[trackId] || null);\n          this.state = State.IDLE;\n        }\n        break;\n      }\n      case State.WAITING_INIT_PTS: {\n        // Ensure we don't get stuck in the WAITING_INIT_PTS state if the waiting frag CC doesn't match any initPTS\n        const waitingData = this.waitingData;\n        if (waitingData) {\n          const { frag, part, cache, complete } = waitingData;\n          if (this.initPTS[frag.cc] !== undefined) {\n            this.waitingData = null;\n            this.waitingVideoCC = -1;\n            this.state = State.FRAG_LOADING;\n            const payload = cache.flush();\n            const data: FragLoadedData = {\n              frag,\n              part,\n              payload,\n              networkDetails: null,\n            };\n            this._handleFragmentLoadProgress(data);\n            if (complete) {\n              super._handleFragmentLoadComplete(data);\n            }\n          } else if (this.videoTrackCC !== this.waitingVideoCC) {\n            // Drop waiting fragment if videoTrackCC has changed since waitingFragment was set and initPTS was not found\n            this.log(\n              `Waiting fragment cc (${frag.cc}) cancelled because video is at cc ${this.videoTrackCC}`,\n            );\n            this.clearWaitingFragment();\n          } else {\n            // Drop waiting fragment if an earlier fragment is needed\n            const pos = this.getLoadPosition();\n            const bufferInfo = BufferHelper.bufferInfo(\n              this.mediaBuffer,\n              pos,\n              this.config.maxBufferHole,\n            );\n            const waitingFragmentAtPosition = fragmentWithinToleranceTest(\n              bufferInfo.end,\n              this.config.maxFragLookUpTolerance,\n              frag,\n            );\n            if (waitingFragmentAtPosition < 0) {\n              this.log(\n                `Waiting fragment cc (${frag.cc}) @ ${frag.start} cancelled because another fragment at ${bufferInfo.end} is needed`,\n              );\n              this.clearWaitingFragment();\n            }\n          }\n        } else {\n          this.state = State.IDLE;\n        }\n      }\n    }\n\n    this.onTickEnd();\n  }\n\n  clearWaitingFragment() {\n    const waitingData = this.waitingData;\n    if (waitingData) {\n      this.fragmentTracker.removeFragment(waitingData.frag);\n      this.waitingData = null;\n      this.waitingVideoCC = -1;\n      this.state = State.IDLE;\n    }\n  }\n\n  protected resetLoadingState() {\n    this.clearWaitingFragment();\n    super.resetLoadingState();\n  }\n\n  protected onTickEnd() {\n    const { media } = this;\n    if (!media?.readyState) {\n      // Exit early if we don't have media or if the media hasn't buffered anything yet (readyState 0)\n      return;\n    }\n\n    this.lastCurrentTime = media.currentTime;\n  }\n\n  private doTickIdle() {\n    const { hls, levels, media, trackId } = this;\n    const config = hls.config;\n\n    // 1. if video not attached AND\n    //    start fragment already requested OR start frag prefetch not enabled\n    // 2. if tracks or track not loaded and selected\n    // then exit loop\n    // => if media not attached but start frag prefetch is enabled and start frag not requested yet, we will not exit loop\n    if (\n      (!media && (this.startFragRequested || !config.startFragPrefetch)) ||\n      !levels?.[trackId]\n    ) {\n      return;\n    }\n\n    const levelInfo = levels[trackId];\n\n    const trackDetails = levelInfo.details;\n    if (\n      !trackDetails ||\n      (trackDetails.live && this.levelLastLoaded !== levelInfo) ||\n      this.waitForCdnTuneIn(trackDetails)\n    ) {\n      this.state = State.WAITING_TRACK;\n      return;\n    }\n\n    const bufferable = this.mediaBuffer ? this.mediaBuffer : this.media;\n    if (this.bufferFlushed && bufferable) {\n      this.bufferFlushed = false;\n      this.afterBufferFlushed(\n        bufferable,\n        ElementaryStreamTypes.AUDIO,\n        PlaylistLevelType.AUDIO,\n      );\n    }\n\n    const bufferInfo = this.getFwdBufferInfo(\n      bufferable,\n      PlaylistLevelType.AUDIO,\n    );\n    if (bufferInfo === null) {\n      return;\n    }\n    const { bufferedTrack, switchingTrack } = this;\n\n    if (!switchingTrack && this._streamEnded(bufferInfo, trackDetails)) {\n      hls.trigger(Events.BUFFER_EOS, { type: 'audio' });\n      this.state = State.ENDED;\n      return;\n    }\n\n    const mainBufferInfo = this.getFwdBufferInfo(\n      this.videoBuffer ? this.videoBuffer : this.media,\n      PlaylistLevelType.MAIN,\n    );\n    const bufferLen = bufferInfo.len;\n    const maxBufLen = this.getMaxBufferLength(mainBufferInfo?.len);\n\n    const fragments = trackDetails.fragments;\n    const start = fragments[0].start;\n    let targetBufferTime = this.flushing\n      ? this.getLoadPosition()\n      : bufferInfo.end;\n\n    if (switchingTrack && media) {\n      const pos = this.getLoadPosition();\n      // STABLE\n      if (\n        bufferedTrack &&\n        !mediaAttributesIdentical(switchingTrack.attrs, bufferedTrack.attrs)\n      ) {\n        targetBufferTime = pos;\n      }\n      // if currentTime (pos) is less than alt audio playlist start time, it means that alt audio is ahead of currentTime\n      if (trackDetails.PTSKnown && pos < start) {\n        // if everything is buffered from pos to start or if audio buffer upfront, let's seek to start\n        if (bufferInfo.end > start || bufferInfo.nextStart) {\n          this.log(\n            'Alt audio track ahead of main track, seek to start of alt audio track',\n          );\n          media.currentTime = start + 0.05;\n        }\n      }\n    }\n\n    // if buffer length is less than maxBufLen, or near the end, find a fragment to load\n    if (\n      bufferLen >= maxBufLen &&\n      !switchingTrack &&\n      targetBufferTime < fragments[fragments.length - 1].start\n    ) {\n      return;\n    }\n\n    let frag = this.getNextFragment(targetBufferTime, trackDetails);\n    let atGap = false;\n    // Avoid loop loading by using nextLoadPosition set for backtracking and skipping consecutive GAP tags\n    if (frag && this.isLoopLoading(frag, targetBufferTime)) {\n      atGap = !!frag.gap;\n      frag = this.getNextFragmentLoopLoading(\n        frag,\n        trackDetails,\n        bufferInfo,\n        PlaylistLevelType.MAIN,\n        maxBufLen,\n      );\n    }\n    if (!frag) {\n      this.bufferFlushed = true;\n      return;\n    }\n\n    // Buffer audio up to one target duration ahead of main buffer\n    const atBufferSyncLimit =\n      mainBufferInfo &&\n      frag.start > mainBufferInfo.end + trackDetails.targetduration;\n    if (\n      atBufferSyncLimit ||\n      // Or wait for main buffer after buffing some audio\n      (!mainBufferInfo?.len && bufferInfo.len)\n    ) {\n      // Check fragment-tracker for main fragments since GAP segments do not show up in bufferInfo\n      const mainFrag = this.getAppendedFrag(frag.start, PlaylistLevelType.MAIN);\n      if (mainFrag === null) {\n        return;\n      }\n      // Bridge gaps in main buffer\n      atGap ||=\n        !!mainFrag.gap || (!!atBufferSyncLimit && mainBufferInfo.len === 0);\n      if (\n        (atBufferSyncLimit && !atGap) ||\n        (atGap && bufferInfo.nextStart && bufferInfo.nextStart < mainFrag.end)\n      ) {\n        return;\n      }\n    }\n\n    this.loadFragment(frag, levelInfo, targetBufferTime);\n  }\n\n  protected getMaxBufferLength(mainBufferLength?: number): number {\n    const maxConfigBuffer = super.getMaxBufferLength();\n    if (!mainBufferLength) {\n      return maxConfigBuffer;\n    }\n    return Math.min(\n      Math.max(maxConfigBuffer, mainBufferLength),\n      this.config.maxMaxBufferLength,\n    );\n  }\n\n  onMediaDetaching() {\n    this.videoBuffer = null;\n    this.bufferFlushed = this.flushing = false;\n    super.onMediaDetaching();\n  }\n\n  onAudioTracksUpdated(\n    event: Events.AUDIO_TRACKS_UPDATED,\n    { audioTracks }: AudioTracksUpdatedData,\n  ) {\n    // Reset tranxmuxer is essential for large context switches (Content Steering)\n    this.resetTransmuxer();\n    this.levels = audioTracks.map((mediaPlaylist) => new Level(mediaPlaylist));\n  }\n\n  onAudioTrackSwitching(\n    event: Events.AUDIO_TRACK_SWITCHING,\n    data: AudioTrackSwitchingData,\n  ) {\n    // if any URL found on new audio track, it is an alternate audio track\n    const altAudio = !!data.url;\n    this.trackId = data.id;\n    const { fragCurrent } = this;\n\n    if (fragCurrent) {\n      fragCurrent.abortRequests();\n      this.removeUnbufferedFrags(fragCurrent.start);\n    }\n    this.resetLoadingState();\n    // destroy useless transmuxer when switching audio to main\n    if (!altAudio) {\n      this.resetTransmuxer();\n    } else {\n      // switching to audio track, start timer if not already started\n      this.setInterval(TICK_INTERVAL);\n    }\n\n    // should we switch tracks ?\n    if (altAudio) {\n      this.switchingTrack = data;\n      // main audio track are handled by stream-controller, just do something if switching to alt audio track\n      this.state = State.IDLE;\n      this.flushAudioIfNeeded(data);\n    } else {\n      this.switchingTrack = null;\n      this.bufferedTrack = data;\n      this.state = State.STOPPED;\n    }\n    this.tick();\n  }\n\n  onManifestLoading() {\n    this.fragmentTracker.removeAllFragments();\n    this.startPosition = this.lastCurrentTime = 0;\n    this.bufferFlushed = this.flushing = false;\n    this.levels =\n      this.mainDetails =\n      this.waitingData =\n      this.bufferedTrack =\n      this.cachedTrackLoadedData =\n      this.switchingTrack =\n        null;\n    this.startFragRequested = false;\n    this.trackId = this.videoTrackCC = this.waitingVideoCC = -1;\n  }\n\n  onLevelLoaded(event: Events.LEVEL_LOADED, data: LevelLoadedData) {\n    this.mainDetails = data.details;\n    if (this.cachedTrackLoadedData !== null) {\n      this.hls.trigger(Events.AUDIO_TRACK_LOADED, this.cachedTrackLoadedData);\n      this.cachedTrackLoadedData = null;\n    }\n  }\n\n  onAudioTrackLoaded(event: Events.AUDIO_TRACK_LOADED, data: TrackLoadedData) {\n    if (this.mainDetails == null) {\n      this.cachedTrackLoadedData = data;\n      return;\n    }\n    const { levels } = this;\n    const { details: newDetails, id: trackId } = data;\n    if (!levels) {\n      this.warn(`Audio tracks were reset while loading level ${trackId}`);\n      return;\n    }\n    this.log(\n      `Audio track ${trackId} loaded [${newDetails.startSN},${\n        newDetails.endSN\n      }]${\n        newDetails.lastPartSn\n          ? `[part-${newDetails.lastPartSn}-${newDetails.lastPartIndex}]`\n          : ''\n      },duration:${newDetails.totalduration}`,\n    );\n\n    const track = levels[trackId];\n    let sliding = 0;\n    if (newDetails.live || track.details?.live) {\n      this.checkLiveUpdate(newDetails);\n      const mainDetails = this.mainDetails;\n      if (newDetails.deltaUpdateFailed || !mainDetails) {\n        return;\n      }\n      if (\n        !track.details &&\n        newDetails.hasProgramDateTime &&\n        mainDetails.hasProgramDateTime\n      ) {\n        // Make sure our audio rendition is aligned with the \"main\" rendition, using\n        // pdt as our reference times.\n        alignMediaPlaylistByPDT(newDetails, mainDetails);\n        sliding = newDetails.fragments[0].start;\n      } else {\n        sliding = this.alignPlaylists(\n          newDetails,\n          track.details,\n          this.levelLastLoaded?.details,\n        );\n      }\n    }\n    track.details = newDetails;\n    this.levelLastLoaded = track;\n\n    // compute start position if we are aligned with the main playlist\n    if (!this.startFragRequested && (this.mainDetails || !newDetails.live)) {\n      this.setStartPosition(this.mainDetails || newDetails, sliding);\n    }\n    // only switch back to IDLE state if we were waiting for track to start downloading a new fragment\n    if (\n      this.state === State.WAITING_TRACK &&\n      !this.waitForCdnTuneIn(newDetails)\n    ) {\n      this.state = State.IDLE;\n    }\n\n    // trigger handler right now\n    this.tick();\n  }\n\n  _handleFragmentLoadProgress(data: FragLoadedData) {\n    const { frag, part, payload } = data;\n    const { config, trackId, levels } = this;\n    if (!levels) {\n      this.warn(\n        `Audio tracks were reset while fragment load was in progress. Fragment ${frag.sn} of level ${frag.level} will not be buffered`,\n      );\n      return;\n    }\n\n    const track = levels[trackId] as Level;\n    if (!track) {\n      this.warn('Audio track is undefined on fragment load progress');\n      return;\n    }\n    const details = track.details as LevelDetails;\n    if (!details) {\n      this.warn('Audio track details undefined on fragment load progress');\n      this.removeUnbufferedFrags(frag.start);\n      return;\n    }\n    const audioCodec =\n      config.defaultAudioCodec || track.audioCodec || 'mp4a.40.2';\n\n    let transmuxer = this.transmuxer;\n    if (!transmuxer) {\n      transmuxer = this.transmuxer = new TransmuxerInterface(\n        this.hls,\n        PlaylistLevelType.AUDIO,\n        this._handleTransmuxComplete.bind(this),\n        this._handleTransmuxerFlush.bind(this),\n      );\n    }\n\n    // Check if we have video initPTS\n    // If not we need to wait for it\n    const initPTS = this.initPTS[frag.cc];\n    const initSegmentData = frag.initSegment?.data;\n    if (initPTS !== undefined) {\n      // this.log(`Transmuxing ${sn} of [${details.startSN} ,${details.endSN}],track ${trackId}`);\n      // time Offset is accurate if level PTS is known, or if playlist is not sliding (not live)\n      const accurateTimeOffset = false; // details.PTSKnown || !details.live;\n      const partIndex = part ? part.index : -1;\n      const partial = partIndex !== -1;\n      const chunkMeta = new ChunkMetadata(\n        frag.level,\n        frag.sn as number,\n        frag.stats.chunkCount,\n        payload.byteLength,\n        partIndex,\n        partial,\n      );\n      transmuxer.push(\n        payload,\n        initSegmentData,\n        audioCodec,\n        '',\n        frag,\n        part,\n        details.totalduration,\n        accurateTimeOffset,\n        chunkMeta,\n        initPTS,\n      );\n    } else {\n      this.log(\n        `Unknown video PTS for cc ${frag.cc}, waiting for video PTS before demuxing audio frag ${frag.sn} of [${details.startSN} ,${details.endSN}],track ${trackId}`,\n      );\n      const { cache } = (this.waitingData = this.waitingData || {\n        frag,\n        part,\n        cache: new ChunkCache(),\n        complete: false,\n      });\n      cache.push(new Uint8Array(payload));\n      this.waitingVideoCC = this.videoTrackCC;\n      this.state = State.WAITING_INIT_PTS;\n    }\n  }\n\n  protected _handleFragmentLoadComplete(fragLoadedData: FragLoadedData) {\n    if (this.waitingData) {\n      this.waitingData.complete = true;\n      return;\n    }\n    super._handleFragmentLoadComplete(fragLoadedData);\n  }\n\n  onBufferReset(/* event: Events.BUFFER_RESET */) {\n    // reset reference to sourcebuffers\n    this.mediaBuffer = this.videoBuffer = null;\n    this.loadedmetadata = false;\n  }\n\n  onBufferCreated(event: Events.BUFFER_CREATED, data: BufferCreatedData) {\n    const audioTrack = data.tracks.audio;\n    if (audioTrack) {\n      this.mediaBuffer = audioTrack.buffer || null;\n    }\n    if (data.tracks.video) {\n      this.videoBuffer = data.tracks.video.buffer || null;\n    }\n  }\n\n  onFragBuffered(event: Events.FRAG_BUFFERED, data: FragBufferedData) {\n    const { frag, part } = data;\n    if (frag.type !== PlaylistLevelType.AUDIO) {\n      if (!this.loadedmetadata && frag.type === PlaylistLevelType.MAIN) {\n        const bufferable = this.videoBuffer || this.media;\n        if (bufferable) {\n          const bufferedTimeRanges = BufferHelper.getBuffered(bufferable);\n          if (bufferedTimeRanges.length) {\n            this.loadedmetadata = true;\n          }\n        }\n      }\n      return;\n    }\n    if (this.fragContextChanged(frag)) {\n      // If a level switch was requested while a fragment was buffering, it will emit the FRAG_BUFFERED event upon completion\n      // Avoid setting state back to IDLE or concluding the audio switch; otherwise, the switched-to track will not buffer\n      this.warn(\n        `Fragment ${frag.sn}${part ? ' p: ' + part.index : ''} of level ${\n          frag.level\n        } finished buffering, but was aborted. state: ${\n          this.state\n        }, audioSwitch: ${\n          this.switchingTrack ? this.switchingTrack.name : 'false'\n        }`,\n      );\n      return;\n    }\n    if (frag.sn !== 'initSegment') {\n      this.fragPrevious = frag;\n      const track = this.switchingTrack;\n      if (track) {\n        this.bufferedTrack = track;\n        this.switchingTrack = null;\n        this.hls.trigger(Events.AUDIO_TRACK_SWITCHED, { ...track });\n      }\n    }\n    this.fragBufferedComplete(frag, part);\n  }\n\n  private onError(event: Events.ERROR, data: ErrorData) {\n    if (data.fatal) {\n      this.state = State.ERROR;\n      return;\n    }\n    switch (data.details) {\n      case ErrorDetails.FRAG_GAP:\n      case ErrorDetails.FRAG_PARSING_ERROR:\n      case ErrorDetails.FRAG_DECRYPT_ERROR:\n      case ErrorDetails.FRAG_LOAD_ERROR:\n      case ErrorDetails.FRAG_LOAD_TIMEOUT:\n      case ErrorDetails.KEY_LOAD_ERROR:\n      case ErrorDetails.KEY_LOAD_TIMEOUT:\n        this.onFragmentOrKeyLoadError(PlaylistLevelType.AUDIO, data);\n        break;\n      case ErrorDetails.AUDIO_TRACK_LOAD_ERROR:\n      case ErrorDetails.AUDIO_TRACK_LOAD_TIMEOUT:\n      case ErrorDetails.LEVEL_PARSING_ERROR:\n        // in case of non fatal error while loading track, if not retrying to load track, switch back to IDLE\n        if (\n          !data.levelRetry &&\n          this.state === State.WAITING_TRACK &&\n          data.context?.type === PlaylistContextType.AUDIO_TRACK\n        ) {\n          this.state = State.IDLE;\n        }\n        break;\n      case ErrorDetails.BUFFER_APPEND_ERROR:\n      case ErrorDetails.BUFFER_FULL_ERROR:\n        if (!data.parent || data.parent !== 'audio') {\n          return;\n        }\n        if (data.details === ErrorDetails.BUFFER_APPEND_ERROR) {\n          this.resetLoadingState();\n          return;\n        }\n        if (this.reduceLengthAndFlushBuffer(data)) {\n          this.bufferedTrack = null;\n          super.flushMainBuffer(0, Number.POSITIVE_INFINITY, 'audio');\n        }\n        break;\n      case ErrorDetails.INTERNAL_EXCEPTION:\n        this.recoverWorkerError(data);\n        break;\n      default:\n        break;\n    }\n  }\n\n  private onBufferFlushing(\n    event: Events.BUFFER_FLUSHING,\n    { type }: BufferFlushingData,\n  ) {\n    if (type !== ElementaryStreamTypes.VIDEO) {\n      this.flushing = true;\n    }\n  }\n\n  private onBufferFlushed(\n    event: Events.BUFFER_FLUSHED,\n    { type }: BufferFlushedData,\n  ) {\n    if (type !== ElementaryStreamTypes.VIDEO) {\n      this.flushing = false;\n      this.bufferFlushed = true;\n      if (this.state === State.ENDED) {\n        this.state = State.IDLE;\n      }\n      const mediaBuffer = this.mediaBuffer || this.media;\n      if (mediaBuffer) {\n        this.afterBufferFlushed(mediaBuffer, type, PlaylistLevelType.AUDIO);\n        this.tick();\n      }\n    }\n  }\n\n  private _handleTransmuxComplete(transmuxResult: TransmuxerResult) {\n    const id = 'audio';\n    const { hls } = this;\n    const { remuxResult, chunkMeta } = transmuxResult;\n\n    const context = this.getCurrentContext(chunkMeta);\n    if (!context) {\n      this.resetWhenMissingContext(chunkMeta);\n      return;\n    }\n    const { frag, part, level } = context;\n    const { details } = level;\n    const { audio, text, id3, initSegment } = remuxResult;\n\n    // Check if the current fragment has been aborted. We check this by first seeing if we're still playing the current level.\n    // If we are, subsequently check if the currently loading fragment (fragCurrent) has changed.\n    if (this.fragContextChanged(frag) || !details) {\n      this.fragmentTracker.removeFragment(frag);\n      return;\n    }\n\n    this.state = State.PARSING;\n    if (this.switchingTrack && audio) {\n      this.completeAudioSwitch(this.switchingTrack);\n    }\n\n    if (initSegment?.tracks) {\n      const mapFragment = frag.initSegment || frag;\n      this._bufferInitSegment(\n        level,\n        initSegment.tracks,\n        mapFragment,\n        chunkMeta,\n      );\n      hls.trigger(Events.FRAG_PARSING_INIT_SEGMENT, {\n        frag: mapFragment,\n        id,\n        tracks: initSegment.tracks,\n      });\n      // Only flush audio from old audio tracks when PTS is known on new audio track\n    }\n    if (audio) {\n      const { startPTS, endPTS, startDTS, endDTS } = audio;\n      if (part) {\n        part.elementaryStreams[ElementaryStreamTypes.AUDIO] = {\n          startPTS,\n          endPTS,\n          startDTS,\n          endDTS,\n        };\n      }\n      frag.setElementaryStreamInfo(\n        ElementaryStreamTypes.AUDIO,\n        startPTS,\n        endPTS,\n        startDTS,\n        endDTS,\n      );\n      this.bufferFragmentData(audio, frag, part, chunkMeta);\n    }\n\n    if (id3?.samples?.length) {\n      const emittedID3: FragParsingMetadataData = Object.assign(\n        {\n          id,\n          frag,\n          details,\n        },\n        id3,\n      );\n      hls.trigger(Events.FRAG_PARSING_METADATA, emittedID3);\n    }\n    if (text) {\n      const emittedText: FragParsingUserdataData = Object.assign(\n        {\n          id,\n          frag,\n          details,\n        },\n        text,\n      );\n      hls.trigger(Events.FRAG_PARSING_USERDATA, emittedText);\n    }\n  }\n\n  private _bufferInitSegment(\n    currentLevel: Level,\n    tracks: TrackSet,\n    frag: Fragment,\n    chunkMeta: ChunkMetadata,\n  ) {\n    if (this.state !== State.PARSING) {\n      return;\n    }\n    // delete any video track found on audio transmuxer\n    if (tracks.video) {\n      delete tracks.video;\n    }\n\n    // include levelCodec in audio and video tracks\n    const track = tracks.audio;\n    if (!track) {\n      return;\n    }\n\n    track.id = 'audio';\n\n    const variantAudioCodecs = currentLevel.audioCodec;\n    this.log(\n      `Init audio buffer, container:${track.container}, codecs[level/parsed]=[${variantAudioCodecs}/${track.codec}]`,\n    );\n    // SourceBuffer will use track.levelCodec if defined\n    if (variantAudioCodecs && variantAudioCodecs.split(',').length === 1) {\n      track.levelCodec = variantAudioCodecs;\n    }\n    this.hls.trigger(Events.BUFFER_CODECS, tracks);\n    const initSegment = track.initSegment;\n    if (initSegment?.byteLength) {\n      const segment: BufferAppendingData = {\n        type: 'audio',\n        frag,\n        part: null,\n        chunkMeta,\n        parent: frag.type,\n        data: initSegment,\n      };\n      this.hls.trigger(Events.BUFFER_APPENDING, segment);\n    }\n    // trigger handler right now\n    this.tickImmediate();\n  }\n\n  protected loadFragment(\n    frag: Fragment,\n    track: Level,\n    targetBufferTime: number,\n  ) {\n    // only load if fragment is not loaded or if in audio switch\n    const fragState = this.fragmentTracker.getState(frag);\n    this.fragCurrent = frag;\n\n    // we force a frag loading in audio switch as fragment tracker might not have evicted previous frags in case of quick audio switch\n    if (\n      this.switchingTrack ||\n      fragState === FragmentState.NOT_LOADED ||\n      fragState === FragmentState.PARTIAL\n    ) {\n      if (frag.sn === 'initSegment') {\n        this._loadInitSegment(frag, track);\n      } else if (track.details?.live && !this.initPTS[frag.cc]) {\n        this.log(\n          `Waiting for video PTS in continuity counter ${frag.cc} of live stream before loading audio fragment ${frag.sn} of level ${this.trackId}`,\n        );\n        this.state = State.WAITING_INIT_PTS;\n        const mainDetails = this.mainDetails;\n        if (\n          mainDetails &&\n          mainDetails.fragments[0].start !== track.details.fragments[0].start\n        ) {\n          alignMediaPlaylistByPDT(track.details, mainDetails);\n        }\n      } else {\n        this.startFragRequested = true;\n        super.loadFragment(frag, track, targetBufferTime);\n      }\n    } else {\n      this.clearTrackerIfNeeded(frag);\n    }\n  }\n\n  private flushAudioIfNeeded(switchingTrack: MediaPlaylist) {\n    const { media, bufferedTrack } = this;\n    const bufferedAttributes = bufferedTrack?.attrs;\n    const switchAttributes = switchingTrack.attrs;\n    if (\n      media &&\n      bufferedAttributes &&\n      (bufferedAttributes.CHANNELS !== switchAttributes.CHANNELS ||\n        bufferedTrack.name !== switchingTrack.name ||\n        bufferedTrack.lang !== switchingTrack.lang)\n    ) {\n      this.log('Switching audio track : flushing all audio');\n      super.flushMainBuffer(0, Number.POSITIVE_INFINITY, 'audio');\n      this.bufferedTrack = null;\n    }\n  }\n\n  private completeAudioSwitch(switchingTrack: MediaPlaylist) {\n    const { hls } = this;\n    this.flushAudioIfNeeded(switchingTrack);\n    this.bufferedTrack = switchingTrack;\n    this.switchingTrack = null;\n    hls.trigger(Events.AUDIO_TRACK_SWITCHED, { ...switchingTrack });\n  }\n}\nexport default AudioStreamController;\n", "import BasePlaylistController from './base-playlist-controller';\nimport { Events } from '../events';\nimport { ErrorTypes, ErrorDetails } from '../errors';\nimport { PlaylistContextType } from '../types/loader';\nimport { mediaAttributesIdentical } from '../utils/media-option-attributes';\nimport {\n  audioMatchPredicate,\n  findClosestLevelWithAudioGroup,\n  findMatchingOption,\n  matchesOption,\n} from '../utils/rendition-helper';\nimport type Hls from '../hls';\nimport type {\n  AudioSelectionOption,\n  MediaPlaylist,\n} from '../types/media-playlist';\nimport type { HlsUrlParameters } from '../types/level';\nimport type {\n  ManifestParsedData,\n  AudioTracksUpdatedData,\n  ErrorData,\n  LevelLoadingData,\n  AudioTrackLoadedData,\n  LevelSwitchingData,\n} from '../types/events';\n\nclass AudioTrackController extends BasePlaylistController {\n  private tracks: MediaPlaylist[] = [];\n  private groupIds: (string | undefined)[] | null = null;\n  private tracksInGroup: MediaPlaylist[] = [];\n  private trackId: number = -1;\n  private currentTrack: MediaPlaylist | null = null;\n  private selectDefaultTrack: boolean = true;\n\n  constructor(hls: Hls) {\n    super(hls, '[audio-track-controller]');\n    this.registerListeners();\n  }\n\n  private registerListeners() {\n    const { hls } = this;\n    hls.on(Events.MANIFEST_LOADING, this.onManifestLoading, this);\n    hls.on(Events.MANIFEST_PARSED, this.onManifestParsed, this);\n    hls.on(Events.LEVEL_LOADING, this.onLevelLoading, this);\n    hls.on(Events.LEVEL_SWITCHING, this.onLevelSwitching, this);\n    hls.on(Events.AUDIO_TRACK_LOADED, this.onAudioTrackLoaded, this);\n    hls.on(Events.ERROR, this.onError, this);\n  }\n\n  private unregisterListeners() {\n    const { hls } = this;\n    hls.off(Events.MANIFEST_LOADING, this.onManifestLoading, this);\n    hls.off(Events.MANIFEST_PARSED, this.onManifestParsed, this);\n    hls.off(Events.LEVEL_LOADING, this.onLevelLoading, this);\n    hls.off(Events.LEVEL_SWITCHING, this.onLevelSwitching, this);\n    hls.off(Events.AUDIO_TRACK_LOADED, this.onAudioTrackLoaded, this);\n    hls.off(Events.ERROR, this.onError, this);\n  }\n\n  public destroy() {\n    this.unregisterListeners();\n    this.tracks.length = 0;\n    this.tracksInGroup.length = 0;\n    this.currentTrack = null;\n    super.destroy();\n  }\n\n  protected onManifestLoading(): void {\n    this.tracks = [];\n    this.tracksInGroup = [];\n    this.groupIds = null;\n    this.currentTrack = null;\n    this.trackId = -1;\n    this.selectDefaultTrack = true;\n  }\n\n  protected onManifestParsed(\n    event: Events.MANIFEST_PARSED,\n    data: ManifestParsedData,\n  ): void {\n    this.tracks = data.audioTracks || [];\n  }\n\n  protected onAudioTrackLoaded(\n    event: Events.AUDIO_TRACK_LOADED,\n    data: AudioTrackLoadedData,\n  ): void {\n    const { id, groupId, details } = data;\n    const trackInActiveGroup = this.tracksInGroup[id];\n\n    if (!trackInActiveGroup || trackInActiveGroup.groupId !== groupId) {\n      this.warn(\n        `Audio track with id:${id} and group:${groupId} not found in active group ${trackInActiveGroup?.groupId}`,\n      );\n      return;\n    }\n\n    const curDetails = trackInActiveGroup.details;\n    trackInActiveGroup.details = data.details;\n    this.log(\n      `Audio track ${id} \"${trackInActiveGroup.name}\" lang:${trackInActiveGroup.lang} group:${groupId} loaded [${details.startSN}-${details.endSN}]`,\n    );\n\n    if (id === this.trackId) {\n      this.playlistLoaded(id, data, curDetails);\n    }\n  }\n\n  protected onLevelLoading(\n    event: Events.LEVEL_LOADING,\n    data: LevelLoadingData,\n  ): void {\n    this.switchLevel(data.level);\n  }\n\n  protected onLevelSwitching(\n    event: Events.LEVEL_SWITCHING,\n    data: LevelSwitchingData,\n  ): void {\n    this.switchLevel(data.level);\n  }\n\n  private switchLevel(levelIndex: number) {\n    const levelInfo = this.hls.levels[levelIndex];\n    if (!levelInfo) {\n      return;\n    }\n    const audioGroups = levelInfo.audioGroups || null;\n    const currentGroups = this.groupIds;\n    let currentTrack = this.currentTrack;\n    if (\n      !audioGroups ||\n      currentGroups?.length !== audioGroups?.length ||\n      audioGroups?.some((groupId) => currentGroups?.indexOf(groupId) === -1)\n    ) {\n      this.groupIds = audioGroups;\n      this.trackId = -1;\n      this.currentTrack = null;\n\n      const audioTracks = this.tracks.filter(\n        (track): boolean =>\n          !audioGroups || audioGroups.indexOf(track.groupId) !== -1,\n      );\n      if (audioTracks.length) {\n        // Disable selectDefaultTrack if there are no default tracks\n        if (\n          this.selectDefaultTrack &&\n          !audioTracks.some((track) => track.default)\n        ) {\n          this.selectDefaultTrack = false;\n        }\n        // track.id should match hls.audioTracks index\n        audioTracks.forEach((track, i) => {\n          track.id = i;\n        });\n      } else if (!currentTrack && !this.tracksInGroup.length) {\n        // Do not dispatch AUDIO_TRACKS_UPDATED when there were and are no tracks\n        return;\n      }\n      this.tracksInGroup = audioTracks;\n\n      // Find preferred track\n      const audioPreference = this.hls.config.audioPreference;\n      if (!currentTrack && audioPreference) {\n        const groupIndex = findMatchingOption(\n          audioPreference,\n          audioTracks,\n          audioMatchPredicate,\n        );\n        if (groupIndex > -1) {\n          currentTrack = audioTracks[groupIndex];\n        } else {\n          const allIndex = findMatchingOption(audioPreference, this.tracks);\n          currentTrack = this.tracks[allIndex];\n        }\n      }\n\n      // Select initial track\n      let trackId = this.findTrackId(currentTrack);\n      if (trackId === -1 && currentTrack) {\n        trackId = this.findTrackId(null);\n      }\n\n      // Dispatch events and load track if needed\n      const audioTracksUpdated: AudioTracksUpdatedData = { audioTracks };\n      this.log(\n        `Updating audio tracks, ${\n          audioTracks.length\n        } track(s) found in group(s): ${audioGroups?.join(',')}`,\n      );\n      this.hls.trigger(Events.AUDIO_TRACKS_UPDATED, audioTracksUpdated);\n\n      const selectedTrackId = this.trackId;\n      if (trackId !== -1 && selectedTrackId === -1) {\n        this.setAudioTrack(trackId);\n      } else if (audioTracks.length && selectedTrackId === -1) {\n        const error = new Error(\n          `No audio track selected for current audio group-ID(s): ${this.groupIds?.join(\n            ',',\n          )} track count: ${audioTracks.length}`,\n        );\n        this.warn(error.message);\n\n        this.hls.trigger(Events.ERROR, {\n          type: ErrorTypes.MEDIA_ERROR,\n          details: ErrorDetails.AUDIO_TRACK_LOAD_ERROR,\n          fatal: true,\n          error,\n        });\n      }\n    } else if (this.shouldReloadPlaylist(currentTrack)) {\n      // Retry playlist loading if no playlist is or has been loaded yet\n      this.setAudioTrack(this.trackId);\n    }\n  }\n\n  protected onError(event: Events.ERROR, data: ErrorData): void {\n    if (data.fatal || !data.context) {\n      return;\n    }\n\n    if (\n      data.context.type === PlaylistContextType.AUDIO_TRACK &&\n      data.context.id === this.trackId &&\n      (!this.groupIds || this.groupIds.indexOf(data.context.groupId) !== -1)\n    ) {\n      this.requestScheduled = -1;\n      this.checkRetry(data);\n    }\n  }\n\n  get allAudioTracks(): MediaPlaylist[] {\n    return this.tracks;\n  }\n\n  get audioTracks(): MediaPlaylist[] {\n    return this.tracksInGroup;\n  }\n\n  get audioTrack(): number {\n    return this.trackId;\n  }\n\n  set audioTrack(newId: number) {\n    // If audio track is selected from API then don't choose from the manifest default track\n    this.selectDefaultTrack = false;\n    this.setAudioTrack(newId);\n  }\n\n  public setAudioOption(\n    audioOption: MediaPlaylist | AudioSelectionOption | undefined,\n  ): MediaPlaylist | null {\n    const hls = this.hls;\n    hls.config.audioPreference = audioOption;\n    if (audioOption) {\n      const allAudioTracks = this.allAudioTracks;\n      this.selectDefaultTrack = false;\n      if (allAudioTracks.length) {\n        // First see if current option matches (no switch op)\n        const currentTrack = this.currentTrack;\n        if (\n          currentTrack &&\n          matchesOption(audioOption, currentTrack, audioMatchPredicate)\n        ) {\n          return currentTrack;\n        }\n        // Find option in available tracks (tracksInGroup)\n        const groupIndex = findMatchingOption(\n          audioOption,\n          this.tracksInGroup,\n          audioMatchPredicate,\n        );\n        if (groupIndex > -1) {\n          const track = this.tracksInGroup[groupIndex];\n          this.setAudioTrack(groupIndex);\n          return track;\n        } else if (currentTrack) {\n          // Find option in nearest level audio group\n          let searchIndex = hls.loadLevel;\n          if (searchIndex === -1) {\n            searchIndex = hls.firstAutoLevel;\n          }\n          const switchIndex = findClosestLevelWithAudioGroup(\n            audioOption,\n            hls.levels,\n            allAudioTracks,\n            searchIndex,\n            audioMatchPredicate,\n          );\n          if (switchIndex === -1) {\n            // could not find matching variant\n            return null;\n          }\n          // and switch level to acheive the audio group switch\n          hls.nextLoadLevel = switchIndex;\n        }\n        if (audioOption.channels || audioOption.audioCodec) {\n          // Could not find a match with codec / channels predicate\n          // Find a match without channels or codec\n          const withoutCodecAndChannelsMatch = findMatchingOption(\n            audioOption,\n            allAudioTracks,\n          );\n          if (withoutCodecAndChannelsMatch > -1) {\n            return allAudioTracks[withoutCodecAndChannelsMatch];\n          }\n        }\n      }\n    }\n    return null;\n  }\n\n  private setAudioTrack(newId: number): void {\n    const tracks = this.tracksInGroup;\n\n    // check if level idx is valid\n    if (newId < 0 || newId >= tracks.length) {\n      this.warn(`Invalid audio track id: ${newId}`);\n      return;\n    }\n\n    // stopping live reloading timer if any\n    this.clearTimer();\n\n    this.selectDefaultTrack = false;\n    const lastTrack = this.currentTrack;\n    const track = tracks[newId];\n    const trackLoaded = track.details && !track.details.live;\n    if (newId === this.trackId && track === lastTrack && trackLoaded) {\n      return;\n    }\n    this.log(\n      `Switching to audio-track ${newId} \"${track.name}\" lang:${track.lang} group:${track.groupId} channels:${track.channels}`,\n    );\n    this.trackId = newId;\n    this.currentTrack = track;\n    this.hls.trigger(Events.AUDIO_TRACK_SWITCHING, { ...track });\n    // Do not reload track unless live\n    if (trackLoaded) {\n      return;\n    }\n    const hlsUrlParameters = this.switchParams(\n      track.url,\n      lastTrack?.details,\n      track.details,\n    );\n    this.loadPlaylist(hlsUrlParameters);\n  }\n\n  private findTrackId(currentTrack: MediaPlaylist | null): number {\n    const audioTracks = this.tracksInGroup;\n    for (let i = 0; i < audioTracks.length; i++) {\n      const track = audioTracks[i];\n      if (this.selectDefaultTrack && !track.default) {\n        continue;\n      }\n      if (\n        !currentTrack ||\n        matchesOption(currentTrack, track, audioMatchPredicate)\n      ) {\n        return i;\n      }\n    }\n    if (currentTrack) {\n      const { name, lang, assocLang, characteristics, audioCodec, channels } =\n        currentTrack;\n      for (let i = 0; i < audioTracks.length; i++) {\n        const track = audioTracks[i];\n        if (\n          matchesOption(\n            { name, lang, assocLang, characteristics, audioCodec, channels },\n            track,\n            audioMatchPredicate,\n          )\n        ) {\n          return i;\n        }\n      }\n      for (let i = 0; i < audioTracks.length; i++) {\n        const track = audioTracks[i];\n        if (\n          mediaAttributesIdentical(currentTrack.attrs, track.attrs, [\n            'LANGUAGE',\n            'ASSOC-LANGUAGE',\n            'CHARACTERISTICS',\n          ])\n        ) {\n          return i;\n        }\n      }\n      for (let i = 0; i < audioTracks.length; i++) {\n        const track = audioTracks[i];\n        if (\n          mediaAttributesIdentical(currentTrack.attrs, track.attrs, [\n            'LANGUAGE',\n          ])\n        ) {\n          return i;\n        }\n      }\n    }\n    return -1;\n  }\n\n  protected loadPlaylist(hlsUrlParameters?: HlsUrlParameters): void {\n    const audioTrack = this.currentTrack;\n    if (this.shouldLoadPlaylist(audioTrack) && audioTrack) {\n      super.loadPlaylist();\n      const id = audioTrack.id;\n      const groupId = audioTrack.groupId as string;\n      let url = audioTrack.url;\n      if (hlsUrlParameters) {\n        try {\n          url = hlsUrlParameters.addDirectives(url);\n        } catch (error) {\n          this.warn(\n            `Could not construct new URL with HLS Delivery Directives: ${error}`,\n          );\n        }\n      }\n      // track not retrieved yet, or live playlist we need to (re)load it\n      this.log(\n        `loading audio-track playlist ${id} \"${audioTrack.name}\" lang:${audioTrack.lang} group:${groupId}`,\n      );\n      this.clearTimer();\n      this.hls.trigger(Events.AUDIO_TRACK_LOADING, {\n        url,\n        id,\n        groupId,\n        deliveryDirectives: hlsUrlParameters || null,\n      });\n    }\n  }\n}\n\nexport default AudioTrackController;\n", "import { Events } from '../events';\nimport { Bufferable, BufferHelper } from '../utils/buffer-helper';\nimport { findFragmentByPTS } from './fragment-finders';\nimport { alignMediaPlaylistByPDT } from '../utils/discontinuities';\nimport { addSliding } from '../utils/level-helper';\nimport { FragmentState } from './fragment-tracker';\nimport BaseStreamController, { State } from './base-stream-controller';\nimport { PlaylistLevelType } from '../types/loader';\nimport { Level } from '../types/level';\nimport { subtitleOptionsIdentical } from '../utils/media-option-attributes';\nimport { ErrorDetails, ErrorTypes } from '../errors';\nimport type { NetworkComponentAPI } from '../types/component-api';\nimport type Hls from '../hls';\nimport type { FragmentTracker } from './fragment-tracker';\nimport type KeyLoader from '../loader/key-loader';\nimport type { LevelDetails } from '../loader/level-details';\nimport type { Fragment } from '../loader/fragment';\nimport type {\n  ErrorData,\n  FragLoadedData,\n  SubtitleFragProcessed,\n  SubtitleTracksUpdatedData,\n  TrackLoadedData,\n  TrackSwitchedData,\n  BufferFlushingData,\n  LevelLoadedData,\n  FragBufferedData,\n} from '../types/events';\n\nconst TICK_INTERVAL = 500; // how often to tick in ms\n\ninterface TimeRange {\n  start: number;\n  end: number;\n}\n\nexport class SubtitleStreamController\n  extends BaseStreamController\n  implements NetworkComponentAPI\n{\n  private currentTrackId: number = -1;\n  private tracksBuffered: Array<TimeRange[]> = [];\n  private mainDetails: LevelDetails | null = null;\n\n  constructor(\n    hls: Hls,\n    fragmentTracker: FragmentTracker,\n    keyLoader: KeyLoader,\n  ) {\n    super(\n      hls,\n      fragmentTracker,\n      keyLoader,\n      '[subtitle-stream-controller]',\n      PlaylistLevelType.SUBTITLE,\n    );\n    this._registerListeners();\n  }\n\n  protected onHandlerDestroying() {\n    this._unregisterListeners();\n    super.onHandlerDestroying();\n    this.mainDetails = null;\n  }\n\n  private _registerListeners() {\n    const { hls } = this;\n    hls.on(Events.MEDIA_ATTACHED, this.onMediaAttached, this);\n    hls.on(Events.MEDIA_DETACHING, this.onMediaDetaching, this);\n    hls.on(Events.MANIFEST_LOADING, this.onManifestLoading, this);\n    hls.on(Events.LEVEL_LOADED, this.onLevelLoaded, this);\n    hls.on(Events.ERROR, this.onError, this);\n    hls.on(Events.SUBTITLE_TRACKS_UPDATED, this.onSubtitleTracksUpdated, this);\n    hls.on(Events.SUBTITLE_TRACK_SWITCH, this.onSubtitleTrackSwitch, this);\n    hls.on(Events.SUBTITLE_TRACK_LOADED, this.onSubtitleTrackLoaded, this);\n    hls.on(Events.SUBTITLE_FRAG_PROCESSED, this.onSubtitleFragProcessed, this);\n    hls.on(Events.BUFFER_FLUSHING, this.onBufferFlushing, this);\n    hls.on(Events.FRAG_BUFFERED, this.onFragBuffered, this);\n  }\n\n  private _unregisterListeners() {\n    const { hls } = this;\n    hls.off(Events.MEDIA_ATTACHED, this.onMediaAttached, this);\n    hls.off(Events.MEDIA_DETACHING, this.onMediaDetaching, this);\n    hls.off(Events.MANIFEST_LOADING, this.onManifestLoading, this);\n    hls.off(Events.LEVEL_LOADED, this.onLevelLoaded, this);\n    hls.off(Events.ERROR, this.onError, this);\n    hls.off(Events.SUBTITLE_TRACKS_UPDATED, this.onSubtitleTracksUpdated, this);\n    hls.off(Events.SUBTITLE_TRACK_SWITCH, this.onSubtitleTrackSwitch, this);\n    hls.off(Events.SUBTITLE_TRACK_LOADED, this.onSubtitleTrackLoaded, this);\n    hls.off(Events.SUBTITLE_FRAG_PROCESSED, this.onSubtitleFragProcessed, this);\n    hls.off(Events.BUFFER_FLUSHING, this.onBufferFlushing, this);\n    hls.off(Events.FRAG_BUFFERED, this.onFragBuffered, this);\n  }\n\n  startLoad(startPosition: number) {\n    this.stopLoad();\n    this.state = State.IDLE;\n\n    this.setInterval(TICK_INTERVAL);\n\n    this.nextLoadPosition =\n      this.startPosition =\n      this.lastCurrentTime =\n        startPosition;\n\n    this.tick();\n  }\n\n  onManifestLoading() {\n    this.mainDetails = null;\n    this.fragmentTracker.removeAllFragments();\n  }\n\n  onMediaDetaching(): void {\n    this.tracksBuffered = [];\n    super.onMediaDetaching();\n  }\n\n  onLevelLoaded(event: Events.LEVEL_LOADED, data: LevelLoadedData) {\n    this.mainDetails = data.details;\n  }\n\n  onSubtitleFragProcessed(\n    event: Events.SUBTITLE_FRAG_PROCESSED,\n    data: SubtitleFragProcessed,\n  ) {\n    const { frag, success } = data;\n    this.fragPrevious = frag;\n    this.state = State.IDLE;\n    if (!success) {\n      return;\n    }\n\n    const buffered = this.tracksBuffered[this.currentTrackId];\n    if (!buffered) {\n      return;\n    }\n\n    // Create/update a buffered array matching the interface used by BufferHelper.bufferedInfo\n    // so we can re-use the logic used to detect how much has been buffered\n    let timeRange: TimeRange | undefined;\n    const fragStart = frag.start;\n    for (let i = 0; i < buffered.length; i++) {\n      if (fragStart >= buffered[i].start && fragStart <= buffered[i].end) {\n        timeRange = buffered[i];\n        break;\n      }\n    }\n\n    const fragEnd = frag.start + frag.duration;\n    if (timeRange) {\n      timeRange.end = fragEnd;\n    } else {\n      timeRange = {\n        start: fragStart,\n        end: fragEnd,\n      };\n      buffered.push(timeRange);\n    }\n    this.fragmentTracker.fragBuffered(frag);\n    this.fragBufferedComplete(frag, null);\n  }\n\n  onBufferFlushing(event: Events.BUFFER_FLUSHING, data: BufferFlushingData) {\n    const { startOffset, endOffset } = data;\n    if (startOffset === 0 && endOffset !== Number.POSITIVE_INFINITY) {\n      const endOffsetSubtitles = endOffset - 1;\n      if (endOffsetSubtitles <= 0) {\n        return;\n      }\n      data.endOffsetSubtitles = Math.max(0, endOffsetSubtitles);\n      this.tracksBuffered.forEach((buffered) => {\n        for (let i = 0; i < buffered.length; ) {\n          if (buffered[i].end <= endOffsetSubtitles) {\n            buffered.shift();\n            continue;\n          } else if (buffered[i].start < endOffsetSubtitles) {\n            buffered[i].start = endOffsetSubtitles;\n          } else {\n            break;\n          }\n          i++;\n        }\n      });\n      this.fragmentTracker.removeFragmentsInRange(\n        startOffset,\n        endOffsetSubtitles,\n        PlaylistLevelType.SUBTITLE,\n      );\n    }\n  }\n\n  onFragBuffered(event: Events.FRAG_BUFFERED, data: FragBufferedData) {\n    if (!this.loadedmetadata && data.frag.type === PlaylistLevelType.MAIN) {\n      if (this.media?.buffered.length) {\n        this.loadedmetadata = true;\n      }\n    }\n  }\n\n  // If something goes wrong, proceed to next frag, if we were processing one.\n  onError(event: Events.ERROR, data: ErrorData) {\n    const frag = data.frag;\n\n    if (frag?.type === PlaylistLevelType.SUBTITLE) {\n      if (data.details === ErrorDetails.FRAG_GAP) {\n        this.fragmentTracker.fragBuffered(frag, true);\n      }\n      if (this.fragCurrent) {\n        this.fragCurrent.abortRequests();\n      }\n      if (this.state !== State.STOPPED) {\n        this.state = State.IDLE;\n      }\n    }\n  }\n\n  // Got all new subtitle levels.\n  onSubtitleTracksUpdated(\n    event: Events.SUBTITLE_TRACKS_UPDATED,\n    { subtitleTracks }: SubtitleTracksUpdatedData,\n  ) {\n    if (this.levels && subtitleOptionsIdentical(this.levels, subtitleTracks)) {\n      this.levels = subtitleTracks.map(\n        (mediaPlaylist) => new Level(mediaPlaylist),\n      );\n      return;\n    }\n    this.tracksBuffered = [];\n    this.levels = subtitleTracks.map((mediaPlaylist) => {\n      const level = new Level(mediaPlaylist);\n      this.tracksBuffered[level.id] = [];\n      return level;\n    });\n    this.fragmentTracker.removeFragmentsInRange(\n      0,\n      Number.POSITIVE_INFINITY,\n      PlaylistLevelType.SUBTITLE,\n    );\n    this.fragPrevious = null;\n    this.mediaBuffer = null;\n  }\n\n  onSubtitleTrackSwitch(\n    event: Events.SUBTITLE_TRACK_SWITCH,\n    data: TrackSwitchedData,\n  ) {\n    this.currentTrackId = data.id;\n\n    if (!this.levels?.length || this.currentTrackId === -1) {\n      this.clearInterval();\n      return;\n    }\n\n    // Check if track has the necessary details to load fragments\n    const currentTrack = this.levels[this.currentTrackId];\n    if (currentTrack?.details) {\n      this.mediaBuffer = this.mediaBufferTimeRanges;\n    } else {\n      this.mediaBuffer = null;\n    }\n    if (currentTrack) {\n      this.setInterval(TICK_INTERVAL);\n    }\n  }\n\n  // Got a new set of subtitle fragments.\n  onSubtitleTrackLoaded(\n    event: Events.SUBTITLE_TRACK_LOADED,\n    data: TrackLoadedData,\n  ) {\n    const { currentTrackId, levels } = this;\n    const { details: newDetails, id: trackId } = data;\n    if (!levels) {\n      this.warn(`Subtitle tracks were reset while loading level ${trackId}`);\n      return;\n    }\n    const track: Level = levels[trackId];\n    if (trackId >= levels.length || !track) {\n      return;\n    }\n    this.log(\n      `Subtitle track ${trackId} loaded [${newDetails.startSN},${\n        newDetails.endSN\n      }]${\n        newDetails.lastPartSn\n          ? `[part-${newDetails.lastPartSn}-${newDetails.lastPartIndex}]`\n          : ''\n      },duration:${newDetails.totalduration}`,\n    );\n    this.mediaBuffer = this.mediaBufferTimeRanges;\n    let sliding = 0;\n    if (newDetails.live || track.details?.live) {\n      const mainDetails = this.mainDetails;\n      if (newDetails.deltaUpdateFailed || !mainDetails) {\n        return;\n      }\n      const mainSlidingStartFragment = mainDetails.fragments[0];\n      if (!track.details) {\n        if (newDetails.hasProgramDateTime && mainDetails.hasProgramDateTime) {\n          alignMediaPlaylistByPDT(newDetails, mainDetails);\n          sliding = newDetails.fragments[0].start;\n        } else if (mainSlidingStartFragment) {\n          // line up live playlist with main so that fragments in range are loaded\n          sliding = mainSlidingStartFragment.start;\n          addSliding(newDetails, sliding);\n        }\n      } else {\n        sliding = this.alignPlaylists(\n          newDetails,\n          track.details,\n          this.levelLastLoaded?.details,\n        );\n        if (sliding === 0 && mainSlidingStartFragment) {\n          // realign with main when there is no overlap with last refresh\n          sliding = mainSlidingStartFragment.start;\n          addSliding(newDetails, sliding);\n        }\n      }\n    }\n    track.details = newDetails;\n    this.levelLastLoaded = track;\n\n    if (trackId !== currentTrackId) {\n      return;\n    }\n\n    if (!this.startFragRequested && (this.mainDetails || !newDetails.live)) {\n      this.setStartPosition(this.mainDetails || newDetails, sliding);\n    }\n\n    // trigger handler right now\n    this.tick();\n\n    // If playlist is misaligned because of bad PDT or drift, delete details to resync with main on reload\n    if (\n      newDetails.live &&\n      !this.fragCurrent &&\n      this.media &&\n      this.state === State.IDLE\n    ) {\n      const foundFrag = findFragmentByPTS(\n        null,\n        newDetails.fragments,\n        this.media.currentTime,\n        0,\n      );\n      if (!foundFrag) {\n        this.warn('Subtitle playlist not aligned with playback');\n        track.details = undefined;\n      }\n    }\n  }\n\n  _handleFragmentLoadComplete(fragLoadedData: FragLoadedData) {\n    const { frag, payload } = fragLoadedData;\n    const decryptData = frag.decryptdata;\n    const hls = this.hls;\n\n    if (this.fragContextChanged(frag)) {\n      return;\n    }\n    // check to see if the payload needs to be decrypted\n    if (\n      payload &&\n      payload.byteLength > 0 &&\n      decryptData?.key &&\n      decryptData.iv &&\n      decryptData.method === 'AES-128'\n    ) {\n      const startTime = performance.now();\n      // decrypt the subtitles\n      this.decrypter\n        .decrypt(\n          new Uint8Array(payload),\n          decryptData.key.buffer,\n          decryptData.iv.buffer,\n        )\n        .catch((err) => {\n          hls.trigger(Events.ERROR, {\n            type: ErrorTypes.MEDIA_ERROR,\n            details: ErrorDetails.FRAG_DECRYPT_ERROR,\n            fatal: false,\n            error: err,\n            reason: err.message,\n            frag,\n          });\n          throw err;\n        })\n        .then((decryptedData) => {\n          const endTime = performance.now();\n          hls.trigger(Events.FRAG_DECRYPTED, {\n            frag,\n            payload: decryptedData,\n            stats: {\n              tstart: startTime,\n              tdecrypt: endTime,\n            },\n          });\n        })\n        .catch((err) => {\n          this.warn(`${err.name}: ${err.message}`);\n          this.state = State.IDLE;\n        });\n    }\n  }\n\n  doTick() {\n    if (!this.media) {\n      this.state = State.IDLE;\n      return;\n    }\n\n    if (this.state === State.IDLE) {\n      const { currentTrackId, levels } = this;\n      const track = levels?.[currentTrackId];\n      if (!track || !levels.length || !track.details) {\n        return;\n      }\n      const { config } = this;\n      const currentTime = this.getLoadPosition();\n      const bufferedInfo = BufferHelper.bufferedInfo(\n        this.tracksBuffered[this.currentTrackId] || [],\n        currentTime,\n        config.maxBufferHole,\n      );\n      const { end: targetBufferTime, len: bufferLen } = bufferedInfo;\n\n      const mainBufferInfo = this.getFwdBufferInfo(\n        this.media,\n        PlaylistLevelType.MAIN,\n      );\n      const trackDetails = track.details as LevelDetails;\n      const maxBufLen =\n        this.getMaxBufferLength(mainBufferInfo?.len) +\n        trackDetails.levelTargetDuration;\n\n      if (bufferLen > maxBufLen) {\n        return;\n      }\n      const fragments = trackDetails.fragments;\n      const fragLen = fragments.length;\n      const end = trackDetails.edge;\n\n      let foundFrag: Fragment | null = null;\n      const fragPrevious = this.fragPrevious;\n      if (targetBufferTime < end) {\n        const tolerance = config.maxFragLookUpTolerance;\n        const lookupTolerance =\n          targetBufferTime > end - tolerance ? 0 : tolerance;\n        foundFrag = findFragmentByPTS(\n          fragPrevious,\n          fragments,\n          Math.max(fragments[0].start, targetBufferTime),\n          lookupTolerance,\n        );\n        if (\n          !foundFrag &&\n          fragPrevious &&\n          fragPrevious.start < fragments[0].start\n        ) {\n          foundFrag = fragments[0];\n        }\n      } else {\n        foundFrag = fragments[fragLen - 1];\n      }\n      if (!foundFrag) {\n        return;\n      }\n      foundFrag = this.mapToInitFragWhenRequired(foundFrag) as Fragment;\n      if (foundFrag.sn !== 'initSegment') {\n        // Load earlier fragment in same discontinuity to make up for misaligned playlists and cues that extend beyond end of segment\n        const curSNIdx = foundFrag.sn - trackDetails.startSN;\n        const prevFrag = fragments[curSNIdx - 1];\n        if (\n          prevFrag &&\n          prevFrag.cc === foundFrag.cc &&\n          this.fragmentTracker.getState(prevFrag) === FragmentState.NOT_LOADED\n        ) {\n          foundFrag = prevFrag;\n        }\n      }\n      if (\n        this.fragmentTracker.getState(foundFrag) === FragmentState.NOT_LOADED\n      ) {\n        // only load if fragment is not loaded\n        this.loadFragment(foundFrag, track, targetBufferTime);\n      }\n    }\n  }\n\n  protected getMaxBufferLength(mainBufferLength?: number): number {\n    const maxConfigBuffer = super.getMaxBufferLength();\n    if (!mainBufferLength) {\n      return maxConfigBuffer;\n    }\n    return Math.max(maxConfigBuffer, mainBufferLength);\n  }\n\n  protected loadFragment(\n    frag: Fragment,\n    level: Level,\n    targetBufferTime: number,\n  ) {\n    this.fragCurrent = frag;\n    if (frag.sn === 'initSegment') {\n      this._loadInitSegment(frag, level);\n    } else {\n      this.startFragRequested = true;\n      super.loadFragment(frag, level, targetBufferTime);\n    }\n  }\n\n  get mediaBufferTimeRanges(): Bufferable {\n    return new BufferableInstance(\n      this.tracksBuffered[this.currentTrackId] || [],\n    );\n  }\n}\n\nclass BufferableInstance implements Bufferable {\n  public readonly buffered: TimeRanges;\n\n  constructor(timeranges: TimeRange[]) {\n    const getRange = (\n      name: 'start' | 'end',\n      index: number,\n      length: number,\n    ): number => {\n      index = index >>> 0;\n      if (index > length - 1) {\n        throw new DOMException(\n          `Failed to execute '${name}' on 'TimeRanges': The index provided (${index}) is greater than the maximum bound (${length})`,\n        );\n      }\n      return timeranges[index][name];\n    };\n    this.buffered = {\n      get length() {\n        return timeranges.length;\n      },\n      end(index: number): number {\n        return getRange('end', index, timeranges.length);\n      },\n      start(index: number): number {\n        return getRange('start', index, timeranges.length);\n      },\n    };\n  }\n}\n", "import BasePlaylistController from './base-playlist-controller';\nimport { Events } from '../events';\nimport {\n  clearCurrentCues,\n  filterSubtitleTracks,\n} from '../utils/texttrack-utils';\nimport { PlaylistContextType } from '../types/loader';\nimport {\n  mediaAttributesIdentical,\n  subtitleTrackMatchesTextTrack,\n} from '../utils/media-option-attributes';\nimport { findMatchingOption, matchesOption } from '../utils/rendition-helper';\nimport type Hls from '../hls';\nimport type {\n  MediaPlaylist,\n  SubtitleSelectionOption,\n} from '../types/media-playlist';\nimport type { HlsUrlParameters } from '../types/level';\nimport type {\n  ErrorData,\n  LevelLoadingData,\n  MediaAttachedData,\n  SubtitleTracksUpdatedData,\n  ManifestParsedData,\n  TrackLoadedData,\n  LevelSwitchingData,\n} from '../types/events';\n\nclass SubtitleTrackController extends BasePlaylistController {\n  private media: HTMLMediaElement | null = null;\n  private tracks: MediaPlaylist[] = [];\n  private groupIds: (string | undefined)[] | null = null;\n  private tracksInGroup: MediaPlaylist[] = [];\n  private trackId: number = -1;\n  private currentTrack: MediaPlaylist | null = null;\n  private selectDefaultTrack: boolean = true;\n  private queuedDefaultTrack: number = -1;\n  private asyncPollTrackChange: () => void = () => this.pollTrackChange(0);\n  private useTextTrackPolling: boolean = false;\n  private subtitlePollingInterval: number = -1;\n  private _subtitleDisplay: boolean = true;\n\n  constructor(hls: Hls) {\n    super(hls, '[subtitle-track-controller]');\n    this.registerListeners();\n  }\n\n  public destroy() {\n    this.unregisterListeners();\n    this.tracks.length = 0;\n    this.tracksInGroup.length = 0;\n    this.currentTrack = null;\n    this.onTextTracksChanged = this.asyncPollTrackChange = null as any;\n    super.destroy();\n  }\n\n  public get subtitleDisplay(): boolean {\n    return this._subtitleDisplay;\n  }\n\n  public set subtitleDisplay(value: boolean) {\n    this._subtitleDisplay = value;\n    if (this.trackId > -1) {\n      this.toggleTrackModes();\n    }\n  }\n\n  private registerListeners() {\n    const { hls } = this;\n    hls.on(Events.MEDIA_ATTACHED, this.onMediaAttached, this);\n    hls.on(Events.MEDIA_DETACHING, this.onMediaDetaching, this);\n    hls.on(Events.MANIFEST_LOADING, this.onManifestLoading, this);\n    hls.on(Events.MANIFEST_PARSED, this.onManifestParsed, this);\n    hls.on(Events.LEVEL_LOADING, this.onLevelLoading, this);\n    hls.on(Events.LEVEL_SWITCHING, this.onLevelSwitching, this);\n    hls.on(Events.SUBTITLE_TRACK_LOADED, this.onSubtitleTrackLoaded, this);\n    hls.on(Events.ERROR, this.onError, this);\n  }\n\n  private unregisterListeners() {\n    const { hls } = this;\n    hls.off(Events.MEDIA_ATTACHED, this.onMediaAttached, this);\n    hls.off(Events.MEDIA_DETACHING, this.onMediaDetaching, this);\n    hls.off(Events.MANIFEST_LOADING, this.onManifestLoading, this);\n    hls.off(Events.MANIFEST_PARSED, this.onManifestParsed, this);\n    hls.off(Events.LEVEL_LOADING, this.onLevelLoading, this);\n    hls.off(Events.LEVEL_SWITCHING, this.onLevelSwitching, this);\n    hls.off(Events.SUBTITLE_TRACK_LOADED, this.onSubtitleTrackLoaded, this);\n    hls.off(Events.ERROR, this.onError, this);\n  }\n\n  // Listen for subtitle track change, then extract the current track ID.\n  protected onMediaAttached(\n    event: Events.MEDIA_ATTACHED,\n    data: MediaAttachedData,\n  ): void {\n    this.media = data.media;\n    if (!this.media) {\n      return;\n    }\n\n    if (this.queuedDefaultTrack > -1) {\n      this.subtitleTrack = this.queuedDefaultTrack;\n      this.queuedDefaultTrack = -1;\n    }\n\n    this.useTextTrackPolling = !(\n      this.media.textTracks && 'onchange' in this.media.textTracks\n    );\n    if (this.useTextTrackPolling) {\n      this.pollTrackChange(500);\n    } else {\n      this.media.textTracks.addEventListener(\n        'change',\n        this.asyncPollTrackChange,\n      );\n    }\n  }\n\n  private pollTrackChange(timeout: number) {\n    self.clearInterval(this.subtitlePollingInterval);\n    this.subtitlePollingInterval = self.setInterval(\n      this.onTextTracksChanged,\n      timeout,\n    );\n  }\n\n  protected onMediaDetaching(): void {\n    if (!this.media) {\n      return;\n    }\n\n    self.clearInterval(this.subtitlePollingInterval);\n    if (!this.useTextTrackPolling) {\n      this.media.textTracks.removeEventListener(\n        'change',\n        this.asyncPollTrackChange,\n      );\n    }\n\n    if (this.trackId > -1) {\n      this.queuedDefaultTrack = this.trackId;\n    }\n\n    const textTracks = filterSubtitleTracks(this.media.textTracks);\n    // Clear loaded cues on media detachment from tracks\n    textTracks.forEach((track) => {\n      clearCurrentCues(track);\n    });\n    // Disable all subtitle tracks before detachment so when reattached only tracks in that content are enabled.\n    this.subtitleTrack = -1;\n    this.media = null;\n  }\n\n  protected onManifestLoading(): void {\n    this.tracks = [];\n    this.groupIds = null;\n    this.tracksInGroup = [];\n    this.trackId = -1;\n    this.currentTrack = null;\n    this.selectDefaultTrack = true;\n  }\n\n  // Fired whenever a new manifest is loaded.\n  protected onManifestParsed(\n    event: Events.MANIFEST_PARSED,\n    data: ManifestParsedData,\n  ): void {\n    this.tracks = data.subtitleTracks;\n  }\n\n  protected onSubtitleTrackLoaded(\n    event: Events.SUBTITLE_TRACK_LOADED,\n    data: TrackLoadedData,\n  ): void {\n    const { id, groupId, details } = data;\n    const trackInActiveGroup = this.tracksInGroup[id];\n\n    if (!trackInActiveGroup || trackInActiveGroup.groupId !== groupId) {\n      this.warn(\n        `Subtitle track with id:${id} and group:${groupId} not found in active group ${trackInActiveGroup?.groupId}`,\n      );\n      return;\n    }\n\n    const curDetails = trackInActiveGroup.details;\n    trackInActiveGroup.details = data.details;\n    this.log(\n      `Subtitle track ${id} \"${trackInActiveGroup.name}\" lang:${trackInActiveGroup.lang} group:${groupId} loaded [${details.startSN}-${details.endSN}]`,\n    );\n\n    if (id === this.trackId) {\n      this.playlistLoaded(id, data, curDetails);\n    }\n  }\n\n  protected onLevelLoading(\n    event: Events.LEVEL_LOADING,\n    data: LevelLoadingData,\n  ): void {\n    this.switchLevel(data.level);\n  }\n\n  protected onLevelSwitching(\n    event: Events.LEVEL_SWITCHING,\n    data: LevelSwitchingData,\n  ): void {\n    this.switchLevel(data.level);\n  }\n\n  private switchLevel(levelIndex: number) {\n    const levelInfo = this.hls.levels[levelIndex];\n    if (!levelInfo) {\n      return;\n    }\n    const subtitleGroups = levelInfo.subtitleGroups || null;\n    const currentGroups = this.groupIds;\n    let currentTrack = this.currentTrack;\n    if (\n      !subtitleGroups ||\n      currentGroups?.length !== subtitleGroups?.length ||\n      subtitleGroups?.some((groupId) => currentGroups?.indexOf(groupId) === -1)\n    ) {\n      this.groupIds = subtitleGroups;\n      this.trackId = -1;\n      this.currentTrack = null;\n\n      const subtitleTracks = this.tracks.filter(\n        (track): boolean =>\n          !subtitleGroups || subtitleGroups.indexOf(track.groupId) !== -1,\n      );\n      if (subtitleTracks.length) {\n        // Disable selectDefaultTrack if there are no default tracks\n        if (\n          this.selectDefaultTrack &&\n          !subtitleTracks.some((track) => track.default)\n        ) {\n          this.selectDefaultTrack = false;\n        }\n        // track.id should match hls.audioTracks index\n        subtitleTracks.forEach((track, i) => {\n          track.id = i;\n        });\n      } else if (!currentTrack && !this.tracksInGroup.length) {\n        // Do not dispatch SUBTITLE_TRACKS_UPDATED when there were and are no tracks\n        return;\n      }\n      this.tracksInGroup = subtitleTracks;\n\n      // Find preferred track\n      const subtitlePreference = this.hls.config.subtitlePreference;\n      if (!currentTrack && subtitlePreference) {\n        this.selectDefaultTrack = false;\n        const groupIndex = findMatchingOption(\n          subtitlePreference,\n          subtitleTracks,\n        );\n        if (groupIndex > -1) {\n          currentTrack = subtitleTracks[groupIndex];\n        } else {\n          const allIndex = findMatchingOption(subtitlePreference, this.tracks);\n          currentTrack = this.tracks[allIndex];\n        }\n      }\n\n      // Select initial track\n      let trackId = this.findTrackId(currentTrack);\n      if (trackId === -1 && currentTrack) {\n        trackId = this.findTrackId(null);\n      }\n\n      // Dispatch events and load track if needed\n      const subtitleTracksUpdated: SubtitleTracksUpdatedData = {\n        subtitleTracks,\n      };\n      this.log(\n        `Updating subtitle tracks, ${\n          subtitleTracks.length\n        } track(s) found in \"${subtitleGroups?.join(',')}\" group-id`,\n      );\n      this.hls.trigger(Events.SUBTITLE_TRACKS_UPDATED, subtitleTracksUpdated);\n\n      if (trackId !== -1 && this.trackId === -1) {\n        this.setSubtitleTrack(trackId);\n      }\n    } else if (this.shouldReloadPlaylist(currentTrack)) {\n      // Retry playlist loading if no playlist is or has been loaded yet\n      this.setSubtitleTrack(this.trackId);\n    }\n  }\n\n  private findTrackId(currentTrack: MediaPlaylist | null): number {\n    const tracks = this.tracksInGroup;\n    const selectDefault = this.selectDefaultTrack;\n    for (let i = 0; i < tracks.length; i++) {\n      const track = tracks[i];\n      if (\n        (selectDefault && !track.default) ||\n        (!selectDefault && !currentTrack)\n      ) {\n        continue;\n      }\n      if (!currentTrack || matchesOption(track, currentTrack)) {\n        return i;\n      }\n    }\n    if (currentTrack) {\n      for (let i = 0; i < tracks.length; i++) {\n        const track = tracks[i];\n        if (\n          mediaAttributesIdentical(currentTrack.attrs, track.attrs, [\n            'LANGUAGE',\n            'ASSOC-LANGUAGE',\n            'CHARACTERISTICS',\n          ])\n        ) {\n          return i;\n        }\n      }\n      for (let i = 0; i < tracks.length; i++) {\n        const track = tracks[i];\n        if (\n          mediaAttributesIdentical(currentTrack.attrs, track.attrs, [\n            'LANGUAGE',\n          ])\n        ) {\n          return i;\n        }\n      }\n    }\n    return -1;\n  }\n\n  private findTrackForTextTrack(textTrack: TextTrack | null): number {\n    if (textTrack) {\n      const tracks = this.tracksInGroup;\n      for (let i = 0; i < tracks.length; i++) {\n        const track = tracks[i];\n        if (subtitleTrackMatchesTextTrack(track, textTrack)) {\n          return i;\n        }\n      }\n    }\n    return -1;\n  }\n\n  protected onError(event: Events.ERROR, data: ErrorData): void {\n    if (data.fatal || !data.context) {\n      return;\n    }\n\n    if (\n      data.context.type === PlaylistContextType.SUBTITLE_TRACK &&\n      data.context.id === this.trackId &&\n      (!this.groupIds || this.groupIds.indexOf(data.context.groupId) !== -1)\n    ) {\n      this.checkRetry(data);\n    }\n  }\n\n  get allSubtitleTracks(): MediaPlaylist[] {\n    return this.tracks;\n  }\n\n  /** get alternate subtitle tracks list from playlist **/\n  get subtitleTracks(): MediaPlaylist[] {\n    return this.tracksInGroup;\n  }\n\n  /** get/set index of the selected subtitle track (based on index in subtitle track lists) **/\n  get subtitleTrack(): number {\n    return this.trackId;\n  }\n\n  set subtitleTrack(newId: number) {\n    this.selectDefaultTrack = false;\n    this.setSubtitleTrack(newId);\n  }\n\n  public setSubtitleOption(\n    subtitleOption: MediaPlaylist | SubtitleSelectionOption | undefined,\n  ): MediaPlaylist | null {\n    this.hls.config.subtitlePreference = subtitleOption;\n    if (subtitleOption) {\n      const allSubtitleTracks = this.allSubtitleTracks;\n      this.selectDefaultTrack = false;\n      if (allSubtitleTracks.length) {\n        // First see if current option matches (no switch op)\n        const currentTrack = this.currentTrack;\n        if (currentTrack && matchesOption(subtitleOption, currentTrack)) {\n          return currentTrack;\n        }\n        // Find option in current group\n        const groupIndex = findMatchingOption(\n          subtitleOption,\n          this.tracksInGroup,\n        );\n        if (groupIndex > -1) {\n          const track = this.tracksInGroup[groupIndex];\n          this.setSubtitleTrack(groupIndex);\n          return track;\n        } else if (currentTrack) {\n          // If this is not the initial selection return null\n          // option should have matched one in active group\n          return null;\n        } else {\n          // Find the option in all tracks for initial selection\n          const allIndex = findMatchingOption(\n            subtitleOption,\n            allSubtitleTracks,\n          );\n          if (allIndex > -1) {\n            return allSubtitleTracks[allIndex];\n          }\n        }\n      }\n    }\n    return null;\n  }\n\n  protected loadPlaylist(hlsUrlParameters?: HlsUrlParameters): void {\n    super.loadPlaylist();\n    const currentTrack = this.currentTrack;\n    if (this.shouldLoadPlaylist(currentTrack) && currentTrack) {\n      const id = currentTrack.id;\n      const groupId = currentTrack.groupId as string;\n      let url = currentTrack.url;\n      if (hlsUrlParameters) {\n        try {\n          url = hlsUrlParameters.addDirectives(url);\n        } catch (error) {\n          this.warn(\n            `Could not construct new URL with HLS Delivery Directives: ${error}`,\n          );\n        }\n      }\n      this.log(`Loading subtitle playlist for id ${id}`);\n      this.hls.trigger(Events.SUBTITLE_TRACK_LOADING, {\n        url,\n        id,\n        groupId,\n        deliveryDirectives: hlsUrlParameters || null,\n      });\n    }\n  }\n\n  /**\n   * Disables the old subtitleTrack and sets current mode on the next subtitleTrack.\n   * This operates on the DOM textTracks.\n   * A value of -1 will disable all subtitle tracks.\n   */\n  private toggleTrackModes(): void {\n    const { media } = this;\n    if (!media) {\n      return;\n    }\n\n    const textTracks = filterSubtitleTracks(media.textTracks);\n    const currentTrack = this.currentTrack;\n    let nextTrack;\n    if (currentTrack) {\n      nextTrack = textTracks.filter((textTrack) =>\n        subtitleTrackMatchesTextTrack(currentTrack, textTrack),\n      )[0];\n      if (!nextTrack) {\n        this.warn(\n          `Unable to find subtitle TextTrack with name \"${currentTrack.name}\" and language \"${currentTrack.lang}\"`,\n        );\n      }\n    }\n    [].slice.call(textTracks).forEach((track) => {\n      if (track.mode !== 'disabled' && track !== nextTrack) {\n        track.mode = 'disabled';\n      }\n    });\n    if (nextTrack) {\n      const mode = this.subtitleDisplay ? 'showing' : 'hidden';\n      if (nextTrack.mode !== mode) {\n        nextTrack.mode = mode;\n      }\n    }\n  }\n\n  /**\n   * This method is responsible for validating the subtitle index and periodically reloading if live.\n   * Dispatches the SUBTITLE_TRACK_SWITCH event, which instructs the subtitle-stream-controller to load the selected track.\n   */\n  private setSubtitleTrack(newId: number): void {\n    const tracks = this.tracksInGroup;\n\n    // setting this.subtitleTrack will trigger internal logic\n    // if media has not been attached yet, it will fail\n    // we keep a reference to the default track id\n    // and we'll set subtitleTrack when onMediaAttached is triggered\n    if (!this.media) {\n      this.queuedDefaultTrack = newId;\n      return;\n    }\n\n    // exit if track id as already set or invalid\n    if (newId < -1 || newId >= tracks.length || !Number.isFinite(newId)) {\n      this.warn(`Invalid subtitle track id: ${newId}`);\n      return;\n    }\n\n    // stopping live reloading timer if any\n    this.clearTimer();\n\n    this.selectDefaultTrack = false;\n    const lastTrack = this.currentTrack;\n    const track: MediaPlaylist | null = tracks[newId] || null;\n    this.trackId = newId;\n    this.currentTrack = track;\n    this.toggleTrackModes();\n    if (!track) {\n      // switch to -1\n      this.hls.trigger(Events.SUBTITLE_TRACK_SWITCH, { id: newId });\n      return;\n    }\n    const trackLoaded = !!track.details && !track.details.live;\n    if (newId === this.trackId && track === lastTrack && trackLoaded) {\n      return;\n    }\n    this.log(\n      `Switching to subtitle-track ${newId}` +\n        (track\n          ? ` \"${track.name}\" lang:${track.lang} group:${track.groupId}`\n          : ''),\n    );\n    const { id, groupId = '', name, type, url } = track;\n    this.hls.trigger(Events.SUBTITLE_TRACK_SWITCH, {\n      id,\n      groupId,\n      name,\n      type,\n      url,\n    });\n    const hlsUrlParameters = this.switchParams(\n      track.url,\n      lastTrack?.details,\n      track.details,\n    );\n    this.loadPlaylist(hlsUrlParameters);\n  }\n\n  private onTextTracksChanged = () => {\n    if (!this.useTextTrackPolling) {\n      self.clearInterval(this.subtitlePollingInterval);\n    }\n    // Media is undefined when switching streams via loadSource()\n    if (!this.media || !this.hls.config.renderTextTracksNatively) {\n      return;\n    }\n\n    let textTrack: TextTrack | null = null;\n    const tracks = filterSubtitleTracks(this.media.textTracks);\n    for (let i = 0; i < tracks.length; i++) {\n      if (tracks[i].mode === 'hidden') {\n        // Do not break in case there is a following track with showing.\n        textTrack = tracks[i];\n      } else if (tracks[i].mode === 'showing') {\n        textTrack = tracks[i];\n        break;\n      }\n    }\n\n    // Find internal track index for TextTrack\n    const trackId = this.findTrackForTextTrack(textTrack);\n    if (this.subtitleTrack !== trackId) {\n      this.setSubtitleTrack(trackId);\n    }\n  };\n}\n\nexport default SubtitleTrackController;\n", "import { logger } from '../utils/logger';\nimport type {\n  BufferOperation,\n  BufferOperationQueues,\n  SourceBuffers,\n  SourceBufferName,\n} from '../types/buffer';\n\nexport default class BufferOperationQueue {\n  private buffers: SourceBuffers;\n  private queues: BufferOperationQueues = {\n    video: [],\n    audio: [],\n    audiovideo: [],\n  };\n\n  constructor(sourceBufferReference: SourceBuffers) {\n    this.buffers = sourceBufferReference;\n  }\n\n  public append(\n    operation: BufferOperation,\n    type: SourceBufferName,\n    pending?: boolean,\n  ) {\n    const queue = this.queues[type];\n    queue.push(operation);\n    if (queue.length === 1 && !pending) {\n      this.executeNext(type);\n    }\n  }\n\n  public insertAbort(operation: BufferOperation, type: SourceBufferName) {\n    const queue = this.queues[type];\n    queue.unshift(operation);\n    this.executeNext(type);\n  }\n\n  public appendBlocker(type: SourceBufferName): Promise<{}> {\n    let execute;\n    const promise: Promise<{}> = new Promise((resolve) => {\n      execute = resolve;\n    });\n    const operation: BufferOperation = {\n      execute,\n      onStart: () => {},\n      onComplete: () => {},\n      onError: () => {},\n    };\n\n    this.append(operation, type);\n    return promise;\n  }\n\n  public executeNext(type: SourceBufferName) {\n    const queue = this.queues[type];\n    if (queue.length) {\n      const operation: BufferOperation = queue[0];\n      try {\n        // Operations are expected to result in an 'updateend' event being fired. If not, the queue will lock. Operations\n        // which do not end with this event must call _onSBUpdateEnd manually\n        operation.execute();\n      } catch (error) {\n        logger.warn(\n          `[buffer-operation-queue]: Exception executing \"${type}\" SourceBuffer operation: ${error}`,\n        );\n        operation.onError(error);\n\n        // Only shift the current operation off, otherwise the updateend handler will do this for us\n        const sb = this.buffers[type];\n        if (!sb?.updating) {\n          this.shiftAndExecuteNext(type);\n        }\n      }\n    }\n  }\n\n  public shiftAndExecuteNext(type: SourceBufferName) {\n    this.queues[type].shift();\n    this.executeNext(type);\n  }\n\n  public current(type: SourceBufferName) {\n    return this.queues[type][0];\n  }\n}\n", "import { Events } from '../events';\nimport { logger } from '../utils/logger';\nimport { ErrorDetails, ErrorTypes } from '../errors';\nimport { BufferHelper } from '../utils/buffer-helper';\nimport {\n  getCodecCompatibleName,\n  pickMostCompleteCodecName,\n} from '../utils/codecs';\nimport {\n  getMediaSource,\n  isManagedMediaSource,\n} from '../utils/mediasource-helper';\nimport { ElementaryStreamTypes } from '../loader/fragment';\nimport type { TrackSet } from '../types/track';\nimport BufferOperationQueue from './buffer-operation-queue';\nimport {\n  BufferOperation,\n  SourceBuffers,\n  SourceBufferName,\n  SourceBufferListeners,\n} from '../types/buffer';\nimport type {\n  LevelUpdatedData,\n  BufferAppendingData,\n  MediaAttachingData,\n  ManifestParsedData,\n  BufferCodecsData,\n  BufferEOSData,\n  BufferFlushingData,\n  FragParsedData,\n  FragChangedData,\n  ErrorData,\n} from '../types/events';\nimport type { ComponentAPI } from '../types/component-api';\nimport type { ChunkMetadata } from '../types/transmuxer';\nimport type Hls from '../hls';\nimport type { LevelDetails } from '../loader/level-details';\nimport type { HlsConfig } from '../config';\n\nconst VIDEO_CODEC_PROFILE_REPLACE =\n  /(avc[1234]|hvc1|hev1|dvh[1e]|vp09|av01)(?:\\.[^.,]+)+/;\n\ninterface BufferedChangeEvent extends Event {\n  readonly addedRanges?: TimeRanges;\n  readonly removedRanges?: TimeRanges;\n}\n\nexport default class BufferController implements ComponentAPI {\n  // The level details used to determine duration, target-duration and live\n  private details: LevelDetails | null = null;\n  // cache the self generated object url to detect hijack of video tag\n  private _objectUrl: string | null = null;\n  // A queue of buffer operations which require the SourceBuffer to not be updating upon execution\n  private operationQueue!: BufferOperationQueue;\n  // References to event listeners for each SourceBuffer, so that they can be referenced for event removal\n  private listeners!: SourceBufferListeners;\n\n  private hls: Hls;\n\n  // The number of BUFFER_CODEC events received before any sourceBuffers are created\n  public bufferCodecEventsExpected: number = 0;\n\n  // The total number of BUFFER_CODEC events received\n  private _bufferCodecEventsTotal: number = 0;\n\n  // A reference to the attached media element\n  public media: HTMLMediaElement | null = null;\n\n  // A reference to the active media source\n  public mediaSource: MediaSource | null = null;\n\n  // Last MP3 audio chunk appended\n  private lastMpegAudioChunk: ChunkMetadata | null = null;\n\n  private appendSource: boolean;\n\n  // counters\n  public appendErrors = {\n    audio: 0,\n    video: 0,\n    audiovideo: 0,\n  };\n\n  public tracks: TrackSet = {};\n  public pendingTracks: TrackSet = {};\n  public sourceBuffer!: SourceBuffers;\n\n  protected log: (msg: any) => void;\n  protected warn: (msg: any, obj?: any) => void;\n  protected error: (msg: any, obj?: any) => void;\n\n  constructor(hls: Hls) {\n    this.hls = hls;\n    const logPrefix = '[buffer-controller]';\n    this.appendSource = isManagedMediaSource(\n      getMediaSource(hls.config.preferManagedMediaSource),\n    );\n    this.log = logger.log.bind(logger, logPrefix);\n    this.warn = logger.warn.bind(logger, logPrefix);\n    this.error = logger.error.bind(logger, logPrefix);\n    this._initSourceBuffer();\n    this.registerListeners();\n  }\n\n  public hasSourceTypes(): boolean {\n    return (\n      this.getSourceBufferTypes().length > 0 ||\n      Object.keys(this.pendingTracks).length > 0\n    );\n  }\n\n  public destroy() {\n    this.unregisterListeners();\n    this.details = null;\n    this.lastMpegAudioChunk = null;\n    // @ts-ignore\n    this.hls = null;\n  }\n\n  protected registerListeners() {\n    const { hls } = this;\n    hls.on(Events.MEDIA_ATTACHING, this.onMediaAttaching, this);\n    hls.on(Events.MEDIA_DETACHING, this.onMediaDetaching, this);\n    hls.on(Events.MANIFEST_LOADING, this.onManifestLoading, this);\n    hls.on(Events.MANIFEST_PARSED, this.onManifestParsed, this);\n    hls.on(Events.BUFFER_RESET, this.onBufferReset, this);\n    hls.on(Events.BUFFER_APPENDING, this.onBufferAppending, this);\n    hls.on(Events.BUFFER_CODECS, this.onBufferCodecs, this);\n    hls.on(Events.BUFFER_EOS, this.onBufferEos, this);\n    hls.on(Events.BUFFER_FLUSHING, this.onBufferFlushing, this);\n    hls.on(Events.LEVEL_UPDATED, this.onLevelUpdated, this);\n    hls.on(Events.FRAG_PARSED, this.onFragParsed, this);\n    hls.on(Events.FRAG_CHANGED, this.onFragChanged, this);\n  }\n\n  protected unregisterListeners() {\n    const { hls } = this;\n    hls.off(Events.MEDIA_ATTACHING, this.onMediaAttaching, this);\n    hls.off(Events.MEDIA_DETACHING, this.onMediaDetaching, this);\n    hls.off(Events.MANIFEST_LOADING, this.onManifestLoading, this);\n    hls.off(Events.MANIFEST_PARSED, this.onManifestParsed, this);\n    hls.off(Events.BUFFER_RESET, this.onBufferReset, this);\n    hls.off(Events.BUFFER_APPENDING, this.onBufferAppending, this);\n    hls.off(Events.BUFFER_CODECS, this.onBufferCodecs, this);\n    hls.off(Events.BUFFER_EOS, this.onBufferEos, this);\n    hls.off(Events.BUFFER_FLUSHING, this.onBufferFlushing, this);\n    hls.off(Events.LEVEL_UPDATED, this.onLevelUpdated, this);\n    hls.off(Events.FRAG_PARSED, this.onFragParsed, this);\n    hls.off(Events.FRAG_CHANGED, this.onFragChanged, this);\n  }\n\n  private _initSourceBuffer() {\n    this.sourceBuffer = {};\n    this.operationQueue = new BufferOperationQueue(this.sourceBuffer);\n    this.listeners = {\n      audio: [],\n      video: [],\n      audiovideo: [],\n    };\n    this.appendErrors = {\n      audio: 0,\n      video: 0,\n      audiovideo: 0,\n    };\n    this.lastMpegAudioChunk = null;\n  }\n\n  private onManifestLoading() {\n    this.bufferCodecEventsExpected = this._bufferCodecEventsTotal = 0;\n    this.details = null;\n  }\n\n  protected onManifestParsed(\n    event: Events.MANIFEST_PARSED,\n    data: ManifestParsedData,\n  ) {\n    // in case of alt audio 2 BUFFER_CODECS events will be triggered, one per stream controller\n    // sourcebuffers will be created all at once when the expected nb of tracks will be reached\n    // in case alt audio is not used, only one BUFFER_CODEC event will be fired from main stream controller\n    // it will contain the expected nb of source buffers, no need to compute it\n    let codecEvents: number = 2;\n    if ((data.audio && !data.video) || !data.altAudio || !__USE_ALT_AUDIO__) {\n      codecEvents = 1;\n    }\n    this.bufferCodecEventsExpected = this._bufferCodecEventsTotal = codecEvents;\n    this.log(`${this.bufferCodecEventsExpected} bufferCodec event(s) expected`);\n  }\n\n  protected onMediaAttaching(\n    event: Events.MEDIA_ATTACHING,\n    data: MediaAttachingData,\n  ) {\n    const media = (this.media = data.media);\n    const MediaSource = getMediaSource(this.appendSource);\n\n    if (media && MediaSource) {\n      const ms = (this.mediaSource = new MediaSource());\n      this.log(`created media source: ${ms.constructor?.name}`);\n      // MediaSource listeners are arrow functions with a lexical scope, and do not need to be bound\n      ms.addEventListener('sourceopen', this._onMediaSourceOpen);\n      ms.addEventListener('sourceended', this._onMediaSourceEnded);\n      ms.addEventListener('sourceclose', this._onMediaSourceClose);\n      if (this.appendSource) {\n        ms.addEventListener('startstreaming', this._onStartStreaming);\n        ms.addEventListener('endstreaming', this._onEndStreaming);\n      }\n\n      // cache the locally generated object url\n      const objectUrl = (this._objectUrl = self.URL.createObjectURL(ms));\n      // link video and media Source\n      if (this.appendSource) {\n        try {\n          media.removeAttribute('src');\n          // ManagedMediaSource will not open without disableRemotePlayback set to false or source alternatives\n          const MMS = (self as any).ManagedMediaSource;\n          media.disableRemotePlayback =\n            media.disableRemotePlayback || (MMS && ms instanceof MMS);\n          removeSourceChildren(media);\n          addSource(media, objectUrl);\n          media.load();\n        } catch (error) {\n          media.src = objectUrl;\n        }\n      } else {\n        media.src = objectUrl;\n      }\n      media.addEventListener('emptied', this._onMediaEmptied);\n    }\n  }\n  private _onEndStreaming = (event) => {\n    if (!this.hls) {\n      return;\n    }\n    this.hls.pauseBuffering();\n  };\n  private _onStartStreaming = (event) => {\n    if (!this.hls) {\n      return;\n    }\n    this.hls.resumeBuffering();\n  };\n\n  protected onMediaDetaching() {\n    const { media, mediaSource, _objectUrl } = this;\n    if (mediaSource) {\n      this.log('media source detaching');\n      if (mediaSource.readyState === 'open') {\n        try {\n          // endOfStream could trigger exception if any sourcebuffer is in updating state\n          // we don't really care about checking sourcebuffer state here,\n          // as we are anyway detaching the MediaSource\n          // let's just avoid this exception to propagate\n          mediaSource.endOfStream();\n        } catch (err) {\n          this.warn(\n            `onMediaDetaching: ${err.message} while calling endOfStream`,\n          );\n        }\n      }\n      // Clean up the SourceBuffers by invoking onBufferReset\n      this.onBufferReset();\n      mediaSource.removeEventListener('sourceopen', this._onMediaSourceOpen);\n      mediaSource.removeEventListener('sourceended', this._onMediaSourceEnded);\n      mediaSource.removeEventListener('sourceclose', this._onMediaSourceClose);\n      if (this.appendSource) {\n        mediaSource.removeEventListener(\n          'startstreaming',\n          this._onStartStreaming,\n        );\n        mediaSource.removeEventListener('endstreaming', this._onEndStreaming);\n      }\n\n      // Detach properly the MediaSource from the HTMLMediaElement as\n      // suggested in https://github.com/w3c/media-source/issues/53.\n      if (media) {\n        media.removeEventListener('emptied', this._onMediaEmptied);\n        if (_objectUrl) {\n          self.URL.revokeObjectURL(_objectUrl);\n        }\n\n        // clean up video tag src only if it's our own url. some external libraries might\n        // hijack the video tag and change its 'src' without destroying the Hls instance first\n        if (this.mediaSrc === _objectUrl) {\n          media.removeAttribute('src');\n          if (this.appendSource) {\n            removeSourceChildren(media);\n          }\n          media.load();\n        } else {\n          this.warn(\n            'media|source.src was changed by a third party - skip cleanup',\n          );\n        }\n      }\n\n      this.mediaSource = null;\n      this.media = null;\n      this._objectUrl = null;\n      this.bufferCodecEventsExpected = this._bufferCodecEventsTotal;\n      this.pendingTracks = {};\n      this.tracks = {};\n    }\n\n    this.hls.trigger(Events.MEDIA_DETACHED, undefined);\n  }\n\n  protected onBufferReset() {\n    this.getSourceBufferTypes().forEach((type) => {\n      this.resetBuffer(type);\n    });\n    this._initSourceBuffer();\n  }\n\n  private resetBuffer(type: SourceBufferName) {\n    const sb = this.sourceBuffer[type];\n    try {\n      if (sb) {\n        this.removeBufferListeners(type);\n        // Synchronously remove the SB from the map before the next call in order to prevent an async function from\n        // accessing it\n        this.sourceBuffer[type] = undefined;\n        if (this.mediaSource?.sourceBuffers.length) {\n          this.mediaSource.removeSourceBuffer(sb);\n        }\n      }\n    } catch (err) {\n      this.warn(`onBufferReset ${type}`, err);\n    }\n  }\n\n  protected onBufferCodecs(\n    event: Events.BUFFER_CODECS,\n    data: BufferCodecsData,\n  ) {\n    const sourceBufferCount = this.getSourceBufferTypes().length;\n    const trackNames = Object.keys(data);\n    trackNames.forEach((trackName) => {\n      if (sourceBufferCount) {\n        // check if SourceBuffer codec needs to change\n        const track = this.tracks[trackName];\n        if (track && typeof track.buffer.changeType === 'function') {\n          const { id, codec, levelCodec, container, metadata } =\n            data[trackName];\n          const currentCodecFull = pickMostCompleteCodecName(\n            track.codec,\n            track.levelCodec,\n          );\n          const currentCodec = currentCodecFull?.replace(\n            VIDEO_CODEC_PROFILE_REPLACE,\n            '$1',\n          );\n          let trackCodec = pickMostCompleteCodecName(codec, levelCodec);\n          const nextCodec = trackCodec?.replace(\n            VIDEO_CODEC_PROFILE_REPLACE,\n            '$1',\n          );\n          if (trackCodec && currentCodec !== nextCodec) {\n            if (trackName.slice(0, 5) === 'audio') {\n              trackCodec = getCodecCompatibleName(\n                trackCodec,\n                this.appendSource,\n              );\n            }\n            const mimeType = `${container};codecs=${trackCodec}`;\n            this.appendChangeType(trackName, mimeType);\n            this.log(`switching codec ${currentCodecFull} to ${trackCodec}`);\n            this.tracks[trackName] = {\n              buffer: track.buffer,\n              codec,\n              container,\n              levelCodec,\n              metadata,\n              id,\n            };\n          }\n        }\n      } else {\n        // if source buffer(s) not created yet, appended buffer tracks in this.pendingTracks\n        this.pendingTracks[trackName] = data[trackName];\n      }\n    });\n\n    // if sourcebuffers already created, do nothing ...\n    if (sourceBufferCount) {\n      return;\n    }\n\n    const bufferCodecEventsExpected = Math.max(\n      this.bufferCodecEventsExpected - 1,\n      0,\n    );\n    if (this.bufferCodecEventsExpected !== bufferCodecEventsExpected) {\n      this.log(\n        `${bufferCodecEventsExpected} bufferCodec event(s) expected ${trackNames.join(\n          ',',\n        )}`,\n      );\n      this.bufferCodecEventsExpected = bufferCodecEventsExpected;\n    }\n    if (this.mediaSource && this.mediaSource.readyState === 'open') {\n      this.checkPendingTracks();\n    }\n  }\n\n  protected appendChangeType(type, mimeType) {\n    const { operationQueue } = this;\n    const operation: BufferOperation = {\n      execute: () => {\n        const sb = this.sourceBuffer[type];\n        if (sb) {\n          this.log(`changing ${type} sourceBuffer type to ${mimeType}`);\n          sb.changeType(mimeType);\n        }\n        operationQueue.shiftAndExecuteNext(type);\n      },\n      onStart: () => {},\n      onComplete: () => {},\n      onError: (error: Error) => {\n        this.warn(`Failed to change ${type} SourceBuffer type`, error);\n      },\n    };\n\n    operationQueue.append(operation, type, !!this.pendingTracks[type]);\n  }\n\n  protected onBufferAppending(\n    event: Events.BUFFER_APPENDING,\n    eventData: BufferAppendingData,\n  ) {\n    const { hls, operationQueue, tracks } = this;\n    const { data, type, frag, part, chunkMeta } = eventData;\n    const chunkStats = chunkMeta.buffering[type];\n\n    const bufferAppendingStart = self.performance.now();\n    chunkStats.start = bufferAppendingStart;\n    const fragBuffering = frag.stats.buffering;\n    const partBuffering = part ? part.stats.buffering : null;\n    if (fragBuffering.start === 0) {\n      fragBuffering.start = bufferAppendingStart;\n    }\n    if (partBuffering && partBuffering.start === 0) {\n      partBuffering.start = bufferAppendingStart;\n    }\n\n    // TODO: Only update timestampOffset when audio/mpeg fragment or part is not contiguous with previously appended\n    // Adjusting `SourceBuffer.timestampOffset` (desired point in the timeline where the next frames should be appended)\n    // in Chrome browser when we detect MPEG audio container and time delta between level PTS and `SourceBuffer.timestampOffset`\n    // is greater than 100ms (this is enough to handle seek for VOD or level change for LIVE videos).\n    // More info here: https://github.com/video-dev/hls.js/issues/332#issuecomment-257986486\n    const audioTrack = tracks.audio;\n    let checkTimestampOffset = false;\n    if (type === 'audio' && audioTrack?.container === 'audio/mpeg') {\n      checkTimestampOffset =\n        !this.lastMpegAudioChunk ||\n        chunkMeta.id === 1 ||\n        this.lastMpegAudioChunk.sn !== chunkMeta.sn;\n      this.lastMpegAudioChunk = chunkMeta;\n    }\n\n    const fragStart = frag.start;\n    const operation: BufferOperation = {\n      execute: () => {\n        chunkStats.executeStart = self.performance.now();\n        if (checkTimestampOffset) {\n          const sb = this.sourceBuffer[type];\n          if (sb) {\n            const delta = fragStart - sb.timestampOffset;\n            if (Math.abs(delta) >= 0.1) {\n              this.log(\n                `Updating audio SourceBuffer timestampOffset to ${fragStart} (delta: ${delta}) sn: ${frag.sn})`,\n              );\n              sb.timestampOffset = fragStart;\n            }\n          }\n        }\n        this.appendExecutor(data, type);\n      },\n      onStart: () => {\n        // logger.debug(`[buffer-controller]: ${type} SourceBuffer updatestart`);\n      },\n      onComplete: () => {\n        // logger.debug(`[buffer-controller]: ${type} SourceBuffer updateend`);\n        const end = self.performance.now();\n        chunkStats.executeEnd = chunkStats.end = end;\n        if (fragBuffering.first === 0) {\n          fragBuffering.first = end;\n        }\n        if (partBuffering && partBuffering.first === 0) {\n          partBuffering.first = end;\n        }\n\n        const { sourceBuffer } = this;\n        const timeRanges = {};\n        for (const type in sourceBuffer) {\n          timeRanges[type] = BufferHelper.getBuffered(sourceBuffer[type]);\n        }\n        this.appendErrors[type] = 0;\n        if (type === 'audio' || type === 'video') {\n          this.appendErrors.audiovideo = 0;\n        } else {\n          this.appendErrors.audio = 0;\n          this.appendErrors.video = 0;\n        }\n        this.hls.trigger(Events.BUFFER_APPENDED, {\n          type,\n          frag,\n          part,\n          chunkMeta,\n          parent: frag.type,\n          timeRanges,\n        });\n      },\n      onError: (error: Error) => {\n        // in case any error occured while appending, put back segment in segments table\n        const event: ErrorData = {\n          type: ErrorTypes.MEDIA_ERROR,\n          parent: frag.type,\n          details: ErrorDetails.BUFFER_APPEND_ERROR,\n          sourceBufferName: type,\n          frag,\n          part,\n          chunkMeta,\n          error,\n          err: error,\n          fatal: false,\n        };\n\n        if ((error as DOMException).code === DOMException.QUOTA_EXCEEDED_ERR) {\n          // QuotaExceededError: http://www.w3.org/TR/html5/infrastructure.html#quotaexceedederror\n          // let's stop appending any segments, and report BUFFER_FULL_ERROR error\n          event.details = ErrorDetails.BUFFER_FULL_ERROR;\n        } else {\n          const appendErrorCount = ++this.appendErrors[type];\n          event.details = ErrorDetails.BUFFER_APPEND_ERROR;\n          /* with UHD content, we could get loop of quota exceeded error until\n            browser is able to evict some data from sourcebuffer. Retrying can help recover.\n          */\n          this.warn(\n            `Failed ${appendErrorCount}/${hls.config.appendErrorMaxRetry} times to append segment in \"${type}\" sourceBuffer`,\n          );\n          if (appendErrorCount >= hls.config.appendErrorMaxRetry) {\n            event.fatal = true;\n          }\n        }\n        hls.trigger(Events.ERROR, event);\n      },\n    };\n    operationQueue.append(operation, type, !!this.pendingTracks[type]);\n  }\n\n  protected onBufferFlushing(\n    event: Events.BUFFER_FLUSHING,\n    data: BufferFlushingData,\n  ) {\n    const { operationQueue } = this;\n    const flushOperation = (type: SourceBufferName): BufferOperation => ({\n      execute: this.removeExecutor.bind(\n        this,\n        type,\n        data.startOffset,\n        data.endOffset,\n      ),\n      onStart: () => {\n        // logger.debug(`[buffer-controller]: Started flushing ${data.startOffset} -> ${data.endOffset} for ${type} Source Buffer`);\n      },\n      onComplete: () => {\n        // logger.debug(`[buffer-controller]: Finished flushing ${data.startOffset} -> ${data.endOffset} for ${type} Source Buffer`);\n        this.hls.trigger(Events.BUFFER_FLUSHED, { type });\n      },\n      onError: (error: Error) => {\n        this.warn(`Failed to remove from ${type} SourceBuffer`, error);\n      },\n    });\n\n    if (data.type) {\n      operationQueue.append(flushOperation(data.type), data.type);\n    } else {\n      this.getSourceBufferTypes().forEach((type: SourceBufferName) => {\n        operationQueue.append(flushOperation(type), type);\n      });\n    }\n  }\n\n  protected onFragParsed(event: Events.FRAG_PARSED, data: FragParsedData) {\n    const { frag, part } = data;\n    const buffersAppendedTo: Array<SourceBufferName> = [];\n    const elementaryStreams = part\n      ? part.elementaryStreams\n      : frag.elementaryStreams;\n    if (elementaryStreams[ElementaryStreamTypes.AUDIOVIDEO]) {\n      buffersAppendedTo.push('audiovideo');\n    } else {\n      if (elementaryStreams[ElementaryStreamTypes.AUDIO]) {\n        buffersAppendedTo.push('audio');\n      }\n      if (elementaryStreams[ElementaryStreamTypes.VIDEO]) {\n        buffersAppendedTo.push('video');\n      }\n    }\n\n    const onUnblocked = () => {\n      const now = self.performance.now();\n      frag.stats.buffering.end = now;\n      if (part) {\n        part.stats.buffering.end = now;\n      }\n      const stats = part ? part.stats : frag.stats;\n      this.hls.trigger(Events.FRAG_BUFFERED, {\n        frag,\n        part,\n        stats,\n        id: frag.type,\n      });\n    };\n\n    if (buffersAppendedTo.length === 0) {\n      this.warn(\n        `Fragments must have at least one ElementaryStreamType set. type: ${frag.type} level: ${frag.level} sn: ${frag.sn}`,\n      );\n    }\n\n    this.blockBuffers(onUnblocked, buffersAppendedTo);\n  }\n\n  private onFragChanged(event: Events.FRAG_CHANGED, data: FragChangedData) {\n    this.trimBuffers();\n  }\n\n  // on BUFFER_EOS mark matching sourcebuffer(s) as ended and trigger checkEos()\n  // an undefined data.type will mark all buffers as EOS.\n  protected onBufferEos(event: Events.BUFFER_EOS, data: BufferEOSData) {\n    const ended = this.getSourceBufferTypes().reduce((acc, type) => {\n      const sb = this.sourceBuffer[type];\n      if (sb && (!data.type || data.type === type)) {\n        sb.ending = true;\n        if (!sb.ended) {\n          sb.ended = true;\n          this.log(`${type} sourceBuffer now EOS`);\n        }\n      }\n      return acc && !!(!sb || sb.ended);\n    }, true);\n\n    if (ended) {\n      this.log(`Queueing mediaSource.endOfStream()`);\n      this.blockBuffers(() => {\n        this.getSourceBufferTypes().forEach((type) => {\n          const sb = this.sourceBuffer[type];\n          if (sb) {\n            sb.ending = false;\n          }\n        });\n        const { mediaSource } = this;\n        if (!mediaSource || mediaSource.readyState !== 'open') {\n          if (mediaSource) {\n            this.log(\n              `Could not call mediaSource.endOfStream(). mediaSource.readyState: ${mediaSource.readyState}`,\n            );\n          }\n          return;\n        }\n        this.log(`Calling mediaSource.endOfStream()`);\n        // Allow this to throw and be caught by the enqueueing function\n        mediaSource.endOfStream();\n      });\n    }\n  }\n\n  protected onLevelUpdated(\n    event: Events.LEVEL_UPDATED,\n    { details }: LevelUpdatedData,\n  ) {\n    if (!details.fragments.length) {\n      return;\n    }\n    this.details = details;\n\n    if (this.getSourceBufferTypes().length) {\n      this.blockBuffers(this.updateMediaElementDuration.bind(this));\n    } else {\n      this.updateMediaElementDuration();\n    }\n  }\n\n  trimBuffers() {\n    const { hls, details, media } = this;\n    if (!media || details === null) {\n      return;\n    }\n\n    const sourceBufferTypes = this.getSourceBufferTypes();\n    if (!sourceBufferTypes.length) {\n      return;\n    }\n\n    const config: Readonly<HlsConfig> = hls.config;\n    const currentTime = media.currentTime;\n    const targetDuration = details.levelTargetDuration;\n\n    // Support for deprecated liveBackBufferLength\n    const backBufferLength =\n      details.live && config.liveBackBufferLength !== null\n        ? config.liveBackBufferLength\n        : config.backBufferLength;\n\n    if (Number.isFinite(backBufferLength) && backBufferLength > 0) {\n      const maxBackBufferLength = Math.max(backBufferLength, targetDuration);\n      const targetBackBufferPosition =\n        Math.floor(currentTime / targetDuration) * targetDuration -\n        maxBackBufferLength;\n\n      this.flushBackBuffer(\n        currentTime,\n        targetDuration,\n        targetBackBufferPosition,\n      );\n    }\n\n    if (\n      Number.isFinite(config.frontBufferFlushThreshold) &&\n      config.frontBufferFlushThreshold > 0\n    ) {\n      const frontBufferLength = Math.max(\n        config.maxBufferLength,\n        config.frontBufferFlushThreshold,\n      );\n\n      const maxFrontBufferLength = Math.max(frontBufferLength, targetDuration);\n      const targetFrontBufferPosition =\n        Math.floor(currentTime / targetDuration) * targetDuration +\n        maxFrontBufferLength;\n\n      this.flushFrontBuffer(\n        currentTime,\n        targetDuration,\n        targetFrontBufferPosition,\n      );\n    }\n  }\n\n  flushBackBuffer(\n    currentTime: number,\n    targetDuration: number,\n    targetBackBufferPosition: number,\n  ) {\n    const { details, sourceBuffer } = this;\n    const sourceBufferTypes = this.getSourceBufferTypes();\n\n    sourceBufferTypes.forEach((type: SourceBufferName) => {\n      const sb = sourceBuffer[type];\n      if (sb) {\n        const buffered = BufferHelper.getBuffered(sb);\n        // when target buffer start exceeds actual buffer start\n        if (\n          buffered.length > 0 &&\n          targetBackBufferPosition > buffered.start(0)\n        ) {\n          this.hls.trigger(Events.BACK_BUFFER_REACHED, {\n            bufferEnd: targetBackBufferPosition,\n          });\n\n          // Support for deprecated event:\n          if (details?.live) {\n            this.hls.trigger(Events.LIVE_BACK_BUFFER_REACHED, {\n              bufferEnd: targetBackBufferPosition,\n            });\n          } else if (\n            sb.ended &&\n            buffered.end(buffered.length - 1) - currentTime < targetDuration * 2\n          ) {\n            this.log(\n              `Cannot flush ${type} back buffer while SourceBuffer is in ended state`,\n            );\n            return;\n          }\n\n          this.hls.trigger(Events.BUFFER_FLUSHING, {\n            startOffset: 0,\n            endOffset: targetBackBufferPosition,\n            type,\n          });\n        }\n      }\n    });\n  }\n\n  flushFrontBuffer(\n    currentTime: number,\n    targetDuration: number,\n    targetFrontBufferPosition: number,\n  ) {\n    const { sourceBuffer } = this;\n    const sourceBufferTypes = this.getSourceBufferTypes();\n\n    sourceBufferTypes.forEach((type: SourceBufferName) => {\n      const sb = sourceBuffer[type];\n      if (sb) {\n        const buffered = BufferHelper.getBuffered(sb);\n        const numBufferedRanges = buffered.length;\n        // The buffer is either empty or contiguous\n        if (numBufferedRanges < 2) {\n          return;\n        }\n        const bufferStart = buffered.start(numBufferedRanges - 1);\n        const bufferEnd = buffered.end(numBufferedRanges - 1);\n        // No flush if we can tolerate the current buffer length or the current buffer range we would flush is contiguous with current position\n        if (\n          targetFrontBufferPosition > bufferStart ||\n          (currentTime >= bufferStart && currentTime <= bufferEnd)\n        ) {\n          return;\n        } else if (sb.ended && currentTime - bufferEnd < 2 * targetDuration) {\n          this.log(\n            `Cannot flush ${type} front buffer while SourceBuffer is in ended state`,\n          );\n          return;\n        }\n\n        this.hls.trigger(Events.BUFFER_FLUSHING, {\n          startOffset: bufferStart,\n          endOffset: Infinity,\n          type,\n        });\n      }\n    });\n  }\n\n  /**\n   * Update Media Source duration to current level duration or override to Infinity if configuration parameter\n   * 'liveDurationInfinity` is set to `true`\n   * More details: https://github.com/video-dev/hls.js/issues/355\n   */\n  private updateMediaElementDuration() {\n    if (\n      !this.details ||\n      !this.media ||\n      !this.mediaSource ||\n      this.mediaSource.readyState !== 'open'\n    ) {\n      return;\n    }\n    const { details, hls, media, mediaSource } = this;\n    const levelDuration = details.fragments[0].start + details.totalduration;\n    const mediaDuration = media.duration;\n    const msDuration = Number.isFinite(mediaSource.duration)\n      ? mediaSource.duration\n      : 0;\n\n    if (details.live && hls.config.liveDurationInfinity) {\n      // Override duration to Infinity\n      mediaSource.duration = Infinity;\n      this.updateSeekableRange(details);\n    } else if (\n      (levelDuration > msDuration && levelDuration > mediaDuration) ||\n      !Number.isFinite(mediaDuration)\n    ) {\n      // levelDuration was the last value we set.\n      // not using mediaSource.duration as the browser may tweak this value\n      // only update Media Source duration if its value increase, this is to avoid\n      // flushing already buffered portion when switching between quality level\n      this.log(`Updating Media Source duration to ${levelDuration.toFixed(3)}`);\n      mediaSource.duration = levelDuration;\n    }\n  }\n\n  updateSeekableRange(levelDetails) {\n    const mediaSource = this.mediaSource;\n    const fragments = levelDetails.fragments;\n    const len = fragments.length;\n    if (len && levelDetails.live && mediaSource?.setLiveSeekableRange) {\n      const start = Math.max(0, fragments[0].start);\n      const end = Math.max(start, start + levelDetails.totalduration);\n      this.log(\n        `Media Source duration is set to ${mediaSource.duration}. Setting seekable range to ${start}-${end}.`,\n      );\n      mediaSource.setLiveSeekableRange(start, end);\n    }\n  }\n\n  protected checkPendingTracks() {\n    const { bufferCodecEventsExpected, operationQueue, pendingTracks } = this;\n\n    // Check if we've received all of the expected bufferCodec events. When none remain, create all the sourceBuffers at once.\n    // This is important because the MSE spec allows implementations to throw QuotaExceededErrors if creating new sourceBuffers after\n    // data has been appended to existing ones.\n    // 2 tracks is the max (one for audio, one for video). If we've reach this max go ahead and create the buffers.\n    const pendingTracksCount = Object.keys(pendingTracks).length;\n    if (\n      pendingTracksCount &&\n      (!bufferCodecEventsExpected ||\n        pendingTracksCount === 2 ||\n        'audiovideo' in pendingTracks)\n    ) {\n      // ok, let's create them now !\n      this.createSourceBuffers(pendingTracks);\n      this.pendingTracks = {};\n      // append any pending segments now !\n      const buffers = this.getSourceBufferTypes();\n      if (buffers.length) {\n        this.hls.trigger(Events.BUFFER_CREATED, { tracks: this.tracks });\n        buffers.forEach((type: SourceBufferName) => {\n          operationQueue.executeNext(type);\n        });\n      } else {\n        const error = new Error(\n          'could not create source buffer for media codec(s)',\n        );\n        this.hls.trigger(Events.ERROR, {\n          type: ErrorTypes.MEDIA_ERROR,\n          details: ErrorDetails.BUFFER_INCOMPATIBLE_CODECS_ERROR,\n          fatal: true,\n          error,\n          reason: error.message,\n        });\n      }\n    }\n  }\n\n  protected createSourceBuffers(tracks: TrackSet) {\n    const { sourceBuffer, mediaSource } = this;\n    if (!mediaSource) {\n      throw Error('createSourceBuffers called when mediaSource was null');\n    }\n    for (const trackName in tracks) {\n      if (!sourceBuffer[trackName]) {\n        const track = tracks[trackName as keyof TrackSet];\n        if (!track) {\n          throw Error(\n            `source buffer exists for track ${trackName}, however track does not`,\n          );\n        }\n        // use levelCodec as first priority unless it contains multiple comma-separated codec values\n        let codec =\n          track.levelCodec?.indexOf(',') === -1\n            ? track.levelCodec\n            : track.codec;\n        if (codec) {\n          if (trackName.slice(0, 5) === 'audio') {\n            codec = getCodecCompatibleName(codec, this.appendSource);\n          }\n        }\n        const mimeType = `${track.container};codecs=${codec}`;\n        this.log(`creating sourceBuffer(${mimeType})`);\n        try {\n          const sb = (sourceBuffer[trackName] =\n            mediaSource.addSourceBuffer(mimeType));\n          const sbName = trackName as SourceBufferName;\n          this.addBufferListener(sbName, 'updatestart', this._onSBUpdateStart);\n          this.addBufferListener(sbName, 'updateend', this._onSBUpdateEnd);\n          this.addBufferListener(sbName, 'error', this._onSBUpdateError);\n          // ManagedSourceBuffer bufferedchange event\n          if (this.appendSource) {\n            this.addBufferListener(\n              sbName,\n              'bufferedchange',\n              (type: SourceBufferName, event: BufferedChangeEvent) => {\n                // If media was ejected check for a change. Added ranges are redundant with changes on 'updateend' event.\n                const removedRanges = event.removedRanges;\n                if (removedRanges?.length) {\n                  this.hls.trigger(Events.BUFFER_FLUSHED, {\n                    type: trackName as SourceBufferName,\n                  });\n                }\n              },\n            );\n          }\n\n          this.tracks[trackName] = {\n            buffer: sb,\n            codec: codec,\n            container: track.container,\n            levelCodec: track.levelCodec,\n            metadata: track.metadata,\n            id: track.id,\n          };\n        } catch (err) {\n          this.error(`error while trying to add sourceBuffer: ${err.message}`);\n          this.hls.trigger(Events.ERROR, {\n            type: ErrorTypes.MEDIA_ERROR,\n            details: ErrorDetails.BUFFER_ADD_CODEC_ERROR,\n            fatal: false,\n            error: err,\n            sourceBufferName: trackName as SourceBufferName,\n            mimeType: mimeType,\n          });\n        }\n      }\n    }\n  }\n\n  // Keep as arrow functions so that we can directly reference these functions directly as event listeners\n  private _onMediaSourceOpen = () => {\n    const { media, mediaSource } = this;\n    this.log('Media source opened');\n    if (media) {\n      media.removeEventListener('emptied', this._onMediaEmptied);\n      this.updateMediaElementDuration();\n      this.hls.trigger(Events.MEDIA_ATTACHED, {\n        media,\n        mediaSource: mediaSource as MediaSource,\n      });\n    }\n\n    if (mediaSource) {\n      // once received, don't listen anymore to sourceopen event\n      mediaSource.removeEventListener('sourceopen', this._onMediaSourceOpen);\n    }\n    this.checkPendingTracks();\n  };\n\n  private _onMediaSourceClose = () => {\n    this.log('Media source closed');\n  };\n\n  private _onMediaSourceEnded = () => {\n    this.log('Media source ended');\n  };\n\n  private _onMediaEmptied = () => {\n    const { mediaSrc, _objectUrl } = this;\n    if (mediaSrc !== _objectUrl) {\n      logger.error(\n        `Media element src was set while attaching MediaSource (${_objectUrl} > ${mediaSrc})`,\n      );\n    }\n  };\n\n  private get mediaSrc(): string | undefined {\n    const media = this.media?.querySelector?.('source') || this.media;\n    return media?.src;\n  }\n\n  private _onSBUpdateStart(type: SourceBufferName) {\n    const { operationQueue } = this;\n    const operation = operationQueue.current(type);\n    operation.onStart();\n  }\n\n  private _onSBUpdateEnd(type: SourceBufferName) {\n    if (this.mediaSource?.readyState === 'closed') {\n      this.resetBuffer(type);\n      return;\n    }\n    const { operationQueue } = this;\n    const operation = operationQueue.current(type);\n    operation.onComplete();\n    operationQueue.shiftAndExecuteNext(type);\n  }\n\n  private _onSBUpdateError(type: SourceBufferName, event: Event) {\n    const error = new Error(\n      `${type} SourceBuffer error. MediaSource readyState: ${this.mediaSource?.readyState}`,\n    );\n    this.error(`${error}`, event);\n    // according to http://www.w3.org/TR/media-source/#sourcebuffer-append-error\n    // SourceBuffer errors are not necessarily fatal; if so, the HTMLMediaElement will fire an error event\n    this.hls.trigger(Events.ERROR, {\n      type: ErrorTypes.MEDIA_ERROR,\n      details: ErrorDetails.BUFFER_APPENDING_ERROR,\n      sourceBufferName: type,\n      error,\n      fatal: false,\n    });\n    // updateend is always fired after error, so we'll allow that to shift the current operation off of the queue\n    const operation = this.operationQueue.current(type);\n    if (operation) {\n      operation.onError(error);\n    }\n  }\n\n  // This method must result in an updateend event; if remove is not called, _onSBUpdateEnd must be called manually\n  private removeExecutor(\n    type: SourceBufferName,\n    startOffset: number,\n    endOffset: number,\n  ) {\n    const { media, mediaSource, operationQueue, sourceBuffer } = this;\n    const sb = sourceBuffer[type];\n    if (!media || !mediaSource || !sb) {\n      this.warn(\n        `Attempting to remove from the ${type} SourceBuffer, but it does not exist`,\n      );\n      operationQueue.shiftAndExecuteNext(type);\n      return;\n    }\n    const mediaDuration = Number.isFinite(media.duration)\n      ? media.duration\n      : Infinity;\n    const msDuration = Number.isFinite(mediaSource.duration)\n      ? mediaSource.duration\n      : Infinity;\n    const removeStart = Math.max(0, startOffset);\n    const removeEnd = Math.min(endOffset, mediaDuration, msDuration);\n    if (removeEnd > removeStart && (!sb.ending || sb.ended)) {\n      sb.ended = false;\n      this.log(\n        `Removing [${removeStart},${removeEnd}] from the ${type} SourceBuffer`,\n      );\n      sb.remove(removeStart, removeEnd);\n    } else {\n      // Cycle the queue\n      operationQueue.shiftAndExecuteNext(type);\n    }\n  }\n\n  // This method must result in an updateend event; if append is not called, _onSBUpdateEnd must be called manually\n  private appendExecutor(data: Uint8Array, type: SourceBufferName) {\n    const sb = this.sourceBuffer[type];\n    if (!sb) {\n      if (!this.pendingTracks[type]) {\n        throw new Error(\n          `Attempting to append to the ${type} SourceBuffer, but it does not exist`,\n        );\n      }\n      return;\n    }\n\n    sb.ended = false;\n    sb.appendBuffer(data);\n  }\n\n  // Enqueues an operation to each SourceBuffer queue which, upon execution, resolves a promise. When all promises\n  // resolve, the onUnblocked function is executed. Functions calling this method do not need to unblock the queue\n  // upon completion, since we already do it here\n  private blockBuffers(\n    onUnblocked: () => void,\n    buffers: Array<SourceBufferName> = this.getSourceBufferTypes(),\n  ) {\n    if (!buffers.length) {\n      this.log('Blocking operation requested, but no SourceBuffers exist');\n      Promise.resolve().then(onUnblocked);\n      return;\n    }\n    const { operationQueue } = this;\n\n    // logger.debug(`[buffer-controller]: Blocking ${buffers} SourceBuffer`);\n    const blockingOperations = buffers.map((type) =>\n      operationQueue.appendBlocker(type as SourceBufferName),\n    );\n    Promise.all(blockingOperations).then(() => {\n      // logger.debug(`[buffer-controller]: Blocking operation resolved; unblocking ${buffers} SourceBuffer`);\n      onUnblocked();\n      buffers.forEach((type) => {\n        const sb = this.sourceBuffer[type];\n        // Only cycle the queue if the SB is not updating. There's a bug in Chrome which sets the SB updating flag to\n        // true when changing the MediaSource duration (https://bugs.chromium.org/p/chromium/issues/detail?id=959359&can=2&q=mediasource%20duration)\n        // While this is a workaround, it's probably useful to have around\n        if (!sb?.updating) {\n          operationQueue.shiftAndExecuteNext(type);\n        }\n      });\n    });\n  }\n\n  private getSourceBufferTypes(): Array<SourceBufferName> {\n    return Object.keys(this.sourceBuffer) as Array<SourceBufferName>;\n  }\n\n  private addBufferListener(\n    type: SourceBufferName,\n    event: string,\n    fn: Function,\n  ) {\n    const buffer = this.sourceBuffer[type];\n    if (!buffer) {\n      return;\n    }\n    const listener = fn.bind(this, type);\n    this.listeners[type].push({ event, listener });\n    buffer.addEventListener(event, listener);\n  }\n\n  private removeBufferListeners(type: SourceBufferName) {\n    const buffer = this.sourceBuffer[type];\n    if (!buffer) {\n      return;\n    }\n    this.listeners[type].forEach((l) => {\n      buffer.removeEventListener(l.event, l.listener);\n    });\n  }\n}\n\nfunction removeSourceChildren(node: HTMLElement) {\n  const sourceChildren = node.querySelectorAll('source');\n  [].slice.call(sourceChildren).forEach((source) => {\n    node.removeChild(source);\n  });\n}\n\nfunction addSource(media: HTMLMediaElement, url: string) {\n  const source = self.document.createElement('source');\n  source.type = 'video/mp4';\n  source.src = url;\n  media.appendChild(source);\n}\n", "import OutputFilter from './output-filter';\nimport { logger } from '../utils/logger';\n\n/**\n *\n * This code was ported from the dash.js project at:\n *   https://github.com/Dash-Industry-Forum/dash.js/blob/development/externals/cea608-parser.js\n *   https://github.com/Dash-Industry-Forum/dash.js/commit/8269b26a761e0853bb21d78780ed945144ecdd4d#diff-71bc295a2d6b6b7093a1d3290d53a4b2\n *\n * The original copyright appears below:\n *\n * The copyright in this software is being made available under the BSD License,\n * included below. This software may be subject to other third party and contributor\n * rights, including patent rights, and no such rights are granted under this license.\n *\n * Copyright (c) 2015-2016, DASH Industry Forum.\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without modification,\n * are permitted provided that the following conditions are met:\n *  1. Redistributions of source code must retain the above copyright notice, this\n *  list of conditions and the following disclaimer.\n *  * Redistributions in binary form must reproduce the above copyright notice,\n *  this list of conditions and the following disclaimer in the documentation and/or\n *  other materials provided with the distribution.\n *  2. Neither the name of Dash Industry Forum nor the names of its\n *  contributors may be used to endorse or promote products derived from this software\n *  without specific prior written permission.\n *\n *  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS AS IS AND ANY\n *  EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n *  WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.\n *  IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT,\n *  INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT\n *  NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n *  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n *  WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n *  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n *  POSSIBILITY OF SUCH DAMAGE.\n */\n/**\n *  Exceptions from regular ASCII. CodePoints are mapped to UTF-16 codes\n */\n\nconst specialCea608CharsCodes = {\n  0x2a: 0xe1, // lowercase a, acute accent\n  0x5c: 0xe9, // lowercase e, acute accent\n  0x5e: 0xed, // lowercase i, acute accent\n  0x5f: 0xf3, // lowercase o, acute accent\n  0x60: 0xfa, // lowercase u, acute accent\n  0x7b: 0xe7, // lowercase c with cedilla\n  0x7c: 0xf7, // division symbol\n  0x7d: 0xd1, // uppercase N tilde\n  0x7e: 0xf1, // lowercase n tilde\n  0x7f: 0x2588, // Full block\n  // THIS BLOCK INCLUDES THE 16 EXTENDED (TWO-BYTE) LINE 21 CHARACTERS\n  // THAT COME FROM HI BYTE=0x11 AND LOW BETWEEN 0x30 AND 0x3F\n  // THIS MEANS THAT \\x50 MUST BE ADDED TO THE VALUES\n  0x80: 0xae, // Registered symbol (R)\n  0x81: 0xb0, // degree sign\n  0x82: 0xbd, // 1/2 symbol\n  0x83: 0xbf, // Inverted (open) question mark\n  0x84: 0x2122, // Trademark symbol (TM)\n  0x85: 0xa2, // Cents symbol\n  0x86: 0xa3, // Pounds sterling\n  0x87: 0x266a, // Music 8'th note\n  0x88: 0xe0, // lowercase a, grave accent\n  0x89: 0x20, // transparent space (regular)\n  0x8a: 0xe8, // lowercase e, grave accent\n  0x8b: 0xe2, // lowercase a, circumflex accent\n  0x8c: 0xea, // lowercase e, circumflex accent\n  0x8d: 0xee, // lowercase i, circumflex accent\n  0x8e: 0xf4, // lowercase o, circumflex accent\n  0x8f: 0xfb, // lowercase u, circumflex accent\n  // THIS BLOCK INCLUDES THE 32 EXTENDED (TWO-BYTE) LINE 21 CHARACTERS\n  // THAT COME FROM HI BYTE=0x12 AND LOW BETWEEN 0x20 AND 0x3F\n  0x90: 0xc1, // capital letter A with acute\n  0x91: 0xc9, // capital letter E with acute\n  0x92: 0xd3, // capital letter O with acute\n  0x93: 0xda, // capital letter U with acute\n  0x94: 0xdc, // capital letter U with diaresis\n  0x95: 0xfc, // lowercase letter U with diaeresis\n  0x96: 0x2018, // opening single quote\n  0x97: 0xa1, // inverted exclamation mark\n  0x98: 0x2a, // asterisk\n  0x99: 0x2019, // closing single quote\n  0x9a: 0x2501, // box drawings heavy horizontal\n  0x9b: 0xa9, // copyright sign\n  0x9c: 0x2120, // Service mark\n  0x9d: 0x2022, // (round) bullet\n  0x9e: 0x201c, // Left double quotation mark\n  0x9f: 0x201d, // Right double quotation mark\n  0xa0: 0xc0, // uppercase A, grave accent\n  0xa1: 0xc2, // uppercase A, circumflex\n  0xa2: 0xc7, // uppercase C with cedilla\n  0xa3: 0xc8, // uppercase E, grave accent\n  0xa4: 0xca, // uppercase E, circumflex\n  0xa5: 0xcb, // capital letter E with diaresis\n  0xa6: 0xeb, // lowercase letter e with diaresis\n  0xa7: 0xce, // uppercase I, circumflex\n  0xa8: 0xcf, // uppercase I, with diaresis\n  0xa9: 0xef, // lowercase i, with diaresis\n  0xaa: 0xd4, // uppercase O, circumflex\n  0xab: 0xd9, // uppercase U, grave accent\n  0xac: 0xf9, // lowercase u, grave accent\n  0xad: 0xdb, // uppercase U, circumflex\n  0xae: 0xab, // left-pointing double angle quotation mark\n  0xaf: 0xbb, // right-pointing double angle quotation mark\n  // THIS BLOCK INCLUDES THE 32 EXTENDED (TWO-BYTE) LINE 21 CHARACTERS\n  // THAT COME FROM HI BYTE=0x13 AND LOW BETWEEN 0x20 AND 0x3F\n  0xb0: 0xc3, // Uppercase A, tilde\n  0xb1: 0xe3, // Lowercase a, tilde\n  0xb2: 0xcd, // Uppercase I, acute accent\n  0xb3: 0xcc, // Uppercase I, grave accent\n  0xb4: 0xec, // Lowercase i, grave accent\n  0xb5: 0xd2, // Uppercase O, grave accent\n  0xb6: 0xf2, // Lowercase o, grave accent\n  0xb7: 0xd5, // Uppercase O, tilde\n  0xb8: 0xf5, // Lowercase o, tilde\n  0xb9: 0x7b, // Open curly brace\n  0xba: 0x7d, // Closing curly brace\n  0xbb: 0x5c, // Backslash\n  0xbc: 0x5e, // Caret\n  0xbd: 0x5f, // Underscore\n  0xbe: 0x7c, // Pipe (vertical line)\n  0xbf: 0x223c, // Tilde operator\n  0xc0: 0xc4, // Uppercase A, umlaut\n  0xc1: 0xe4, // Lowercase A, umlaut\n  0xc2: 0xd6, // Uppercase O, umlaut\n  0xc3: 0xf6, // Lowercase o, umlaut\n  0xc4: 0xdf, // Esszett (sharp S)\n  0xc5: 0xa5, // Yen symbol\n  0xc6: 0xa4, // Generic currency sign\n  0xc7: 0x2503, // Box drawings heavy vertical\n  0xc8: 0xc5, // Uppercase A, ring\n  0xc9: 0xe5, // Lowercase A, ring\n  0xca: 0xd8, // Uppercase O, stroke\n  0xcb: 0xf8, // Lowercase o, strok\n  0xcc: 0x250f, // Box drawings heavy down and right\n  0xcd: 0x2513, // Box drawings heavy down and left\n  0xce: 0x2517, // Box drawings heavy up and right\n  0xcf: 0x251b, // Box drawings heavy up and left\n};\n\n/**\n * Utils\n */\nconst getCharForByte = (byte: number) =>\n  String.fromCharCode(specialCea608CharsCodes[byte] || byte);\n\nconst NR_ROWS = 15;\nconst NR_COLS = 100;\n// Tables to look up row from PAC data\nconst rowsLowCh1 = {\n  0x11: 1,\n  0x12: 3,\n  0x15: 5,\n  0x16: 7,\n  0x17: 9,\n  0x10: 11,\n  0x13: 12,\n  0x14: 14,\n};\nconst rowsHighCh1 = {\n  0x11: 2,\n  0x12: 4,\n  0x15: 6,\n  0x16: 8,\n  0x17: 10,\n  0x13: 13,\n  0x14: 15,\n};\nconst rowsLowCh2 = {\n  0x19: 1,\n  0x1a: 3,\n  0x1d: 5,\n  0x1e: 7,\n  0x1f: 9,\n  0x18: 11,\n  0x1b: 12,\n  0x1c: 14,\n};\nconst rowsHighCh2 = {\n  0x19: 2,\n  0x1a: 4,\n  0x1d: 6,\n  0x1e: 8,\n  0x1f: 10,\n  0x1b: 13,\n  0x1c: 15,\n};\n\nconst backgroundColors = [\n  'white',\n  'green',\n  'blue',\n  'cyan',\n  'red',\n  'yellow',\n  'magenta',\n  'black',\n  'transparent',\n];\n\nconst enum VerboseLevel {\n  ERROR = 0,\n  TEXT = 1,\n  WARNING = 2,\n  INFO = 2,\n  DEBUG = 3,\n  DATA = 3,\n}\n\nclass CaptionsLogger {\n  public time: number | null = null;\n  public verboseLevel: VerboseLevel = VerboseLevel.ERROR;\n\n  log(severity: VerboseLevel, msg: string | (() => string)): void {\n    if (this.verboseLevel >= severity) {\n      const m: string = typeof msg === 'function' ? msg() : msg;\n      logger.log(`${this.time} [${severity}] ${m}`);\n    }\n  }\n}\n\nconst numArrayToHexArray = function (numArray: number[]): string[] {\n  const hexArray: string[] = [];\n  for (let j = 0; j < numArray.length; j++) {\n    hexArray.push(numArray[j].toString(16));\n  }\n\n  return hexArray;\n};\n\ntype PenStyles = {\n  foreground: string | null;\n  underline: boolean;\n  italics: boolean;\n  background: string;\n  flash: boolean;\n};\n\nclass PenState {\n  public foreground: string = 'white';\n  public underline: boolean = false;\n  public italics: boolean = false;\n  public background: string = 'black';\n  public flash: boolean = false;\n\n  reset() {\n    this.foreground = 'white';\n    this.underline = false;\n    this.italics = false;\n    this.background = 'black';\n    this.flash = false;\n  }\n\n  setStyles(styles: Partial<PenStyles>) {\n    const attribs = [\n      'foreground',\n      'underline',\n      'italics',\n      'background',\n      'flash',\n    ];\n    for (let i = 0; i < attribs.length; i++) {\n      const style = attribs[i];\n      if (styles.hasOwnProperty(style)) {\n        this[style] = styles[style];\n      }\n    }\n  }\n\n  isDefault() {\n    return (\n      this.foreground === 'white' &&\n      !this.underline &&\n      !this.italics &&\n      this.background === 'black' &&\n      !this.flash\n    );\n  }\n\n  equals(other: PenState) {\n    return (\n      this.foreground === other.foreground &&\n      this.underline === other.underline &&\n      this.italics === other.italics &&\n      this.background === other.background &&\n      this.flash === other.flash\n    );\n  }\n\n  copy(newPenState: PenState) {\n    this.foreground = newPenState.foreground;\n    this.underline = newPenState.underline;\n    this.italics = newPenState.italics;\n    this.background = newPenState.background;\n    this.flash = newPenState.flash;\n  }\n\n  toString(): string {\n    return (\n      'color=' +\n      this.foreground +\n      ', underline=' +\n      this.underline +\n      ', italics=' +\n      this.italics +\n      ', background=' +\n      this.background +\n      ', flash=' +\n      this.flash\n    );\n  }\n}\n\n/**\n * Unicode character with styling and background.\n * @constructor\n */\nclass StyledUnicodeChar {\n  uchar: string = ' ';\n  penState: PenState = new PenState();\n\n  reset() {\n    this.uchar = ' ';\n    this.penState.reset();\n  }\n\n  setChar(uchar: string, newPenState: PenState) {\n    this.uchar = uchar;\n    this.penState.copy(newPenState);\n  }\n\n  setPenState(newPenState: PenState) {\n    this.penState.copy(newPenState);\n  }\n\n  equals(other: StyledUnicodeChar) {\n    return this.uchar === other.uchar && this.penState.equals(other.penState);\n  }\n\n  copy(newChar: StyledUnicodeChar) {\n    this.uchar = newChar.uchar;\n    this.penState.copy(newChar.penState);\n  }\n\n  isEmpty(): boolean {\n    return this.uchar === ' ' && this.penState.isDefault();\n  }\n}\n\n/**\n * CEA-608 row consisting of NR_COLS instances of StyledUnicodeChar.\n * @constructor\n */\nexport class Row {\n  public chars: StyledUnicodeChar[] = [];\n  public pos: number = 0;\n  public currPenState: PenState = new PenState();\n  public cueStartTime: number | null = null;\n  private logger: CaptionsLogger;\n\n  constructor(logger: CaptionsLogger) {\n    for (let i = 0; i < NR_COLS; i++) {\n      this.chars.push(new StyledUnicodeChar());\n    }\n    this.logger = logger;\n  }\n\n  equals(other: Row) {\n    for (let i = 0; i < NR_COLS; i++) {\n      if (!this.chars[i].equals(other.chars[i])) {\n        return false;\n      }\n    }\n    return true;\n  }\n\n  copy(other: Row) {\n    for (let i = 0; i < NR_COLS; i++) {\n      this.chars[i].copy(other.chars[i]);\n    }\n  }\n\n  isEmpty(): boolean {\n    let empty = true;\n    for (let i = 0; i < NR_COLS; i++) {\n      if (!this.chars[i].isEmpty()) {\n        empty = false;\n        break;\n      }\n    }\n    return empty;\n  }\n\n  /**\n   *  Set the cursor to a valid column.\n   */\n  setCursor(absPos: number) {\n    if (this.pos !== absPos) {\n      this.pos = absPos;\n    }\n\n    if (this.pos < 0) {\n      this.logger.log(\n        VerboseLevel.DEBUG,\n        'Negative cursor position ' + this.pos,\n      );\n      this.pos = 0;\n    } else if (this.pos > NR_COLS) {\n      this.logger.log(\n        VerboseLevel.DEBUG,\n        'Too large cursor position ' + this.pos,\n      );\n      this.pos = NR_COLS;\n    }\n  }\n\n  /**\n   * Move the cursor relative to current position.\n   */\n  moveCursor(relPos: number) {\n    const newPos = this.pos + relPos;\n    if (relPos > 1) {\n      for (let i = this.pos + 1; i < newPos + 1; i++) {\n        this.chars[i].setPenState(this.currPenState);\n      }\n    }\n    this.setCursor(newPos);\n  }\n\n  /**\n   * Backspace, move one step back and clear character.\n   */\n  backSpace() {\n    this.moveCursor(-1);\n    this.chars[this.pos].setChar(' ', this.currPenState);\n  }\n\n  insertChar(byte: number) {\n    if (byte >= 0x90) {\n      // Extended char\n      this.backSpace();\n    }\n    const char = getCharForByte(byte);\n    if (this.pos >= NR_COLS) {\n      this.logger.log(\n        VerboseLevel.ERROR,\n        () =>\n          'Cannot insert ' +\n          byte.toString(16) +\n          ' (' +\n          char +\n          ') at position ' +\n          this.pos +\n          '. Skipping it!',\n      );\n      return;\n    }\n    this.chars[this.pos].setChar(char, this.currPenState);\n    this.moveCursor(1);\n  }\n\n  clearFromPos(startPos: number) {\n    let i: number;\n    for (i = startPos; i < NR_COLS; i++) {\n      this.chars[i].reset();\n    }\n  }\n\n  clear() {\n    this.clearFromPos(0);\n    this.pos = 0;\n    this.currPenState.reset();\n  }\n\n  clearToEndOfRow() {\n    this.clearFromPos(this.pos);\n  }\n\n  getTextString() {\n    const chars: string[] = [];\n    let empty = true;\n    for (let i = 0; i < NR_COLS; i++) {\n      const char = this.chars[i].uchar;\n      if (char !== ' ') {\n        empty = false;\n      }\n\n      chars.push(char);\n    }\n    if (empty) {\n      return '';\n    } else {\n      return chars.join('');\n    }\n  }\n\n  setPenStyles(styles: Partial<PenStyles>) {\n    this.currPenState.setStyles(styles);\n    const currChar = this.chars[this.pos];\n    currChar.setPenState(this.currPenState);\n  }\n}\n\n/**\n * Keep a CEA-608 screen of 32x15 styled characters\n * @constructor\n */\nexport class CaptionScreen {\n  rows: Row[] = [];\n  currRow: number = NR_ROWS - 1;\n  nrRollUpRows: number | null = null;\n  lastOutputScreen: CaptionScreen | null = null;\n  logger: CaptionsLogger;\n\n  constructor(logger: CaptionsLogger) {\n    for (let i = 0; i < NR_ROWS; i++) {\n      this.rows.push(new Row(logger));\n    }\n    this.logger = logger;\n  }\n\n  reset() {\n    for (let i = 0; i < NR_ROWS; i++) {\n      this.rows[i].clear();\n    }\n    this.currRow = NR_ROWS - 1;\n  }\n\n  equals(other: CaptionScreen): boolean {\n    let equal = true;\n    for (let i = 0; i < NR_ROWS; i++) {\n      if (!this.rows[i].equals(other.rows[i])) {\n        equal = false;\n        break;\n      }\n    }\n    return equal;\n  }\n\n  copy(other: CaptionScreen) {\n    for (let i = 0; i < NR_ROWS; i++) {\n      this.rows[i].copy(other.rows[i]);\n    }\n  }\n\n  isEmpty(): boolean {\n    let empty = true;\n    for (let i = 0; i < NR_ROWS; i++) {\n      if (!this.rows[i].isEmpty()) {\n        empty = false;\n        break;\n      }\n    }\n    return empty;\n  }\n\n  backSpace() {\n    const row = this.rows[this.currRow];\n    row.backSpace();\n  }\n\n  clearToEndOfRow() {\n    const row = this.rows[this.currRow];\n    row.clearToEndOfRow();\n  }\n\n  /**\n   * Insert a character (without styling) in the current row.\n   */\n  insertChar(char: number) {\n    const row = this.rows[this.currRow];\n    row.insertChar(char);\n  }\n\n  setPen(styles: Partial<PenStyles>) {\n    const row = this.rows[this.currRow];\n    row.setPenStyles(styles);\n  }\n\n  moveCursor(relPos: number) {\n    const row = this.rows[this.currRow];\n    row.moveCursor(relPos);\n  }\n\n  setCursor(absPos: number) {\n    this.logger.log(VerboseLevel.INFO, 'setCursor: ' + absPos);\n    const row = this.rows[this.currRow];\n    row.setCursor(absPos);\n  }\n\n  setPAC(pacData: PACData) {\n    this.logger.log(\n      VerboseLevel.INFO,\n      () => 'pacData = ' + JSON.stringify(pacData),\n    );\n    let newRow = pacData.row - 1;\n    if (this.nrRollUpRows && newRow < this.nrRollUpRows - 1) {\n      newRow = this.nrRollUpRows - 1;\n    }\n\n    // Make sure this only affects Roll-up Captions by checking this.nrRollUpRows\n    if (this.nrRollUpRows && this.currRow !== newRow) {\n      // clear all rows first\n      for (let i = 0; i < NR_ROWS; i++) {\n        this.rows[i].clear();\n      }\n\n      // Copy this.nrRollUpRows rows from lastOutputScreen and place it in the newRow location\n      // topRowIndex - the start of rows to copy (inclusive index)\n      const topRowIndex = this.currRow + 1 - this.nrRollUpRows;\n      // We only copy if the last position was already shown.\n      // We use the cueStartTime value to check this.\n      const lastOutputScreen = this.lastOutputScreen;\n      if (lastOutputScreen) {\n        const prevLineTime = lastOutputScreen.rows[topRowIndex].cueStartTime;\n        const time = this.logger.time;\n        if (prevLineTime !== null && time !== null && prevLineTime < time) {\n          for (let i = 0; i < this.nrRollUpRows; i++) {\n            this.rows[newRow - this.nrRollUpRows + i + 1].copy(\n              lastOutputScreen.rows[topRowIndex + i],\n            );\n          }\n        }\n      }\n    }\n\n    this.currRow = newRow;\n    const row = this.rows[this.currRow];\n    if (pacData.indent !== null) {\n      const indent = pacData.indent;\n      const prevPos = Math.max(indent - 1, 0);\n      row.setCursor(pacData.indent);\n      pacData.color = row.chars[prevPos].penState.foreground;\n    }\n    const styles: PenStyles = {\n      foreground: pacData.color,\n      underline: pacData.underline,\n      italics: pacData.italics,\n      background: 'black',\n      flash: false,\n    };\n    this.setPen(styles);\n  }\n\n  /**\n   * Set background/extra foreground, but first do back_space, and then insert space (backwards compatibility).\n   */\n  setBkgData(bkgData: Partial<PenStyles>) {\n    this.logger.log(\n      VerboseLevel.INFO,\n      () => 'bkgData = ' + JSON.stringify(bkgData),\n    );\n    this.backSpace();\n    this.setPen(bkgData);\n    this.insertChar(0x20); // Space\n  }\n\n  setRollUpRows(nrRows: number | null) {\n    this.nrRollUpRows = nrRows;\n  }\n\n  rollUp() {\n    if (this.nrRollUpRows === null) {\n      this.logger.log(\n        VerboseLevel.DEBUG,\n        'roll_up but nrRollUpRows not set yet',\n      );\n      return; // Not properly setup\n    }\n    this.logger.log(VerboseLevel.TEXT, () => this.getDisplayText());\n    const topRowIndex = this.currRow + 1 - this.nrRollUpRows;\n    const topRow = this.rows.splice(topRowIndex, 1)[0];\n    topRow.clear();\n    this.rows.splice(this.currRow, 0, topRow);\n    this.logger.log(VerboseLevel.INFO, 'Rolling up');\n    // this.logger.log(VerboseLevel.TEXT, this.get_display_text())\n  }\n\n  /**\n   * Get all non-empty rows with as unicode text.\n   */\n  getDisplayText(asOneRow?: boolean) {\n    asOneRow = asOneRow || false;\n    const displayText: string[] = [];\n    let text = '';\n    let rowNr = -1;\n    for (let i = 0; i < NR_ROWS; i++) {\n      const rowText = this.rows[i].getTextString();\n      if (rowText) {\n        rowNr = i + 1;\n        if (asOneRow) {\n          displayText.push('Row ' + rowNr + \": '\" + rowText + \"'\");\n        } else {\n          displayText.push(rowText.trim());\n        }\n      }\n    }\n    if (displayText.length > 0) {\n      if (asOneRow) {\n        text = '[' + displayText.join(' | ') + ']';\n      } else {\n        text = displayText.join('\\n');\n      }\n    }\n    return text;\n  }\n\n  getTextAndFormat() {\n    return this.rows;\n  }\n}\n\n// var modes = ['MODE_ROLL-UP', 'MODE_POP-ON', 'MODE_PAINT-ON', 'MODE_TEXT'];\n\ntype CaptionModes =\n  | 'MODE_ROLL-UP'\n  | 'MODE_POP-ON'\n  | 'MODE_PAINT-ON'\n  | 'MODE_TEXT'\n  | null;\n\nclass Cea608Channel {\n  chNr: number;\n  outputFilter: OutputFilter;\n  mode: CaptionModes;\n  verbose: number;\n  displayedMemory: CaptionScreen;\n  nonDisplayedMemory: CaptionScreen;\n  lastOutputScreen: CaptionScreen;\n  currRollUpRow: Row;\n  writeScreen: CaptionScreen;\n  cueStartTime: number | null;\n  logger: CaptionsLogger;\n\n  constructor(\n    channelNumber: number,\n    outputFilter: OutputFilter,\n    logger: CaptionsLogger,\n  ) {\n    this.chNr = channelNumber;\n    this.outputFilter 